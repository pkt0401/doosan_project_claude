import streamlit as st
import pandas as pd
import numpy as np
import faiss
import openai
import re
import os
import io
from PIL import Image
from sklearn.model_selection import train_test_split

# ------------- ì‹œìŠ¤í…œ ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ -----------------
system_texts = {
    "Korean": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "ì‹œìŠ¤í…œ ê°œìš”",
        "tab_phase1": "ìœ„í—˜ì„± í‰ê°€ (Phase 1)",
        "tab_phase2": "ê°œì„ ëŒ€ì±… ìƒì„± (Phase 2)",
        "overview_header": "LLM ê¸°ë°˜ ìœ„í—˜ì„±í‰ê°€ ì‹œìŠ¤í…œ",
        "overview_text": "ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹° AI Risk AssessmentëŠ” êµ­ë‚´ ë° í•´ì™¸ ê±´ì„¤í˜„ì¥ â€˜ìˆ˜ì‹œìœ„í—˜ì„±í‰ê°€â€™ ë° â€˜ë…¸ë™ë¶€ ì¤‘ëŒ€ì¬í•´ ì‚¬ë¡€â€™ë¥¼ í•™ìŠµí•˜ì—¬ ê°œë°œëœ ìë™ ìœ„í—˜ì„±í‰ê°€ í”„ë¡œê·¸ë¨ ì…ë‹ˆë‹¤. ìƒì„±ëœ ìœ„í—˜ì„±í‰ê°€ëŠ” ë°˜ë“œì‹œ ìˆ˜ì‹œ ìœ„í—˜ì„±í‰ê°€ ì‹¬ì˜íšŒë¥¼ í†µí•´ ê²€ì¦ í›„ ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.",
        "process_title": "AI ìœ„í—˜ì„±í‰ê°€ í”„ë¡œì„¸ìŠ¤",
        "features_title": "ì‹œìŠ¤í…œ íŠ¹ì§• ë° êµ¬ì„±ìš”ì†Œ",
        "phase1_features": """
        #### Phase 1: ìœ„í—˜ì„± í‰ê°€ ìë™í™”
        - ê³µì •ë³„ ì‘ì—…í™œë™ì— ë”°ë¥¸ ìœ„í—˜ì„±í‰ê°€ ë°ì´í„° í•™ìŠµ
        - ì‘ì—…í™œë™ ì…ë ¥ ì‹œ ìœ í•´ìœ„í—˜ìš”ì¸ ìë™ ì˜ˆì¸¡
        - ìœ ì‚¬ ìœ„í—˜ìš”ì¸ ì‚¬ë¡€ ê²€ìƒ‰ ë° í‘œì‹œ
        - ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) ê¸°ë°˜ ìœ„í—˜ë„(ë¹ˆë„, ê°•ë„, T) ì¸¡ì •
        - Excel ê¸°ë°˜ ê³µì •ë³„ ìœ„í—˜ì„±í‰ê°€ ë°ì´í„° ìë™ ë¶„ì„
        - ìœ„í—˜ë“±ê¸‰(A-E) ìë™ ì‚°ì •
        """,
        "phase2_features": """
        #### Phase 2: ê°œì„ ëŒ€ì±… ìë™ ìƒì„±
        - ìœ„í—˜ìš”ì†Œë³„ ë§ì¶¤í˜• ê°œì„ ëŒ€ì±… ìë™ ìƒì„±
        - ë‹¤êµ­ì–´(í•œ/ì˜/ì¤‘) ê°œì„ ëŒ€ì±… ìƒì„± ì§€ì›
        - ê°œì„  ì „í›„ ìœ„í—˜ë„(T) ìë™ ë¹„êµ ë¶„ì„
        - ìœ„í—˜ ê°ì†Œìœ¨(RRR) ì •ëŸ‰ì  ì‚°ì¶œ
        - ê³µì¢…/ê³µì •ë³„ ìµœì  ê°œì„ ëŒ€ì±… ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
        """,
        "phase1_header": "ìœ„í—˜ì„± í‰ê°€ ìë™í™” (Phase 1)",
        "api_key_label": "OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
        "dataset_label": "ë°ì´í„°ì…‹ ì„ íƒ",
        "load_data_label": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±",
        "load_data_btn": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±",
        "api_key_warning": "ê³„ì†í•˜ë ¤ë©´ OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.",
        "data_loading": "ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì¸ë±ìŠ¤ë¥¼ êµ¬ì„±í•˜ëŠ” ì¤‘...",
        "demo_limit_info": "ë°ëª¨ ëª©ì ìœ¼ë¡œ {max_texts}ê°œì˜ í…ìŠ¤íŠ¸ë§Œ ì„ë² ë”©í•©ë‹ˆë‹¤. ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.",
        "data_load_success": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„± ì™„ë£Œ! (ì´ {max_texts}ê°œ í•­ëª© ì²˜ë¦¬)",
        "hazard_prediction_header": "ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡",
        "load_first_warning": "ë¨¼ì € [ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±] ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.",
        "activity_label": "ì‘ì—…í™œë™:",
        "predict_hazard_btn": "ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡í•˜ê¸°",
        "activity_warning": "ì‘ì—…í™œë™ì„ ì…ë ¥í•˜ì„¸ìš”.",
        "predicting_hazard": "ìœ í•´ìœ„í—˜ìš”ì¸ì„ ì˜ˆì¸¡í•˜ëŠ” ì¤‘...",
        "similar_cases_header": "ìœ ì‚¬í•œ ì‚¬ë¡€",
        "similar_case_text": """
        <div class="similar-case">
            <strong>ì‚¬ë¡€ {i}</strong><br>
            <strong>ì‘ì—…í™œë™:</strong> {activity}<br>
            <strong>ìœ í•´ìœ„í—˜ìš”ì¸:</strong> {hazard}<br>
            <strong>ìœ„í—˜ë„:</strong> ë¹ˆë„ {freq}, ê°•ë„ {intensity}, Tê°’ {t_value} (ë“±ê¸‰ {grade})
        </div>
        """,
        "prediction_result_header": "ì˜ˆì¸¡ ê²°ê³¼",
        "activity_result": "ì‘ì—…í™œë™: {activity}",
        "hazard_result": "ì˜ˆì¸¡ëœ ìœ í•´ìœ„í—˜ìš”ì¸: {hazard}",
        "result_table_columns": ["í•­ëª©", "ê°’"],
        "result_table_rows": ["ë¹ˆë„", "ê°•ë„", "T ê°’", "ìœ„í—˜ë“±ê¸‰"],
        "parsing_error": "ìœ„í—˜ì„± í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        "gpt_response": "GPT ì›ë¬¸ ì‘ë‹µ: {response}",
        "phase2_header": "ê°œì„ ëŒ€ì±… ìë™ ìƒì„± (Phase 2)",
        "language_select_label": "ê°œì„ ëŒ€ì±… ì–¸ì–´ ì„ íƒ:",
        "input_method_label": "ì…ë ¥ ë°©ì‹ ì„ íƒ:",
        "input_methods": ["Phase 1 í‰ê°€ ê²°ê³¼ ì‚¬ìš©", "ì§ì ‘ ì…ë ¥"],
        "phase1_results_header": "Phase 1 í‰ê°€ ê²°ê³¼",
        "risk_level_text": "ìœ„í—˜ë„: ë¹ˆë„ {freq}, ê°•ë„ {intensity}, Tê°’ {t_value} (ë“±ê¸‰ {grade})",
        "phase1_first_warning": "ë¨¼ì € Phase 1ì—ì„œ ìœ„í—˜ì„± í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”.",
        "hazard_label": "ìœ í•´ìœ„í—˜ìš”ì¸:",
        "frequency_label": "ë¹ˆë„ (1-5):",
        "intensity_label": "ê°•ë„ (1-5):",
        "t_value_text": "Tê°’: {t_value} (ë“±ê¸‰: {grade})",
        "generate_improvement_btn": "ê°œì„ ëŒ€ì±… ìƒì„±",
        "generating_improvement": "ê°œì„ ëŒ€ì±…ì„ ìƒì„±í•˜ëŠ” ì¤‘...",
        "no_data_warning": "Phase 1ì—ì„œ ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±ì„ ì™„ë£Œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì˜ˆì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
        "improvement_result_header": "ê°œì„ ëŒ€ì±… ìƒì„± ê²°ê³¼",
        "improvement_plan_header": "ê°œì„ ëŒ€ì±…",
        "risk_improvement_header": "ìœ„í—˜ë„ ê°œì„  ê²°ê³¼",
        "comparison_columns": ["í•­ëª©", "ê°œì„  ì „", "ê°œì„  í›„"],
        "risk_reduction_label": "ìœ„í—˜ ê°ì†Œìœ¨ (RRR)",
        "t_value_change_header": "ìœ„í—˜ë„(Tê°’) ë³€í™”",
        "before_improvement": "ê°œì„  ì „ Tê°’:",
        "after_improvement": "ê°œì„  í›„ Tê°’:",
        "parsing_error_improvement": "ê°œì„ ëŒ€ì±… ìƒì„± ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    },
    "English": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "System Overview",
        "tab_phase1": "Risk Assessment (Phase 1)",
        "tab_phase2": "Improvement Measures (Phase 2)",
        "overview_header": "LLM-based Risk Assessment System",
        "overview_text": """
        The risk assessment automation system using LLM (Large Language Model) innovatively improves safety management at construction sites:
        
        1. <span class="highlight">Automatic prediction of 'hazards' and risk level calculation through generative AI</span> <span class="phase-badge">Phase 1</span>
        2. <span class="highlight">Automatic generation of improvement measures and reduction rate prediction to reduce risk level</span> <span class="phase-badge">Phase 2</span>
        3. AI learns existing risk assessments at construction sites by process and their hazard factors
        4. After the development of automatic generation technology, risk analysis and improvement measures based on risk level
        
        This system is expected to evolve into an integrated accident prediction program through the incorporation of AI technology into EHS platforms such as PIMS and Safety Guardian.
        """,
        "process_title": "AI Risk Assessment Process",
        "features_title": "System Features and Components",
        "phase1_features": """
        #### Phase 1: Risk Assessment Automation
        - Learning risk assessment data according to work activities by process
        - Automatic hazard prediction when work activities are entered
        - Similar case search and display
        - Risk level (frequency, intensity, T) measurement based on large language models (LLM)
        - Automatic analysis of Excel-based process-specific risk assessment data
        - Automatic risk grade (A-E) calculation
        """,
        "phase2_features": """
        #### Phase 2: Automatic Generation of Improvement Measures
        - Automatic generation of customized improvement measures for risk factors
        - Multilingual (Korean/English/Chinese) improvement measure generation support
        - Automatic comparative analysis of risk level (T) before and after improvement
        - Quantitative calculation of Risk Reduction Rate (RRR)
        - Building a database of optimal improvement measures by work type/process
        """,
        "phase1_header": "Risk Assessment Automation (Phase 1)",
        "api_key_label": "Enter OpenAI API Key:",
        "dataset_label": "Select Dataset",
        "load_data_label": "Load Data and Configure Index",
        "load_data_btn": "Load Data and Configure Index",
        "api_key_warning": "Please enter an OpenAI API key to continue.",
        "data_loading": "Loading data and configuring index...",
        "demo_limit_info": "For demo purposes, only embedding {max_texts} texts. In a real environment, all data should be processed.",
        "data_load_success": "Data load and index configuration complete! (Total {max_texts} items processed)",
        "hazard_prediction_header": "Hazard Prediction",
        "load_first_warning": "Please click the [Load Data and Configure Index] button first.",
        "activity_label": "Work Activity:",
        "predict_hazard_btn": "Predict Hazards",
        "activity_warning": "Please enter a work activity.",
        "predicting_hazard": "Predicting hazards...",
        "similar_cases_header": "Similar Cases",
        "similar_case_text": """
        <div class="similar-case">
            <strong>Case {i}</strong><br>
            <strong>Work Activity:</strong> {activity}<br>
            <strong>Hazard:</strong> {hazard}<br>
            <strong>Risk Level:</strong> Frequency {freq}, Intensity {intensity}, T-value {t_value} (Grade {grade})
        </div>
        """,
        "prediction_result_header": "Prediction Results",
        "activity_result": "Work Activity: {activity}",
        "hazard_result": "Predicted Hazard: {hazard}",
        "result_table_columns": ["Item", "Value"],
        "result_table_rows": ["Frequency", "Intensity", "T Value", "Risk Grade"],
        "parsing_error": "Unable to parse risk assessment results.",
        "gpt_response": "Original GPT Response: {response}",
        "phase2_header": "Automatic Generation of Improvement Measures (Phase 2)",
        "language_select_label": "Select Language for Improvement Measures:",
        "input_method_label": "Select Input Method:",
        "input_methods": ["Use Phase 1 Assessment Results", "Direct Input"],
        "phase1_results_header": "Phase 1 Assessment Results",
        "risk_level_text": "Risk Level: Frequency {freq}, Intensity {intensity}, T-value {t_value} (Grade {grade})",
        "phase1_first_warning": "Please perform a risk assessment in Phase 1 first.",
        "hazard_label": "Hazard:",
        "frequency_label": "Frequency (1-5):",
        "intensity_label": "Intensity (1-5):",
        "t_value_text": "T-value: {t_value} (Grade: {grade})",
        "generate_improvement_btn": "Generate Improvement Measures",
        "generating_improvement": "Generating improvement measures...",
        "no_data_warning": "Data loading and index configuration was not completed in Phase 1. Using basic examples.",
        "improvement_result_header": "Improvement Measure Generation Results",
        "improvement_plan_header": "Improvement Measures",
        "risk_improvement_header": "Risk Level Improvement Results",
        "comparison_columns": ["Item", "Before Improvement", "After Improvement"],
        "risk_reduction_label": "Risk Reduction Rate (RRR)",
        "t_value_change_header": "Risk Level (T-value) Change",
        "before_improvement": "T-value Before Improvement:",
        "after_improvement": "T-value After Improvement:",
        "parsing_error_improvement": "Unable to parse improvement measure generation results."
    },
    "Chinese": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "ç³»ç»Ÿæ¦‚è¿°",
        "tab_phase1": "é£é™©è¯„ä¼° (ç¬¬1é˜¶æ®µ)",
        "tab_phase2": "æ”¹è¿›æªæ–½ (ç¬¬2é˜¶æ®µ)",
        "overview_header": "åŸºäºLLMçš„é£é™©è¯„ä¼°ç³»ç»Ÿ",
        "overview_text": """
        ä½¿ç”¨LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰çš„é£é™©è¯„ä¼°è‡ªåŠ¨åŒ–ç³»ç»Ÿåˆ›æ–°åœ°æ”¹å–„äº†å»ºç­‘å·¥åœ°çš„å®‰å…¨ç®¡ç†ï¼š
        
        1. <span class="highlight">é€šè¿‡ç”Ÿæˆå¼AIè‡ªåŠ¨é¢„æµ‹"å±å®³å› ç´ "å¹¶è®¡ç®—é£é™©ç­‰çº§</span> <span class="phase-badge">ç¬¬1é˜¶æ®µ</span>
        2. <span class="highlight">è‡ªåŠ¨ç”Ÿæˆæ”¹è¿›æªæ–½å¹¶é¢„æµ‹é™ä½é£é™©çš„æ¯”ç‡</span> <span class="phase-badge">ç¬¬2é˜¶æ®µ</span>
        3. AIæŒ‰å·¥åºå­¦ä¹ å»ºç­‘å·¥åœ°çš„ç°æœ‰é£é™©è¯„ä¼°åŠå…¶å±å®³å› ç´ 
        4. åœ¨è‡ªåŠ¨ç”ŸæˆæŠ€æœ¯å¼€å‘å®Œæˆåï¼ŒåŸºäºé£é™©ç­‰çº§è¿›è¡Œé£é™©åˆ†æå’Œæ”¹è¿›æªæ–½ç”Ÿæˆ
        
        è¯¥ç³»ç»Ÿæœ‰æœ›é€šè¿‡å°†AIæŠ€æœ¯æ•´åˆåˆ°EHSå¹³å°ï¼ˆå¦‚PIMSå’Œå®‰å…¨å«å£«ï¼‰ä¸­ï¼Œå‘å±•æˆä¸ºä¸€ä¸ªç»¼åˆäº‹æ•…é¢„æµ‹ç¨‹åºã€‚
        """,
        "process_title": "AIé£é™©è¯„ä¼°æµç¨‹",
        "features_title": "ç³»ç»Ÿç‰¹ç‚¹å’Œç»„ä»¶",
        "phase1_features": """
        #### ç¬¬1é˜¶æ®µï¼šé£é™©è¯„ä¼°è‡ªåŠ¨åŒ–
        - æŒ‰å·¥åºå­¦ä¹ ä¸å·¥ä½œæ´»åŠ¨ç›¸å…³çš„é£é™©è¯„ä¼°æ•°æ®
        - è¾“å…¥å·¥ä½œæ´»åŠ¨æ—¶è‡ªåŠ¨é¢„æµ‹å±å®³å› ç´ 
        - ç›¸ä¼¼æ¡ˆä¾‹æœç´¢å’Œæ˜¾ç¤º
        - åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„é£é™©ç­‰çº§ï¼ˆé¢‘ç‡ã€å¼ºåº¦ã€Tå€¼ï¼‰æµ‹é‡
        - è‡ªåŠ¨åˆ†æåŸºäºExcelçš„ç‰¹å®šå·¥åºé£é™©è¯„ä¼°æ•°æ®
        - è‡ªåŠ¨è®¡ç®—é£é™©ç­‰çº§(A-E)
        """,
        "phase2_features": """
        #### ç¬¬2é˜¶æ®µï¼šè‡ªåŠ¨ç”Ÿæˆæ”¹è¿›æªæ–½
        - ä¸ºé£é™©å› ç´ è‡ªåŠ¨ç”Ÿæˆå®šåˆ¶çš„æ”¹è¿›æªæ–½
        - å¤šè¯­è¨€ï¼ˆéŸ©è¯­/è‹±è¯­/ä¸­æ–‡ï¼‰æ”¹è¿›æªæ–½ç”Ÿæˆæ”¯æŒ
        - æ”¹è¿›å‰åé£é™©ç­‰çº§ï¼ˆTå€¼ï¼‰çš„è‡ªåŠ¨æ¯”è¾ƒåˆ†æ
        - é£é™©é™ä½ç‡(RRR)çš„å®šé‡è®¡ç®—
        - å»ºç«‹æŒ‰å·¥ä½œç±»å‹/å·¥åºçš„æœ€ä½³æ”¹è¿›æªæ–½æ•°æ®åº“
        """,
        "phase1_header": "é£é™©è¯„ä¼°è‡ªåŠ¨åŒ– (ç¬¬1é˜¶æ®µ)",
        "api_key_label": "è¾“å…¥OpenAI APIå¯†é’¥ï¼š",
        "dataset_label": "é€‰æ‹©æ•°æ®é›†",
        "load_data_label": "åŠ è½½æ•°æ®å’Œé…ç½®ç´¢å¼•",
        "load_data_btn": "åŠ è½½æ•°æ®å’Œé…ç½®ç´¢å¼•",
        "api_key_warning": "è¯·è¾“å…¥OpenAI APIå¯†é’¥ä»¥ç»§ç»­ã€‚",
        "data_loading": "æ­£åœ¨åŠ è½½æ•°æ®å’Œé…ç½®ç´¢å¼•...",
        "demo_limit_info": "å‡ºäºæ¼”ç¤ºç›®çš„ï¼Œä»…åµŒå…¥{max_texts}ä¸ªæ–‡æœ¬ã€‚åœ¨å®é™…ç¯å¢ƒä¸­ï¼Œåº”å¤„ç†æ‰€æœ‰æ•°æ®ã€‚",
        "data_load_success": "æ•°æ®åŠ è½½å’Œç´¢å¼•é…ç½®å®Œæˆï¼ï¼ˆå…±å¤„ç†{max_texts}ä¸ªé¡¹ç›®ï¼‰",
        "hazard_prediction_header": "å±å®³é¢„æµ‹",
        "load_first_warning": "è¯·å…ˆç‚¹å‡»[åŠ è½½æ•°æ®å’Œé…ç½®ç´¢å¼•]æŒ‰é’®ã€‚",
        "activity_label": "å·¥ä½œæ´»åŠ¨ï¼š",
        "predict_hazard_btn": "é¢„æµ‹å±å®³",
        "activity_warning": "è¯·è¾“å…¥å·¥ä½œæ´»åŠ¨ã€‚",
        "predicting_hazard": "æ­£åœ¨é¢„æµ‹å±å®³...",
        "similar_cases_header": "ç›¸ä¼¼æ¡ˆä¾‹",
        "similar_case_text": """
        <div class="similar-case">
            <strong>æ¡ˆä¾‹ {i}</strong><br>
            <strong>å·¥ä½œæ´»åŠ¨ï¼š</strong> {activity}<br>
            <strong>å±å®³ï¼š</strong> {hazard}<br>
            <strong>é£é™©ç­‰çº§ï¼š</strong> é¢‘ç‡ {freq}, å¼ºåº¦ {intensity}, Tå€¼ {t_value} (ç­‰çº§ {grade})
        </div>
        """,
        "prediction_result_header": "é¢„æµ‹ç»“æœ",
        "activity_result": "å·¥ä½œæ´»åŠ¨: {activity}",
        "hazard_result": "é¢„æµ‹çš„å±å®³: {hazard}",
        "result_table_columns": ["é¡¹ç›®", "å€¼"],
        "result_table_rows": ["é¢‘ç‡", "å¼ºåº¦", "Tå€¼", "é£é™©ç­‰çº§"],
        "parsing_error": "æ— æ³•è§£æé£é™©è¯„ä¼°ç»“æœã€‚",
        "gpt_response": "åŸå§‹GPTå“åº”: {response}",
        "phase2_header": "è‡ªåŠ¨ç”Ÿæˆæ”¹è¿›æªæ–½ (ç¬¬2é˜¶æ®µ)",
        "language_select_label": "é€‰æ‹©æ”¹è¿›æªæ–½çš„è¯­è¨€ï¼š",
        "input_method_label": "é€‰æ‹©è¾“å…¥æ–¹æ³•ï¼š",
        "input_methods": ["ä½¿ç”¨ç¬¬1é˜¶æ®µè¯„ä¼°ç»“æœ", "ç›´æ¥è¾“å…¥"],
        "phase1_results_header": "ç¬¬1é˜¶æ®µè¯„ä¼°ç»“æœ",
        "risk_level_text": "é£é™©ç­‰çº§: é¢‘ç‡ {freq}, å¼ºåº¦ {intensity}, Tå€¼ {t_value} (ç­‰çº§ {grade})",
        "phase1_first_warning": "è¯·å…ˆåœ¨ç¬¬1é˜¶æ®µè¿›è¡Œé£é™©è¯„ä¼°ã€‚",
        "hazard_label": "å±å®³ï¼š",
        "frequency_label": "é¢‘ç‡ (1-5)ï¼š",
        "intensity_label": "å¼ºåº¦ (1-5)ï¼š",
        "t_value_text": "Tå€¼: {t_value} (ç­‰çº§: {grade})",
        "generate_improvement_btn": "ç”Ÿæˆæ”¹è¿›æªæ–½",
        "generating_improvement": "æ­£åœ¨ç”Ÿæˆæ”¹è¿›æªæ–½...",
        "no_data_warning": "åœ¨ç¬¬1é˜¶æ®µæœªå®Œæˆæ•°æ®åŠ è½½å’Œç´¢å¼•é…ç½®ã€‚ä½¿ç”¨åŸºæœ¬ç¤ºä¾‹ã€‚",
        "improvement_result_header": "æ”¹è¿›æªæ–½ç”Ÿæˆç»“æœ",
        "improvement_plan_header": "æ”¹è¿›æªæ–½",
        "risk_improvement_header": "é£é™©ç­‰çº§æ”¹è¿›ç»“æœ",
        "comparison_columns": ["é¡¹ç›®", "æ”¹è¿›å‰", "æ”¹è¿›å"],
        "risk_reduction_label": "é£é™©é™ä½ç‡ (RRR)",
        "t_value_change_header": "é£é™©ç­‰çº§ (Tå€¼) å˜åŒ–",
        "before_improvement": "æ”¹è¿›å‰Tå€¼ï¼š",
        "after_improvement": "æ”¹è¿›åTå€¼ï¼š",
        "parsing_error_improvement": "æ— æ³•è§£ææ”¹è¿›æªæ–½ç”Ÿæˆç»“æœã€‚"
    }
}

# ----------------- í˜ì´ì§€ / ìŠ¤íƒ€ì¼ -----------------
st.set_page_config(page_title="Artificial Intelligence Risk Assessment", page_icon="ğŸ› ï¸", layout="wide")

st.markdown("""
<style>
/* (ì§ˆë¬¸ì—ì„œ ì œê³µí•œ ë™ì¼í•œ CSS ê·¸ëŒ€ë¡œ) */
.main-header{font-size:2.5rem;color:#1E88E5;text-align:center;margin-bottom:1rem}
.sub-header{font-size:1.8rem;color:#0D47A1;margin-top:2rem;margin-bottom:1rem}
.metric-container{background-color:#f0f2f6;border-radius:10px;padding:20px;box-shadow:2px 2px 5px rgba(0,0,0,0.1)}
.result-box{background-color:#f8f9fa;border-radius:10px;padding:15px;margin-top:10px;margin-bottom:10px;border-left:5px solid #4CAF50}
.phase-badge{background-color:#4CAF50;color:white;padding:5px 10px;border-radius:15px;font-size:0.8rem;margin-right:10px}
.similar-case{background-color:#f1f8e9;border-radius:8px;padding:12px;margin-bottom:8px;border-left:4px solid #689f38}
.language-selector{position:absolute;top:10px;right:10px;z-index:1000}
</style>
""", unsafe_allow_html=True)

# ----------------- ì„¸ì…˜ ìƒíƒœ -----------------
ss = st.session_state
for key, default in {
    "language":"Korean","index":None,"embeddings":None,
    "retriever_pool_df":None,"last_assessment":None
}.items():
    if key not in ss: ss[key]=default

# ----------------- ì–¸ì–´ ì„ íƒ -----------------
col0, colLang = st.columns([6,1])
with colLang:
    lang = st.selectbox("", list(system_texts.keys()), index=list(system_texts.keys()).index(ss.language))
    ss.language = lang
texts = system_texts[ss.language]

# ----------------- í—¤ë” -----------------
st.markdown(f'<div class="main-header">{texts["title"]}</div>', unsafe_allow_html=True)

# ----------------- íƒ­ êµ¬ì„± -----------------
tabs = st.tabs([texts["tab_overview"], "Risk Assessment âœ¨"])

# -----------------------------------------------------------------------------
# ---------------------------  ê³µìš© ìœ í‹¸ë¦¬í‹° -----------------------------------
# -----------------------------------------------------------------------------

def determine_grade(value: int):
    if 16<=value<=25: return 'A'
    if 10<=value<=15: return 'B'
    if 5<=value<=9: return 'C'
    if 3<=value<=4: return 'D'
    if 1<=value<=2: return 'E'
    return 'Unknown' if ss.language!='Korean' else 'ì•Œ ìˆ˜ ì—†ìŒ'

@st.cache_data(show_spinner=False)
def load_data(selected_dataset_name: str):
    try:
        df = pd.read_excel(f"{selected_dataset_name}.xlsx")
        if 'ì‚­ì œ Del' in df.columns: df.drop(['ì‚­ì œ Del'], axis=1, inplace=True)
        df = df.iloc[1:]
        df.rename(columns={df.columns[4]:'ë¹ˆë„', df.columns[5]:'ê°•ë„'}, inplace=True)
        df['T'] = pd.to_numeric(df['ë¹ˆë„'])*pd.to_numeric(df['ê°•ë„'])
        df = df.iloc[:,:7]
        df.rename(columns={
            'ì‘ì—…í™œë™ ë° ë‚´ìš©\nWork & Contents':'ì‘ì—…í™œë™ ë° ë‚´ìš©',
            'ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥\nHazard & Risk':'ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥',
            'í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥\nDamage & Effect':'í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥',
            df.columns[6]:'T'
        }, inplace=True)
        df['ë“±ê¸‰']=df['T'].apply(determine_grade)
        return df
    except Exception as e:
        st.warning("ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ â€“ "+str(e))
        data={"ì‘ì—…í™œë™ ë° ë‚´ìš©":["Shoring Installation","Transport"],"ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥":["Fall","Collision"],"í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥":["Injury","Damage"],"ë¹ˆë„":[3,3],"ê°•ë„":[2,3]}
        df=pd.DataFrame(data)
        df['T']=df['ë¹ˆë„']*df['ê°•ë„']; df['ë“±ê¸‰']=df['T'].apply(determine_grade)
        return df

def embed_texts_with_openai(texts, model="text-embedding-3-large", api_key=None):
    if api_key: openai.api_key=api_key
    embeddings=[]
    for txt in texts:
        try:
            resp=openai.Embedding.create(model=model, input=[str(txt).replace("\n"," ")])
            embeddings.append(resp["data"][0]["embedding"])
        except: embeddings.append([0]*1536)
    return embeddings

def generate_with_gpt(prompt, api_key, language):
    if api_key: openai.api_key=api_key
    sys_prompts={
        "Korean":"ìœ„í—˜ì„± í‰ê°€ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ë‹µë³€.",
        "English":"Assistant for risk assessment. Respond in English.",
        "Chinese":"é£é™©è¯„ä¼°åŠ©æ‰‹ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"}
    try:
        resp=openai.ChatCompletion.create(model="gpt-4o", messages=[
            {"role":"system","content":sys_prompts.get(language,sys_prompts['Korean'])},
            {"role":"user","content":prompt}], temperature=0.0,max_tokens=250)
        return resp['choices'][0]['message']['content'].strip()
    except Exception as e:
        st.error("GPT í˜¸ì¶œ ì˜¤ë¥˜: "+str(e)); return ""

# ----------------- í”„ë¡¬í”„íŠ¸ / íŒŒì‹± (Phase1+2) -----------------
# (ì§ˆë¬¸ì—ì„œ ì œê³µí•œ construct_prompt_* / parse_* í•¨ìˆ˜ë“¤ì€ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì˜€ìŒ)
def construct_prompt_phase1_hazard(retrieved_docs, activity_text, language="Korean"):
    """ì‘ì—…í™œë™ìœ¼ë¡œë¶€í„° ìœ í•´ìœ„í—˜ìš”ì¸ì„ ì˜ˆì¸¡í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±."""
    # ì–¸ì–´ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    prompt_templates = {
        "Korean": {
            "intro": "ë‹¤ìŒì€ ê±´ì„¤ í˜„ì¥ì˜ ì‘ì—…í™œë™ê³¼ ê·¸ì— ë”°ë¥¸ ìœ í•´ìœ„í—˜ìš”ì¸ì˜ ì˜ˆì‹œì…ë‹ˆë‹¤:\n\n",
            "example_format": "ì˜ˆì‹œ {i}:\nì‘ì—…í™œë™: {activity}\nìœ í•´ìœ„í—˜ìš”ì¸: {hazard}\n\n",
            "query_format": "ì´ì œ ë‹¤ìŒ ì‘ì—…í™œë™ì— ëŒ€í•œ ìœ í•´ìœ„í—˜ìš”ì¸ì„ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”:\nì‘ì—…í™œë™: {activity}\nìœ í•´ìœ„í—˜ìš”ì¸: "
        },
        "English": {
            "intro": "The following are examples of work activities at construction sites and their associated hazards:\n\n",
            "example_format": "Example {i}:\nWork Activity: {activity}\nHazard: {hazard}\n\n",
            "query_format": "Now, please predict the hazard for the following work activity:\nWork Activity: {activity}\nHazard: "
        },
        "Chinese": {
            "intro": "ä»¥ä¸‹æ˜¯å»ºç­‘å·¥åœ°çš„å·¥ä½œæ´»åŠ¨åŠå…¶ç›¸å…³å±å®³çš„ä¾‹å­:\n\n",
            "example_format": "ä¾‹å­ {i}:\nå·¥ä½œæ´»åŠ¨: {activity}\nå±å®³: {hazard}\n\n",
            "query_format": "ç°åœ¨ï¼Œè¯·é¢„æµ‹ä»¥ä¸‹å·¥ä½œæ´»åŠ¨çš„å±å®³:\nå·¥ä½œæ´»åŠ¨: {activity}\nå±å®³: "
        }
    }
    
    # í˜„ì¬ ì–¸ì–´ì˜ í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°
    template = prompt_templates.get(language, prompt_templates["Korean"])
    
    retrieved_examples = []
    for _, doc in retrieved_docs.iterrows():
        try:
            activity = doc['ì‘ì—…í™œë™ ë° ë‚´ìš©']
            hazard = doc['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥']
            retrieved_examples.append((activity, hazard))
        except:
            continue
    
    prompt = template["intro"]
    for i, (activity, hazard) in enumerate(retrieved_examples, 1):
        prompt += template["example_format"].format(i=i, activity=activity, hazard=hazard)
    
    prompt += template["query_format"].format(activity=activity_text)
    
    return prompt

# ë¹ˆë„ì™€ ê°•ë„ ì˜ˆì¸¡ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± (Phase 1)
def construct_prompt_phase1_risk(retrieved_docs, activity_text, hazard_text, language="Korean"):
    """ì‘ì—…í™œë™ê³¼ ìœ í•´ìœ„í—˜ìš”ì¸ì„ ë°”íƒ•ìœ¼ë¡œ ë¹ˆë„ì™€ ê°•ë„ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±."""
    # ì–¸ì–´ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    prompt_templates = {
        "Korean": {
            "example_format": "ì˜ˆì‹œ {i}:\nì…ë ¥: {input}\nì¶œë ¥: {output}\n\n",
            "query_format": "ì…ë ¥: {activity} - {hazard}\nìœ„ ì…ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ë¹ˆë„ì™€ ê°•ë„ë¥¼ ì˜ˆì¸¡í•˜ì„¸ìš”. ë¹ˆë„ëŠ” 1ì—ì„œ 5 ì‚¬ì´ì˜ ì •ìˆ˜ì…ë‹ˆë‹¤. ê°•ë„ëŠ” 1ì—ì„œ 5 ì‚¬ì´ì˜ ì •ìˆ˜ì…ë‹ˆë‹¤. TëŠ” ë¹ˆë„ì™€ ê°•ë„ë¥¼ ê³±í•œ ê°’ì…ë‹ˆë‹¤.\në‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:\n{json_format}\nì¶œë ¥:\n"
        },
        "English": {
            "example_format": "Example {i}:\nInput: {input}\nOutput: {output}\n\n",
            "query_format": "Input: {activity} - {hazard}\nBased on the above input, predict frequency and intensity. Frequency is an integer between 1 and 5. Intensity is an integer between 1 and 5. T is the product of frequency and intensity.\nOutput in the following JSON format:\n{json_format}\nOutput:\n"
        },
        "Chinese": {
            "example_format": "ç¤ºä¾‹ {i}:\nè¾“å…¥: {input}\nè¾“å‡º: {output}\n\n",
            "query_format": "è¾“å…¥: {activity} - {hazard}\næ ¹æ®ä¸Šè¿°è¾“å…¥ï¼Œé¢„æµ‹é¢‘ç‡å’Œå¼ºåº¦ã€‚é¢‘ç‡æ˜¯1åˆ°5ä¹‹é—´çš„æ•´æ•°ã€‚å¼ºåº¦æ˜¯1åˆ°5ä¹‹é—´çš„æ•´æ•°ã€‚Tæ˜¯é¢‘ç‡å’Œå¼ºåº¦çš„ä¹˜ç§¯ã€‚\nè¯·ä»¥ä¸‹åˆ—JSONæ ¼å¼è¾“å‡º:\n{json_format}\nè¾“å‡º:\n"
        }
    }
    
    # JSON í˜•ì‹ ì–¸ì–´ë³„ ì •ì˜
    json_formats = {
        "Korean": '{"ë¹ˆë„": ìˆ«ì, "ê°•ë„": ìˆ«ì, "T": ìˆ«ì}',
        "English": '{"frequency": number, "intensity": number, "T": number}',
        "Chinese": '{"é¢‘ç‡": æ•°å­—, "å¼ºåº¦": æ•°å­—, "T": æ•°å­—}'
    }
    
    # í˜„ì¬ ì–¸ì–´ì˜ í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°
    template = prompt_templates.get(language, prompt_templates["Korean"])
    json_format = json_formats.get(language, json_formats["Korean"])
    
    retrieved_examples = []
    for _, doc in retrieved_docs.iterrows():
        try:
            example_input = f"{doc['ì‘ì—…í™œë™ ë° ë‚´ìš©']} - {doc['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥']}"
            frequency = int(doc['ë¹ˆë„'])
            intensity = int(doc['ê°•ë„'])
            T_value = frequency * intensity
            
            # ì–¸ì–´ë³„ JSON ì¶œë ¥ í˜•ì‹
            if language == "Korean":
                example_output = f'{{"ë¹ˆë„": {frequency}, "ê°•ë„": {intensity}, "T": {T_value}}}'
            elif language == "English":
                example_output = f'{{"frequency": {frequency}, "intensity": {intensity}, "T": {T_value}}}'
            elif language == "Chinese":
                example_output = f'{{"é¢‘ç‡": {frequency}, "å¼ºåº¦": {intensity}, "T": {T_value}}}'
            else:
                example_output = f'{{"ë¹ˆë„": {frequency}, "ê°•ë„": {intensity}, "T": {T_value}}}'
                
            retrieved_examples.append((example_input, example_output))
        except:
            continue
    
    prompt = ""
    for i, (example_input, example_output) in enumerate(retrieved_examples, 1):
        prompt += template["example_format"].format(i=i, input=example_input, output=example_output)
    
    prompt += template["query_format"].format(
        activity=activity_text, 
        hazard=hazard_text,
        json_format=json_format
    )
    
    return prompt


# GPT ì¶œë ¥ íŒŒì‹± (Phase 1)
def parse_gpt_output_phase1(gpt_output, language="Korean"):
    """GPT ì¶œë ¥ì—ì„œ {ë¹ˆë„, ê°•ë„, T}ë¥¼ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¶”ì¶œ."""
    # ì–¸ì–´ë³„ JSON íŒ¨í„´
    json_patterns = {
        "Korean": r'\{"ë¹ˆë„":\s*([1-5]),\s*"ê°•ë„":\s*([1-5]),\s*"T":\s*([0-9]+)\}',
        "English": r'\{"frequency":\s*([1-5]),\s*"intensity":\s*([1-5]),\s*"T":\s*([0-9]+)\}',
        "Chinese": r'\{"é¢‘ç‡":\s*([1-5]),\s*"å¼ºåº¦":\s*([1-5]),\s*"T":\s*([0-9]+)\}'
    }
    
    pattern = json_patterns.get(language, json_patterns["Korean"])
    match = re.search(pattern, gpt_output)
    
    if match:
        pred_frequency = int(match.group(1))
        pred_intensity = int(match.group(2))
        pred_T = int(match.group(3))
        return pred_frequency, pred_intensity, pred_T
    else:
        # ë‹¤ë¥¸ íŒ¨í„´ë„ ì‹œë„ (GPTê°€ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ì‘ë‹µí–ˆì„ ê²½ìš°)
        for lang, pattern in json_patterns.items():
            if lang != language:  # ì´ë¯¸ ì‹œë„í•œ ì–¸ì–´ëŠ” ê±´ë„ˆë›´ë‹¤
                match = re.search(pattern, gpt_output)
                if match:
                    pred_frequency = int(match.group(1))
                    pred_intensity = int(match.group(2))
                    pred_T = int(match.group(3))
                    return pred_frequency, pred_intensity, pred_T
        
        return None

def construct_prompt_phase2(retrieved_docs, activity_text, hazard_text, freq, intensity, T, target_language="Korean"):
    """
    ê°œì„ ëŒ€ì±… ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    """
    # ì˜ˆì‹œ ì„¹ì…˜ êµ¬ì„±
    example_section = ""
    examples_added = 0
    
    # ì–¸ì–´ë³„ í•„ë“œëª…
    field_names = {
        "Korean": {
            "improvement_fields": ['ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ', 'ê°œì„ ëŒ€ì±…', 'ê°œì„ ë°©ì•ˆ'],
            "activity": "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "hazard": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥",
            "freq": "ë¹ˆë„",
            "intensity": "ê°•ë„",
            "example_intro": "Example:",
            "input_activity": "Input (Activity): ",
            "input_hazard": "Input (Hazard): ",
            "input_freq": "Input (Original Frequency): ",
            "input_intensity": "Input (Original Intensity): ",
            "input_t": "Input (Original T): ",
            "output_intro": "Output (Improvement Plan and Risk Reduction) in JSON:",
            "improvement": "ê°œì„ ëŒ€ì±…",
            "improved_freq": "ê°œì„  í›„ ë¹ˆë„",
            "improved_intensity": "ê°œì„  í›„ ê°•ë„",
            "improved_t": "ê°œì„  í›„ T",
            "reduction_rate": "T ê°ì†Œìœ¨"
        },
        "English": {
            "improvement_fields": ['Improvement Measures', 'Improvement Plan', 'Countermeasures'],
            "activity": "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "hazard": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥",
            "freq": "ë¹ˆë„",
            "intensity": "ê°•ë„",
            "example_intro": "Example:",
            "input_activity": "Input (Activity): ",
            "input_hazard": "Input (Hazard): ",
            "input_freq": "Input (Original Frequency): ",
            "input_intensity": "Input (Original Intensity): ",
            "input_t": "Input (Original T): ",
            "output_intro": "Output (Improvement Plan and Risk Reduction) in JSON:",
            "improvement": "improvement_plan",
            "improved_freq": "improved_frequency",
            "improved_intensity": "improved_intensity",
            "improved_t": "improved_T",
            "reduction_rate": "reduction_rate"
        },
        "Chinese": {
            "improvement_fields": ['æ”¹è¿›æªæ–½', 'æ”¹è¿›è®¡åˆ’', 'å¯¹ç­–'],
            "activity": "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "hazard": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥",
            "freq": "ë¹ˆë„",
            "intensity": "ê°•ë„",
            "example_intro": "ç¤ºä¾‹:",
            "input_activity": "è¾“å…¥ (å·¥ä½œæ´»åŠ¨): ",
            "input_hazard": "è¾“å…¥ (å±å®³): ",
            "input_freq": "è¾“å…¥ (åŸé¢‘ç‡): ",
            "input_intensity": "è¾“å…¥ (åŸå¼ºåº¦): ",
            "input_t": "è¾“å…¥ (åŸTå€¼): ",
            "output_intro": "è¾“å‡º (æ”¹è¿›è®¡åˆ’å’Œé£é™©é™ä½) ä»¥JSONæ ¼å¼:",
            "improvement": "æ”¹è¿›æªæ–½",
            "improved_freq": "æ”¹è¿›åé¢‘ç‡",
            "improved_intensity": "æ”¹è¿›åå¼ºåº¦",
            "improved_t": "æ”¹è¿›åTå€¼",
            "reduction_rate": "Tå€¼é™ä½ç‡"
        }
    }
    
    # í˜„ì¬ ì–¸ì–´ì— ë§ëŠ” í•„ë“œëª… ê°€ì ¸ì˜¤ê¸°
    fields = field_names.get(target_language, field_names["Korean"])
    
    for _, row in retrieved_docs.iterrows():
        try:
            # Phase2 ë°ì´í„°ì…‹ì— ìˆëŠ” ê°œì„ ëŒ€ì±… í•„ë“œ ì‚¬ìš© ì‹œë„
            improvement_plan = ""
            for field in fields["improvement_fields"]:
                if field in row and pd.notna(row[field]):
                    improvement_plan = row[field]
                    break
            
            if not improvement_plan:
                continue  # ê°œì„ ëŒ€ì±…ì´ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                
            original_freq = int(row[fields["freq"]]) if fields["freq"] in row else 3
            original_intensity = int(row[fields["intensity"]]) if fields["intensity"] in row else 3
            original_T = original_freq * original_intensity
                
            # ê°œì„  í›„ ë°ì´í„° ì‹œë„
            improved_freq = 1
            improved_intensity = 1
            improved_T = 1
            
            for field_pattern in [('ê°œì„  í›„ ë¹ˆë„', 'ê°œì„  í›„ ê°•ë„', 'ê°œì„  í›„ T'), ('ê°œì„ ë¹ˆë„', 'ê°œì„ ê°•ë„', 'ê°œì„ T')]:
                if all(field in row for field in field_pattern):
                    improved_freq = int(row[field_pattern[0]])
                    improved_intensity = int(row[field_pattern[1]])
                    improved_T = int(row[field_pattern[2]])
                    break
            
            example_section += (
                f"{fields['example_intro']}\n"
                f"{fields['input_activity']}{row[fields['activity']]}\n"
                f"{fields['input_hazard']}{row[fields['hazard']]}\n"
                f"{fields['input_freq']}{original_freq}\n"
                f"{fields['input_intensity']}{original_intensity}\n"
                f"{fields['input_t']}{original_T}\n"
                f"{fields['output_intro']}\n"
                "{\n"
                f'  "{fields["improvement"]}": "{improvement_plan}",\n'
                f'  "{fields["improved_freq"]}": {improved_freq},\n'
                f'  "{fields["improved_intensity"]}": {improved_intensity},\n'
                f'  "{fields["improved_t"]}": {improved_T},\n'
                f'  "{fields["reduction_rate"]}": {compute_rrr(original_T, improved_T):.2f}\n'
                "}\n\n"
            )
            
            examples_added += 1
            if examples_added >= 3:  # ìµœëŒ€ 3ê°œ ì˜ˆì‹œë§Œ ì‚¬ìš©
                break
                
        except Exception as e:
            # ì—ëŸ¬ ë°œìƒ ì‹œ í•´ë‹¹ ì˜ˆì‹œ ê±´ë„ˆë›°ê¸°
            continue
    
    # ì˜ˆì‹œê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì˜ˆì‹œ ì¶”ê°€ (ì–¸ì–´ë³„)
    if examples_added == 0:
        # í•œêµ­ì–´ ê¸°ë³¸ ì˜ˆì‹œ
        if target_language == "Korean":
            example_section = """
Example:
Input (Activity): Excavation and backfilling
Input (Hazard): Collapse of excavation wall due to improper sloping
Input (Original Frequency): 3
Input (Original Intensity): 4
Input (Original T): 12
Output (Improvement Plan and Risk Reduction) in JSON:
{
  "ê°œì„ ëŒ€ì±…": "1) í† ì–‘ ë¶„ë¥˜ì— ë”°ë¥¸ ì ì ˆí•œ ê²½ì‚¬ ìœ ì§€ 2) êµ´ì°© ë²½ë©´ ë³´ê°• 3) ì •ê¸°ì ì¸ ì§€ë°˜ ìƒíƒœ ê²€ì‚¬ ì‹¤ì‹œ",
  "ê°œì„  í›„ ë¹ˆë„": 1,
  "ê°œì„  í›„ ê°•ë„": 2,
  "ê°œì„  í›„ T": 2,
  "T ê°ì†Œìœ¨": 83.33
}

Example:
Input (Activity): Lifting operation
Input (Hazard): Material fall due to improper rigging
Input (Original Frequency): 2
Input (Original Intensity): 5
Input (Original T): 10
Output (Improvement Plan and Risk Reduction) in JSON:
{
  "ê°œì„ ëŒ€ì±…": "1) ë¦¬ê¹… ì „ë¬¸ê°€ ì‘ì—… ì°¸ì—¬ 2) ë¦¬ê¹… ì¥ë¹„ ì‚¬ì „ ì ê²€ 3) ì•ˆì „ êµ¬ì—­ ì„¤ì • ë° ì ‘ê·¼ í†µì œ",
  "ê°œì„  í›„ ë¹ˆë„": 1,
  "ê°œì„  í›„ ê°•ë„": 2,
  "ê°œì„  í›„ T": 2,
  "T ê°ì†Œìœ¨": 80.00
}
"""
        # ì˜ì–´ ê¸°ë³¸ ì˜ˆì‹œ
        elif target_language == "English":
            example_section = """
Example:
Input (Activity): Excavation and backfilling
Input (Hazard): Collapse of excavation wall due to improper sloping
Input (Original Frequency): 3
Input (Original Intensity): 4
Input (Original T): 12
Output (Improvement Plan and Risk Reduction) in JSON:
{
  "improvement_plan": "1) Maintain proper slope according to soil classification 2) Reinforce excavation walls 3) Conduct regular ground condition inspections",
  "improved_frequency": 1,
  "improved_intensity": 2,
  "improved_T": 2,
  "reduction_rate": 83.33
}

Example:
Input (Activity): Lifting operation
Input (Hazard): Material fall due to improper rigging
Input (Original Frequency): 2
Input (Original Intensity): 5
Input (Original T): 10
Output (Improvement Plan and Risk Reduction) in JSON:
{
  "improvement_plan": "1) Involve rigging experts in operations 2) Pre-inspect rigging equipment 3) Set up safety zones and control access",
  "improved_frequency": 1,
  "improved_intensity": 2,
  "improved_T": 2,
  "reduction_rate": 80.00
}
"""
        # ì¤‘êµ­ì–´ ê¸°ë³¸ ì˜ˆì‹œ
        elif target_language == "Chinese":
            example_section = """
ç¤ºä¾‹:
è¾“å…¥ (å·¥ä½œæ´»åŠ¨): Excavation and backfilling
è¾“å…¥ (å±å®³): Collapse of excavation wall due to improper sloping
è¾“å…¥ (åŸé¢‘ç‡): 3
è¾“å…¥ (åŸå¼ºåº¦): 4
è¾“å…¥ (åŸTå€¼): 12
è¾“å‡º (æ”¹è¿›è®¡åˆ’å’Œé£é™©é™ä½) ä»¥JSONæ ¼å¼:
{
  "æ”¹è¿›æªæ–½": "1) æ ¹æ®åœŸå£¤åˆ†ç±»ç»´æŒé€‚å½“çš„æ–œå¡ 2) åŠ å›ºæŒ–æ˜å¢™å£ 3) å®šæœŸè¿›è¡Œåœ°é¢çŠ¶å†µæ£€æŸ¥",
  "æ”¹è¿›åé¢‘ç‡": 1,
  "æ”¹è¿›åå¼ºåº¦": 2,
  "æ”¹è¿›åTå€¼": 2,
  "Tå€¼é™ä½ç‡": 83.33
}

ç¤ºä¾‹:
è¾“å…¥ (å·¥ä½œæ´»åŠ¨): Lifting operation
è¾“å…¥ (å±å®³): Material fall due to improper rigging
è¾“å…¥ (åŸé¢‘ç‡): 2
è¾“å…¥ (åŸå¼ºåº¦): 5
è¾“å…¥ (åŸTå€¼): 10
è¾“å‡º (æ”¹è¿›è®¡åˆ’å’Œé£é™©é™ä½) ä»¥JSONæ ¼å¼:
{
  "æ”¹è¿›æªæ–½": "1) åŠè£…ä¸“å®¶å‚ä¸ä½œä¸š 2) é¢„æ£€æŸ¥åŠè£…è®¾å¤‡ 3) è®¾ç½®å®‰å…¨åŒºåŸŸå¹¶æ§åˆ¶è¿›å…¥",
  "æ”¹è¿›åé¢‘ç‡": 1,
  "æ”¹è¿›åå¼ºåº¦": 2,
  "æ”¹è¿›åTå€¼": 2,
  "Tå€¼é™ä½ç‡": 80.00
}
"""
    
    # ì–¸ì–´ë³„ JSON ì¶œë ¥ í‚¤ ì´ë¦„
    json_keys = {
        "Korean": {
            "improvement": "ê°œì„ ëŒ€ì±…",
            "improved_freq": "ê°œì„  í›„ ë¹ˆë„",
            "improved_intensity": "ê°œì„  í›„ ê°•ë„",
            "improved_t": "ê°œì„  í›„ T",
            "reduction_rate": "T ê°ì†Œìœ¨"
        },
        "English": {
            "improvement": "improvement_plan",
            "improved_freq": "improved_frequency",
            "improved_intensity": "improved_intensity",
            "improved_t": "improved_T",
            "reduction_rate": "reduction_rate"
        },
        "Chinese": {
            "improvement": "æ”¹è¿›æªæ–½",
            "improved_freq": "æ”¹è¿›åé¢‘ç‡",
            "improved_intensity": "æ”¹è¿›åå¼ºåº¦", 
            "improved_t": "æ”¹è¿›åTå€¼",
            "reduction_rate": "Tå€¼é™ä½ç‡"
        }
    }
    
    # ê° ì–¸ì–´ë³„ ì•ˆë‚´ ë©”ì‹œì§€
    instructions = {
        "Korean": {
            "new_input": "ë‹¤ìŒì€ ìƒˆë¡œìš´ ì…ë ¥ì…ë‹ˆë‹¤:",
            "input_activity": "ì…ë ¥ (ì‘ì—…í™œë™): ",
            "input_hazard": "ì…ë ¥ (ìœ í•´ìœ„í—˜ìš”ì¸): ",
            "input_freq": "ì…ë ¥ (ì›ë˜ ë¹ˆë„): ",
            "input_intensity": "ì…ë ¥ (ì›ë˜ ê°•ë„): ",
            "input_t": "ì…ë ¥ (ì›ë˜ T): ",
            "output_format": "ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ì„ ì œê³µí•˜ì„¸ìš”:",
            "improvement_write": "ê°œì„ ëŒ€ì±…(ê°œì„ ëŒ€ì±…)ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.",
            "provide_measures": "ìµœì†Œ 3ê°œì˜ êµ¬ì²´ì ì¸ ê°œì„  ì¡°ì¹˜ë¥¼ ë²ˆí˜¸ê°€ ë§¤ê²¨ì§„ ëª©ë¡ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”.",
            "valid_json": "ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•˜ë„ë¡ í•˜ì„¸ìš”.",
            "output": "ì¶œë ¥:"
        },
        "English": {
            "new_input": "Now here is a new input:",
            "input_activity": "Input (Activity): ",
            "input_hazard": "Input (Hazard): ",
            "input_freq": "Input (Original Frequency): ",
            "input_intensity": "Input (Original Intensity): ",
            "input_t": "Input (Original T): ",
            "output_format": "Please provide the output in JSON format with these keys:",
            "improvement_write": "Please write the improvement measures (improvement_plan) in English.",
            "provide_measures": "Provide at least 3 specific improvement measures as a numbered list.",
            "valid_json": "Make sure to return only valid JSON.",
            "output": "Output:"
        },
        "Chinese": {
            "new_input": "ä»¥ä¸‹æ˜¯æ–°çš„è¾“å…¥:",
            "input_activity": "è¾“å…¥ (å·¥ä½œæ´»åŠ¨): ",
            "input_hazard": "è¾“å…¥ (å±å®³): ",
            "input_freq": "è¾“å…¥ (åŸé¢‘ç‡): ",
            "input_intensity": "è¾“å…¥ (åŸå¼ºåº¦): ",
            "input_t": "è¾“å…¥ (åŸTå€¼): ",
            "output_format": "è¯·ä»¥ä»¥ä¸‹JSONæ ¼å¼æä¾›è¾“å‡º:",
            "improvement_write": "è¯·ç”¨ä¸­æ–‡ç¼–å†™æ”¹è¿›æªæ–½(æ”¹è¿›æªæ–½)ã€‚",
            "provide_measures": "æä¾›è‡³å°‘3é¡¹å…·ä½“çš„æ”¹è¿›æªæ–½ï¼Œåˆ—ä¸ºç¼–å·åˆ—è¡¨ã€‚",
            "valid_json": "è¯·ç¡®ä¿åªè¿”å›æœ‰æ•ˆçš„JSONã€‚",
            "output": "è¾“å‡º:"
        }
    }
    
    # í˜„ì¬ ì–¸ì–´ì˜ í‚¤ì™€ ì•ˆë‚´ ë©”ì‹œì§€
    keys = json_keys.get(target_language, json_keys["Korean"])
    instr = instructions.get(target_language, instructions["Korean"])
    
    # ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = (
        f"{example_section}"
        f"{instr['new_input']}\n"
        f"{instr['input_activity']}{activity_text}\n"
        f"{instr['input_hazard']}{hazard_text}\n"
        f"{instr['input_freq']}{freq}\n"
        f"{instr['input_intensity']}{intensity}\n"
        f"{instr['input_t']}{T}\n\n"
        f"{instr['output_format']}\n"
        "{\n"
        f'  "{keys["improvement"]}": "í•­ëª©ë³„ ê°œì„ ëŒ€ì±… ë¦¬ìŠ¤íŠ¸", \n'
        f'  "{keys["improved_freq"]}": (an integer in [1..5]),\n'
        f'  "{keys["improved_intensity"]}": (an integer in [1..5]),\n'
        f'  "{keys["improved_t"]}": (Improved Frequency * Improved Severity),\n'
        f'  "{keys["reduction_rate"]}": (percentage of risk reduction)\n'
        "}\n\n"
        f"{instr['improvement_write']}\n"
        f"{instr['provide_measures']}\n"
        f"{instr['valid_json']}\n"
        f"{instr['output']}\n"
    )
    
    return prompt

# GPT ì‘ë‹µ íŒŒì‹± (Phase 2)
def parse_gpt_output_phase2(gpt_output, language="Korean"):
    """GPT ì‘ë‹µì—ì„œ JSON ë°ì´í„°ë¥¼ ì¶”ì¶œ"""
    try:
        # JSON ë¸”ë¡ì´ ìˆëŠ” ê²½ìš° ì¶”ì¶œ ì‹œë„
        pattern = re.compile(r"```json(.*?)```", re.DOTALL)
        match = pattern.search(gpt_output)

        if match:
            json_str = match.group(1).strip()
        else:
            # JSON ë¸”ë¡ í‘œì‹œê°€ ì—†ëŠ” ê²½ìš° ì›ë¬¸ì„ JSONìœ¼ë¡œ íŒŒì‹± ì‹œë„
            json_str = gpt_output.replace("```", "").strip()

        import json
        result = json.loads(json_str)
        
        # ì–¸ì–´ë³„ í‚¤ ë§¤í•‘
        key_mappings = {
            "Korean": {
                "improvement": ["ê°œì„ ëŒ€ì±…"],
                "improved_freq": ["ê°œì„  í›„ ë¹ˆë„", "ê°œì„ ë¹ˆë„"],
                "improved_intensity": ["ê°œì„  í›„ ê°•ë„", "ê°œì„ ê°•ë„"],
                "improved_t": ["ê°œì„  í›„ T", "ê°œì„ T", "ê°œì„  í›„ t"],
                "reduction_rate": ["T ê°ì†Œìœ¨", "ê°ì†Œìœ¨", "ìœ„í—˜ ê°ì†Œìœ¨"]
            },
            "English": {
                "improvement": ["improvement_plan", "improvement_measures", "improvements"],
                "improved_freq": ["improved_frequency", "new_frequency", "frequency_after"],
                "improved_intensity": ["improved_intensity", "new_intensity", "intensity_after"],
                "improved_t": ["improved_T", "new_T", "T_after"],
                "reduction_rate": ["reduction_rate", "risk_reduction_rate", "rrr"]
            },
            "Chinese": {
                "improvement": ["æ”¹è¿›æªæ–½", "æ”¹è¿›è®¡åˆ’", "æ”¹å–„æªæ–½"],
                "improved_freq": ["æ”¹è¿›åé¢‘ç‡", "æ–°é¢‘ç‡", "é¢‘ç‡æ”¹è¿›å"],
                "improved_intensity": ["æ”¹è¿›åå¼ºåº¦", "æ–°å¼ºåº¦", "å¼ºåº¦æ”¹è¿›å"],
                "improved_t": ["æ”¹è¿›åTå€¼", "æ–°Tå€¼", "Tå€¼æ”¹è¿›å"],
                "reduction_rate": ["Tå€¼é™ä½ç‡", "é£é™©é™ä½ç‡", "é™ä½ç‡"]
            }
        }
        
        # ê²°ê³¼ ë§¤í•‘
        mapped_result = {}
        
        # í˜„ì¬ ì–¸ì–´ì˜ í‚¤ ë§¤í•‘ ê°€ì ¸ì˜¤ê¸°
        mappings = key_mappings.get(language, key_mappings["Korean"])
        
        # ê°œì„ ëŒ€ì±… í‚¤ ë§¤í•‘
        for result_key, possible_keys in mappings.items():
            for key in possible_keys:
                if key in result:
                    mapped_result[result_key] = result[key]
                    break
            
        return mapped_result
    except Exception as e:
        st.error(f"JSON íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.write("ì›ë³¸ GPT ì‘ë‹µ:", gpt_output)
        return None
# -----------------------------------------------------------------------------
# ---------------------------  Overview íƒ­ ------------------------------------
# -----------------------------------------------------------------------------
with tabs[0]:
    st.markdown(f'<div class="sub-header">{texts["overview_header"]}</div>', unsafe_allow_html=True)
    colA,colB=st.columns([3,2])
    with colA: st.markdown(f"<div class='info-text'>{texts['overview_text']}</div>",unsafe_allow_html=True)
    with colB:
        st.markdown(f"<div style='text-align:center;'><b>{texts['process_title']}</b></div>",unsafe_allow_html=True)

# -----------------------------------------------------------------------------
# --------------  Risk Assessment í†µí•© íƒ­ --------------------------------------
# -----------------------------------------------------------------------------
with tabs[1]:
    st.markdown(f'<div class="sub-header">{texts["tab_phase1"]} & {texts["tab_phase2"]}</div>', unsafe_allow_html=True)

    # â‘  API Key & Dataset â¤µï¸ ---------------------------------------------
    api_key = st.text_input(texts['api_key_label'], type='password', key='api_key_all')
    dataset_name = st.selectbox(texts['dataset_label'], [
        "SWRO ê±´ì¶•ê³µì • (ê±´ì¶•)","Civil (í† ëª©)","Marine (í† ëª©)","SWRO ê¸°ê³„ê³µì‚¬ (í”ŒëœíŠ¸)","SWRO ì „ê¸°ì‘ì—…í‘œì¤€ (í”ŒëœíŠ¸)"], key='dataset_all')
    if ss.retriever_pool_df is None or st.button(texts['load_data_btn']):
        if not api_key: st.warning(texts['api_key_warning'])
        else:
            with st.spinner(texts['data_loading']):
                df=load_data(dataset_name.split(' ')[0])
                train_df,_=train_test_split(df, test_size=0.1, random_state=42)
                pool_df=train_df.copy(); pool_df['content']=pool_df.apply(lambda r:' '.join(r.values.astype(str)),axis=1)
                to_embed=pool_df['content'].tolist(); max_texts=min(len(to_embed),20)
                st.info(texts['demo_limit_info'].format(max_texts=max_texts))
                embeds=embed_texts_with_openai(to_embed[:max_texts], api_key=api_key)
                vecs=np.array(embeds,dtype='float32'); dim=vecs.shape[1]; index=faiss.IndexFlatL2(dim); index.add(vecs)
                ss.index=index; ss.embeddings=vecs; ss.retriever_pool_df=pool_df.iloc[:max_texts]
                st.success(texts['data_load_success'].format(max_texts=max_texts))

    st.divider()

    # â‘¡ Work Activity ì…ë ¥ âœ Phase1 & Phase2 ìë™ ì‹¤í–‰ ------------------
    activity = st.text_input(texts['activity_label'], key='user_activity')
    run_button = st.button("ğŸš€ ì‹¤í–‰ / Run")
    if run_button and activity:
        if not api_key:
            st.warning(texts['api_key_warning'])
        elif ss.index is None:
            st.warning(texts['load_first_warning'])
        else:
            with st.spinner("Processing â€¦"):
                # ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ -------------------------------------------
                q_emb=embed_texts_with_openai([activity], api_key=api_key)[0]
                D,I=ss.index.search(np.array([q_emb],dtype='float32'), k=min(3,len(ss.retriever_pool_df)))
                sim_docs=ss.retriever_pool_df.iloc[I[0]]

                # Hazard ì˜ˆì¸¡ ---------------------------------------------
                p1_prompt=construct_prompt_phase1_hazard(sim_docs, activity, ss.language)
                hazard=generate_with_gpt(p1_prompt, api_key, ss.language)

                # ë¹ˆë„Â·ê°•ë„ ì˜ˆì¸¡ -------------------------------------------
                p1b_prompt=construct_prompt_phase1_risk(sim_docs, activity, hazard, ss.language)
                risk_json=generate_with_gpt(p1b_prompt, api_key, ss.language)
                parse=parse_gpt_output_phase1(risk_json, ss.language)
                if not parse: st.error(texts['parsing_error']); st.stop()
                freq,inten,T=parse; grade=determine_grade(T)

                # ê°œì„ ëŒ€ì±… ìƒì„± -------------------------------------------
                p2_prompt=construct_prompt_phase2(sim_docs,activity,hazard,freq,inten,T,ss.language)
                p2_out=generate_with_gpt(p2_prompt, api_key, ss.language)
                parsed2=parse_gpt_output_phase2(p2_out, ss.language)
                if not parsed2: st.error(texts['parsing_error_improvement']); st.stop()

                imp_plan=parsed2.get('improvement','')
                imp_freq=parsed2.get('improved_freq',1)
                imp_int=parsed2.get('improved_intensity',1)
                imp_T=parsed2.get('improved_t',imp_freq*imp_int)
                rrr=parsed2.get('reduction_rate', (T-imp_T)/T*100 if T else 0)

                # --------------------- ì¶œë ¥ -----------------------------
                st.markdown(f"### {texts['similar_cases_header']}")
                for i,(_,doc) in enumerate(sim_docs.iterrows(),1):
                    imp_field=[c for c in doc.index if re.search('ê°œì„ ëŒ€ì±…|Improvement|æ”¹è¿›',c)]
                    imp_text=doc[imp_field[0]] if imp_field else ''
                    st.markdown(texts['similar_case_text'].format(i=i,activity=doc['ì‘ì—…í™œë™ ë° ë‚´ìš©'],hazard=doc['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥'],freq=doc['ë¹ˆë„'],intensity=doc['ê°•ë„'],t_value=doc['T'],grade=doc['ë“±ê¸‰']),unsafe_allow_html=True)
                    if imp_text:
                        st.markdown(f"<div style='margin-left:20px;'>{imp_text}</div>", unsafe_allow_html=True)

                result_df=pd.DataFrame({texts['result_table_columns'][0]:texts['result_table_rows'],texts['result_table_columns'][1]:[freq,inten,T,grade]})
                comp_df=pd.DataFrame({texts['comparison_columns'][0]:texts['result_table_rows'],texts['comparison_columns'][1]:[freq,inten,T,grade],texts['comparison_columns'][2]:[imp_freq,imp_int,imp_T,determine_grade(imp_T)]})

                st.markdown(f"### {texts['prediction_result_header']}")
                st.markdown(texts['activity_result'].format(activity=activity))
                st.markdown(texts['hazard_result'].format(hazard=hazard))
                st.table(result_df)

                st.markdown(f"### {texts['improvement_result_header']}")
                colL,colR=st.columns([3,2])
                with colL:
                    st.markdown(f"##### {texts['improvement_plan_header']}")
                    st.markdown(imp_plan)
                with colR:
                    st.markdown(f"##### {texts['risk_improvement_header']}")
                    st.table(comp_df)
                    st.metric(label=texts['risk_reduction_label'], value=f"{rrr:.2f}%")

                # ---------------- Excel Export --------------------------
                def to_excel_bytes():
                    output=io.BytesIO()
                    with pd.ExcelWriter(output, engine='xlsxwriter') as wr:
                        result_df.to_excel(wr, sheet_name='Phase1', index=False)
                        comp_df.to_excel(wr, sheet_name='Phase2', index=False)
                    return output.getvalue()
                st.download_button("ğŸ“¥ ê²°ê³¼ Excel ë‹¤ìš´ë¡œë“œ", data=to_excel_bytes(), file_name="risk_assessment.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

                # Phase2 progress bars
                st.markdown(f"#### {texts['t_value_change_header']}")
                c1,c2=st.columns(2)
                with c1:
                    st.markdown(f"**{texts['before_improvement']}**")
                    st.progress(T/25)
                with c2:
                    st.markdown(f"**{texts['after_improvement']}**")
                    st.progress(imp_T/25)

# ------------------- í‘¸í„° ------------------------
st.markdown('<hr>', unsafe_allow_html=True)
footA,footB=st.columns(2)
with footA:
    if os.path.exists('cau.png'): st.image('cau.png', width=140)
with footB:
    if os.path.exists('doosan.png'): st.image('doosan.png', width=160)
