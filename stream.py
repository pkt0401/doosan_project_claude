import streamlit as st
import pandas as pd
import numpy as np
import faiss
import openai
import re
import os
from PIL import Image
from sklearn.model_selection import train_test_split

# ì–¸ì–´ ì„¤ì • í…ìŠ¤íŠ¸ ì •ì˜
system_texts = {
    "Korean": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "ì‹œìŠ¤í…œ ê°œìš”",
        "tab_phase1": "ìœ„í—˜ì„± í‰ê°€ (Phase 1)",
        "tab_phase2": "ê°œì„ ëŒ€ì±… ìƒì„± (Phase 2)",
        "overview_header": "LLM ê¸°ë°˜ ìœ„í—˜ì„±í‰ê°€ ì‹œìŠ¤í…œ",
        "overview_text": """
        LLM(Large Language Model)ì„ í™œìš©í•œ ìœ„í—˜ì„±í‰ê°€ ìë™í™” ì‹œìŠ¤í…œì€ ê±´ì„¤ í˜„ì¥ì˜ ì•ˆì „ ê´€ë¦¬ë¥¼ í˜ì‹ ì ìœ¼ë¡œ ê°œì„ í•©ë‹ˆë‹¤:
        
        1. <span class="highlight">ì‘ì—… ë‚´ìš© ì…ë ¥ ì‹œ ìƒì„±í˜• AIë¥¼ í†µí•œ 'ìœ í•´ìœ„í—˜ìš”ì¸' ìë™ ì˜ˆì¸¡ ë° ìœ„í—˜ ë“±ê¸‰ ì‚°ì •</span> <span class="phase-badge">Phase 1</span>
        2. <span class="highlight">ìœ„í—˜ë„ ê°ì†Œë¥¼ ìœ„í•œ ê°œì„ ëŒ€ì±… ìë™ ìƒì„± ë° ê°ì†Œìœ¨ ì˜ˆì¸¡</span> <span class="phase-badge">Phase 2</span>
        3. AIëŠ” ê±´ì„¤í˜„ì¥ì˜ ê¸°ì¡´ ìœ„í—˜ì„±í‰ê°€ë¥¼ ê³µì •ë³„ë¡œ êµ¬ë¶„í•˜ê³ , í•´ë‹¹ ìœ í•´ìœ„í—˜ìš”ì¸ì„ í•™ìŠµ
        4. ìë™ ìƒì„± ê¸°ìˆ  ê°œë°œ ì™„ë£Œ í›„ ìœ„í—˜ë„ ê¸°ë°˜ ì‚¬ê³ ìœ„í—˜ì„± ë¶„ì„ ë° ê°œì„ ëŒ€ì±… ìƒì„±
        
        ì´ ì‹œìŠ¤í…œì€ PIMS ë° ì•ˆì „ì§€í‚´ì´ ë“± EHS í”Œë«í¼ì— AI ê¸°ìˆ  íƒ‘ì¬ë¥¼ í†µí•´ í†µí•© ì‚¬ê³  ì˜ˆì¸¡ í”„ë¡œê·¸ë¨ìœ¼ë¡œ ë°œì „ ì˜ˆì •ì…ë‹ˆë‹¤.
        """,
        "process_title": "AI ìœ„í—˜ì„±í‰ê°€ í”„ë¡œì„¸ìŠ¤",
        "process_steps": ["ì‘ì—…ë‚´ìš© ì…ë ¥", "AI ìœ„í—˜ë¶„ì„", "ìœ í•´ìš”ì¸ ì˜ˆì¸¡", "ìœ„í—˜ë“±ê¸‰ ì‚°ì •", "ê°œì„ ëŒ€ì±… ìë™ìƒì„±", "ì•ˆì „ì¡°ì¹˜ ì ìš©"],
        "features_title": "ì‹œìŠ¤í…œ íŠ¹ì§• ë° êµ¬ì„±ìš”ì†Œ",
        "phase1_features": """
        #### Phase 1: ìœ„í—˜ì„± í‰ê°€ ìë™í™”
        - ê³µì •ë³„ ì‘ì—…í™œë™ì— ë”°ë¥¸ ìœ„í—˜ì„±í‰ê°€ ë°ì´í„° í•™ìŠµ
        - ì‘ì—…í™œë™ ì…ë ¥ ì‹œ ìœ í•´ìœ„í—˜ìš”ì¸ ìë™ ì˜ˆì¸¡
        - ìœ ì‚¬ ìœ„í—˜ìš”ì¸ ì‚¬ë¡€ ê²€ìƒ‰ ë° í‘œì‹œ
        - ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) ê¸°ë°˜ ìœ„í—˜ë„(ë¹ˆë„, ê°•ë„, T) ì¸¡ì •
        - Excel ê¸°ë°˜ ê³µì •ë³„ ìœ„í—˜ì„±í‰ê°€ ë°ì´í„° ìë™ ë¶„ì„
        - ìœ„í—˜ë“±ê¸‰(A-E) ìë™ ì‚°ì •
        """,
        "phase2_features": """
        #### Phase 2: ê°œì„ ëŒ€ì±… ìë™ ìƒì„±
        - ìœ„í—˜ìš”ì†Œë³„ ë§ì¶¤í˜• ê°œì„ ëŒ€ì±… ìë™ ìƒì„±
        - ë‹¤êµ­ì–´(í•œ/ì˜/ì¤‘) ê°œì„ ëŒ€ì±… ìƒì„± ì§€ì›
        - ê°œì„  ì „í›„ ìœ„í—˜ë„(T) ìë™ ë¹„êµ ë¶„ì„
        - ìœ„í—˜ ê°ì†Œìœ¨(RRR) ì •ëŸ‰ì  ì‚°ì¶œ
        - ê³µì¢…/ê³µì •ë³„ ìµœì  ê°œì„ ëŒ€ì±… ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
        """,
        "phase1_header": "ìœ„í—˜ì„± í‰ê°€ ìë™í™” (Phase 1)",
        "api_key_label": "OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
        "dataset_label": "ë°ì´í„°ì…‹ ì„ íƒ",
        "load_data_label": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±",
        "load_data_btn": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±",
        "api_key_warning": "ê³„ì†í•˜ë ¤ë©´ OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.",
        "data_loading": "ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì¸ë±ìŠ¤ë¥¼ êµ¬ì„±í•˜ëŠ” ì¤‘...",
        "demo_limit_info": "ë°ëª¨ ëª©ì ìœ¼ë¡œ {max_texts}ê°œì˜ í…ìŠ¤íŠ¸ë§Œ ì„ë² ë”©í•©ë‹ˆë‹¤. ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.",
        "data_load_success": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„± ì™„ë£Œ! (ì´ {max_texts}ê°œ í•­ëª© ì²˜ë¦¬)",
        "hazard_prediction_header": "ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡",
        "load_first_warning": "ë¨¼ì € [ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±] ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.",
        "activity_label": "ì‘ì—…í™œë™:",
        "predict_hazard_btn": "ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡í•˜ê¸°",
        "activity_warning": "ì‘ì—…í™œë™ì„ ì…ë ¥í•˜ì„¸ìš”.",
        "predicting_hazard": "ìœ í•´ìœ„í—˜ìš”ì¸ì„ ì˜ˆì¸¡í•˜ëŠ” ì¤‘...",
        "similar_cases_header": "ìœ ì‚¬í•œ ì‚¬ë¡€",
        "similar_case_text": """
        <div class="similar-case">
            <strong>ì‚¬ë¡€ {i}</strong><br>
            <strong>ì‘ì—…í™œë™:</strong> {activity}<br>
            <strong>ìœ í•´ìœ„í—˜ìš”ì¸:</strong> {hazard}<br>
            <strong>ìœ„í—˜ë„:</strong> ë¹ˆë„ {freq}, ê°•ë„ {intensity}, Tê°’ {t_value} (ë“±ê¸‰ {grade})
        </div>
        """,
        "prediction_result_header": "ì˜ˆì¸¡ ê²°ê³¼",
        "activity_result": "ì‘ì—…í™œë™: {activity}",
        "hazard_result": "ì˜ˆì¸¡ëœ ìœ í•´ìœ„í—˜ìš”ì¸: {hazard}",
        "result_table_columns": ["í•­ëª©", "ê°’"],
        "result_table_rows": ["ë¹ˆë„", "ê°•ë„", "T ê°’", "ìœ„í—˜ë“±ê¸‰"],
        "parsing_error": "ìœ„í—˜ì„± í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        "gpt_response": "GPT ì›ë¬¸ ì‘ë‹µ: {response}",
        "phase2_header": "ê°œì„ ëŒ€ì±… ìë™ ìƒì„± (Phase 2)",
        "language_select_label": "ê°œì„ ëŒ€ì±… ì–¸ì–´ ì„ íƒ:",
        "input_method_label": "ì…ë ¥ ë°©ì‹ ì„ íƒ:",
        "input_methods": ["Phase 1 í‰ê°€ ê²°ê³¼ ì‚¬ìš©", "ì§ì ‘ ì…ë ¥"],
        "phase1_results_header": "Phase 1 í‰ê°€ ê²°ê³¼",
        "risk_level_text": "ìœ„í—˜ë„: ë¹ˆë„ {freq}, ê°•ë„ {intensity}, Tê°’ {t_value} (ë“±ê¸‰ {grade})",
        "phase1_first_warning": "ë¨¼ì € Phase 1ì—ì„œ ìœ„í—˜ì„± í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”.",
        "hazard_label": "ìœ í•´ìœ„í—˜ìš”ì¸:",
        "frequency_label": "ë¹ˆë„ (1-5):",
        "intensity_label": "ê°•ë„ (1-5):",
        "t_value_text": "Tê°’: {t_value} (ë“±ê¸‰: {grade})",
        "generate_improvement_btn": "ê°œì„ ëŒ€ì±… ìƒì„±",
        "generating_improvement": "ê°œì„ ëŒ€ì±…ì„ ìƒì„±í•˜ëŠ” ì¤‘...",
        "no_data_warning": "Phase 1ì—ì„œ ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±ì„ ì™„ë£Œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì˜ˆì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
        "improvement_result_header": "ê°œì„ ëŒ€ì±… ìƒì„± ê²°ê³¼",
        "improvement_plan_header": "ê°œì„ ëŒ€ì±…",
        "risk_improvement_header": "ìœ„í—˜ë„ ê°œì„  ê²°ê³¼",
        "comparison_columns": ["í•­ëª©", "ê°œì„  ì „", "ê°œì„  í›„"],
        "risk_reduction_label": "ìœ„í—˜ ê°ì†Œìœ¨ (RRR)",
        "t_value_change_header": "ìœ„í—˜ë„(Tê°’) ë³€í™”",
        "before_improvement": "ê°œì„  ì „ Tê°’:",
        "after_improvement": "ê°œì„  í›„ Tê°’:",
        "parsing_error_improvement": "ê°œì„ ëŒ€ì±… ìƒì„± ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    },
    "English": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "System Overview",
        "tab_phase1": "Risk Assessment (Phase 1)",
        "tab_phase2": "Improvement Measures (Phase 2)",
        "overview_header": "LLM-based Risk Assessment System",
        "overview_text": """
        The risk assessment automation system using LLM (Large Language Model) innovatively improves safety management at construction sites:
        
        1. <span class="highlight">Automatic prediction of 'hazards' and risk level calculation through generative AI</span> <span class="phase-badge">Phase 1</span>
        2. <span class="highlight">Automatic generation of improvement measures and reduction rate prediction to reduce risk level</span> <span class="phase-badge">Phase 2</span>
        3. AI learns existing risk assessments at construction sites by process and their hazard factors
        4. After the development of automatic generation technology, risk analysis and improvement measures based on risk level
        
        This system is expected to evolve into an integrated accident prediction program through the incorporation of AI technology into EHS platforms such as PIMS and Safety Guardian.
        """,
        "process_title": "AI Risk Assessment Process",
        "process_steps": ["Work Input", "AI Risk Analysis", "Hazard Prediction", "Risk Level Calculation", "Auto-generate Improvements", "Safety Measures"],
        "features_title": "System Features and Components",
        "phase1_features": """
        #### Phase 1: Risk Assessment Automation
        - Learning risk assessment data according to work activities by process
        - Automatic hazard prediction when work activities are entered
        - Similar case search and display
        - Risk level (frequency, intensity, T) measurement based on large language models (LLM)
        - Automatic analysis of Excel-based process-specific risk assessment data
        - Automatic risk grade (A-E) calculation
        """,
        "phase2_features": """
        #### Phase 2: Automatic Generation of Improvement Measures
        - Automatic generation of customized improvement measures for risk factors
        - Multilingual (Korean/English/Chinese) improvement measure generation support
        - Automatic comparative analysis of risk level (T) before and after improvement
        - Quantitative calculation of Risk Reduction Rate (RRR)
        - Building a database of optimal improvement measures by work type/process
        """,
        "phase1_header": "Risk Assessment Automation (Phase 1)",
        "api_key_label": "Enter OpenAI API Key:",
        "dataset_label": "Select Dataset",
        "load_data_label": "Load Data and Configure Index",
        "load_data_btn": "Load Data and Configure Index",
        "api_key_warning": "Please enter an OpenAI API key to continue.",
        "data_loading": "Loading data and configuring index...",
        "demo_limit_info": "For demo purposes, only embedding {max_texts} texts. In a real environment, all data should be processed.",
        "data_load_success": "Data load and index configuration complete! (Total {max_texts} items processed)",
        "hazard_prediction_header": "Hazard Prediction",
        "load_first_warning": "Please click the [Load Data and Configure Index] button first.",
        "activity_label": "Work Activity:",
        "predict_hazard_btn": "Predict Hazards",
        "activity_warning": "Please enter a work activity.",
        "predicting_hazard": "Predicting hazards...",
        "similar_cases_header": "Similar Cases",
        "similar_case_text": """
        <div class="similar-case">
            <strong>Case {i}</strong><br>
            <strong>Work Activity:</strong> {activity}<br>
            <strong>Hazard:</strong> {hazard}<br>
            <strong>Risk Level:</strong> Frequency {freq}, Intensity {intensity}, T-value {t_value} (Grade {grade})
        </div>
        """,
        "prediction_result_header": "Prediction Results",
        "activity_result": "Work Activity: {activity}",
        "hazard_result": "Predicted Hazard: {hazard}",
        "result_table_columns": ["Item", "Value"],
        "result_table_rows": ["Frequency", "Intensity", "T Value", "Risk Grade"],
        "parsing_error": "Unable to parse risk assessment results.",
        "gpt_response": "Original GPT Response: {response}",
        "phase2_header": "Automatic Generation of Improvement Measures (Phase 2)",
        "language_select_label": "Select Language for Improvement Measures:",
        "input_method_label": "Select Input Method:",
        "input_methods": ["Use Phase 1 Assessment Results", "Direct Input"],
        "phase1_results_header": "Phase 1 Assessment Results",
        "risk_level_text": "Risk Level: Frequency {freq}, Intensity {intensity}, T-value {t_value} (Grade {grade})",
        "phase1_first_warning": "Please perform a risk assessment in Phase 1 first.",
        "hazard_label": "Hazard:",
        "frequency_label": "Frequency (1-5):",
        "intensity_label": "Intensity (1-5):",
        "t_value_text": "T-value: {t_value} (Grade: {grade})",
        "generate_improvement_btn": "Generate Improvement Measures",
        "generating_improvement": "Generating improvement measures...",
        "no_data_warning": "Data loading and index configuration was not completed in Phase 1. Using basic examples.",
        "improvement_result_header": "Improvement Measure Generation Results",
        "improvement_plan_header": "Improvement Measures",
        "risk_improvement_header": "Risk Level Improvement Results",
        "comparison_columns": ["Item", "Before Improvement", "After Improvement"],
        "risk_reduction_label": "Risk Reduction Rate (RRR)",
        "t_value_change_header": "Risk Level (T-value) Change",
        "before_improvement": "T-value Before Improvement:",
        "after_improvement": "T-value After Improvement:",
        "parsing_error_improvement": "Unable to parse improvement measure generation results."
    },
    "Chinese": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "ç³»ç»Ÿæ¦‚è¿°",
        "tab_phase1": "é£é™©è¯„ä¼° (ç¬¬1é˜¶æ®µ)",
        "tab_phase2": "æ”¹è¿›æªæ–½ (ç¬¬2é˜¶æ®µ)",
        "overview_header": "åŸºäºLLMçš„é£é™©è¯„ä¼°ç³»ç»Ÿ",
        "overview_text": """
        ä½¿ç”¨LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰çš„é£é™©è¯„ä¼°è‡ªåŠ¨åŒ–ç³»ç»Ÿåˆ›æ–°åœ°æ”¹å–„äº†å»ºç­‘å·¥åœ°çš„å®‰å…¨ç®¡ç†ï¼š
        
        1. <span class="highlight">é€šè¿‡ç”Ÿæˆå¼AIè‡ªåŠ¨é¢„æµ‹"å±å®³å› ç´ "å¹¶è®¡ç®—é£é™©ç­‰çº§</span> <span class="phase-badge">ç¬¬1é˜¶æ®µ</span>
        2. <span class="highlight">è‡ªåŠ¨ç”Ÿæˆæ”¹è¿›æªæ–½å¹¶é¢„æµ‹é™ä½é£é™©çš„æ¯”ç‡</span> <span class="phase-badge">ç¬¬2é˜¶æ®µ</span>
        3. AIæŒ‰å·¥åºå­¦ä¹ å»ºç­‘å·¥åœ°çš„ç°æœ‰é£é™©è¯„ä¼°åŠå…¶å±å®³å› ç´ 
        4. åœ¨è‡ªåŠ¨ç”ŸæˆæŠ€æœ¯å¼€å‘å®Œæˆåï¼ŒåŸºäºé£é™©ç­‰çº§è¿›è¡Œé£é™©åˆ†æå’Œæ”¹è¿›æªæ–½ç”Ÿæˆ
        
        è¯¥ç³»ç»Ÿæœ‰æœ›é€šè¿‡å°†AIæŠ€æœ¯æ•´åˆåˆ°EHSå¹³å°ï¼ˆå¦‚PIMSå’Œå®‰å…¨å«å£«ï¼‰ä¸­ï¼Œå‘å±•æˆä¸ºä¸€ä¸ªç»¼åˆäº‹æ•…é¢„æµ‹ç¨‹åºã€‚
        """,
        "process_title": "AIé£é™©è¯„ä¼°æµç¨‹",
        "process_steps": ["å·¥ä½œè¾“å…¥", "AIé£é™©åˆ†æ", "å±å®³é¢„æµ‹", "é£é™©ç­‰çº§è®¡ç®—", "è‡ªåŠ¨ç”Ÿæˆæ”¹è¿›æªæ–½", "å®‰å…¨æªæ–½"],
        "features_title": "ç³»ç»Ÿç‰¹ç‚¹å’Œç»„ä»¶",
        "phase1_features": """
        #### ç¬¬1é˜¶æ®µï¼šé£é™©è¯„ä¼°è‡ªåŠ¨åŒ–
        - æŒ‰å·¥åºå­¦ä¹ ä¸å·¥ä½œæ´»åŠ¨ç›¸å…³çš„é£é™©è¯„ä¼°æ•°æ®
        - è¾“å…¥å·¥ä½œæ´»åŠ¨æ—¶è‡ªåŠ¨é¢„æµ‹å±å®³å› ç´ 
        - ç›¸ä¼¼æ¡ˆä¾‹æœç´¢å’Œæ˜¾ç¤º
        - åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„é£é™©ç­‰çº§ï¼ˆé¢‘ç‡ã€å¼ºåº¦ã€Tå€¼ï¼‰æµ‹é‡
        - è‡ªåŠ¨åˆ†æåŸºäºExcelçš„ç‰¹å®šå·¥åºé£é™©è¯„ä¼°æ•°æ®
        - è‡ªåŠ¨è®¡ç®—é£é™©ç­‰çº§(A-E)
        """,
        "phase2_features": """
        #### ç¬¬2é˜¶æ®µï¼šè‡ªåŠ¨ç”Ÿæˆæ”¹è¿›æªæ–½
        - ä¸ºé£é™©å› ç´ è‡ªåŠ¨ç”Ÿæˆå®šåˆ¶çš„æ”¹è¿›æªæ–½
        - å¤šè¯­è¨€ï¼ˆéŸ©è¯­/è‹±è¯­/ä¸­æ–‡ï¼‰æ”¹è¿›æªæ–½ç”Ÿæˆæ”¯æŒ
        - æ”¹è¿›å‰åé£é™©ç­‰çº§ï¼ˆTå€¼ï¼‰çš„è‡ªåŠ¨æ¯”è¾ƒåˆ†æ
        - é£é™©é™ä½ç‡(RRR)çš„å®šé‡è®¡ç®—
        - å»ºç«‹æŒ‰å·¥ä½œç±»å‹/å·¥åºçš„æœ€ä½³æ”¹è¿›æªæ–½æ•°æ®åº“
        """,
        "phase1_header": "é£é™©è¯„ä¼°è‡ªåŠ¨åŒ– (ç¬¬1é˜¶æ®µ)",
        "api_key_label": "è¾“å…¥OpenAI APIå¯†é’¥ï¼š",
        "dataset_label": "é€‰æ‹©æ•°æ®é›†",
        "load_data_label": "åŠ è½½æ•°æ®å’Œé…ç½®ç´¢å¼•",
        "load_data_btn": "åŠ è½½æ•°æ®å’Œé…ç½®ç´¢å¼•",
        "api_key_warning": "è¯·è¾“å…¥OpenAI APIå¯†é’¥ä»¥ç»§ç»­ã€‚",
        "data_loading": "æ­£åœ¨åŠ è½½æ•°æ®å’Œé…ç½®ç´¢å¼•...",
        "demo_limit_info": "å‡ºäºæ¼”ç¤ºç›®çš„ï¼Œä»…åµŒå…¥{max_texts}ä¸ªæ–‡æœ¬ã€‚åœ¨å®é™…ç¯å¢ƒä¸­ï¼Œåº”å¤„ç†æ‰€æœ‰æ•°æ®ã€‚",
        "data_load_success": "æ•°æ®åŠ è½½å’Œç´¢å¼•é…ç½®å®Œæˆï¼ï¼ˆå…±å¤„ç†{max_texts}ä¸ªé¡¹ç›®ï¼‰",
        "hazard_prediction_header": "å±å®³é¢„æµ‹",
        "load_first_warning": "è¯·å…ˆç‚¹å‡»[åŠ è½½æ•°æ®å’Œé…ç½®ç´¢å¼•]æŒ‰é’®ã€‚",
        "activity_label": "å·¥ä½œæ´»åŠ¨ï¼š",
        "predict_hazard_btn": "é¢„æµ‹å±å®³",
        "activity_warning": "è¯·è¾“å…¥å·¥ä½œæ´»åŠ¨ã€‚",
        "predicting_hazard": "æ­£åœ¨é¢„æµ‹å±å®³...",
        "similar_cases_header": "ç›¸ä¼¼æ¡ˆä¾‹",
        "similar_case_text": """
        <div class="similar-case">
            <strong>æ¡ˆä¾‹ {i}</strong><br>
            <strong>å·¥ä½œæ´»åŠ¨ï¼š</strong> {activity}<br>
            <strong>å±å®³ï¼š</strong> {hazard}<br>
            <strong>é£é™©ç­‰çº§ï¼š</strong> é¢‘ç‡ {freq}, å¼ºåº¦ {intensity}, Tå€¼ {t_value} (ç­‰çº§ {grade})
        </div>
        """,
        "prediction_result_header": "é¢„æµ‹ç»“æœ",
        "activity_result": "å·¥ä½œæ´»åŠ¨: {activity}",
        "hazard_result": "é¢„æµ‹çš„å±å®³: {hazard}",
        "result_table_columns": ["é¡¹ç›®", "å€¼"],
        "result_table_rows": ["é¢‘ç‡", "å¼ºåº¦", "Tå€¼", "é£é™©ç­‰çº§"],
        "parsing_error": "æ— æ³•è§£æé£é™©è¯„ä¼°ç»“æœã€‚",
        "gpt_response": "åŸå§‹GPTå“åº”: {response}",
        "phase2_header": "è‡ªåŠ¨ç”Ÿæˆæ”¹è¿›æªæ–½ (ç¬¬2é˜¶æ®µ)",
        "language_select_label": "é€‰æ‹©æ”¹è¿›æªæ–½çš„è¯­è¨€ï¼š",
        "input_method_label": "é€‰æ‹©è¾“å…¥æ–¹æ³•ï¼š",
        "input_methods": ["ä½¿ç”¨ç¬¬1é˜¶æ®µè¯„ä¼°ç»“æœ", "ç›´æ¥è¾“å…¥"],
        "phase1_results_header": "ç¬¬1é˜¶æ®µè¯„ä¼°ç»“æœ",
        "risk_level_text": "é£é™©ç­‰çº§: é¢‘ç‡ {freq}, å¼ºåº¦ {intensity}, Tå€¼ {t_value} (ç­‰çº§ {grade})",
        "phase1_first_warning": "è¯·å…ˆåœ¨ç¬¬1é˜¶æ®µè¿›è¡Œé£é™©è¯„ä¼°ã€‚",
        "hazard_label": "å±å®³ï¼š",
        "frequency_label": "é¢‘ç‡ (1-5)ï¼š",
        "intensity_label": "å¼ºåº¦ (1-5)ï¼š",
        "t_value_text": "Tå€¼: {t_value} (ç­‰çº§: {grade})",
        "generate_improvement_btn": "ç”Ÿæˆæ”¹è¿›æªæ–½",
        "generating_improvement": "æ­£åœ¨ç”Ÿæˆæ”¹è¿›æªæ–½...",
        "no_data_warning": "åœ¨ç¬¬1é˜¶æ®µæœªå®Œæˆæ•°æ®åŠ è½½å’Œç´¢å¼•é…ç½®ã€‚ä½¿ç”¨åŸºæœ¬ç¤ºä¾‹ã€‚",
        "improvement_result_header": "æ”¹è¿›æªæ–½ç”Ÿæˆç»“æœ",
        "improvement_plan_header": "æ”¹è¿›æªæ–½",
        "risk_improvement_header": "é£é™©ç­‰çº§æ”¹è¿›ç»“æœ",
        "comparison_columns": ["é¡¹ç›®", "æ”¹è¿›å‰", "æ”¹è¿›å"],
        "risk_reduction_label": "é£é™©é™ä½ç‡ (RRR)",
        "t_value_change_header": "é£é™©ç­‰çº§ (Tå€¼) å˜åŒ–",
        "before_improvement": "æ”¹è¿›å‰Tå€¼ï¼š",
        "after_improvement": "æ”¹è¿›åTå€¼ï¼š",
        "parsing_error_improvement": "æ— æ³•è§£ææ”¹è¿›æªæ–½ç”Ÿæˆç»“æœã€‚"
    }
}

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Artificial Intelligence Risk Assessment",
    page_icon="ğŸ› ï¸",
    layout="wide"
)

# ìŠ¤íƒ€ì¼ ì ìš©
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E88E5;
        text-align: center;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.8rem;
        color: #0D47A1;
        margin-top: 2rem;
        margin-bottom: 1rem;
    }
    .metric-container {
        background-color: #f0f2f6;
        border-radius: 10px;
        padding: 20px;
        box-shadow: 2px 2px 5px rgba(0,0,0,0.1);
    }
    .info-text {
        font-size: 1rem;
        color: #424242;
        margin-bottom: 1rem;
    }
    .highlight {
        background-color: #e3f2fd;
        padding: 5px;
        border-radius: 5px;
    }
    .result-box {
        background-color: #f8f9fa;
        border-radius: 10px;
        padding: 15px;
        margin-top: 10px;
        margin-bottom: 10px;
        border-left: 5px solid #4CAF50;
    }
    .phase-badge {
        background-color: #4CAF50;
        color: white;
        padding: 5px 10px;
        border-radius: 15px;
        font-size: 0.8rem;
        margin-right: 10px;
    }
    .similar-case {
        background-color: #f1f8e9;
        border-radius: 8px;
        padding: 12px;
        margin-bottom: 8px;
        border-left: 4px solid #689f38;
    }
    .language-selector {
        position: absolute;
        top: 10px;
        right: 10px;
        z-index: 1000;
    }
</style>
""", unsafe_allow_html=True)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "index" not in st.session_state:
    st.session_state.index = None
if "embeddings" not in st.session_state:
    st.session_state.embeddings = None
if "retriever_pool_df" not in st.session_state:
    st.session_state.retriever_pool_df = None
if "language" not in st.session_state:
    st.session_state.language = "Korean"

# ìƒë‹¨ì— ì–¸ì–´ ì„ íƒê¸° ì¶”ê°€
col1, col2 = st.columns([6, 1])
with col2:
    selected_language = st.selectbox(
        "",
        options=list(system_texts.keys()),
        index=list(system_texts.keys()).index(st.session_state.language) if st.session_state.language in system_texts else 0,
        key="language_selector"
    )
    st.session_state.language = selected_language

# í˜„ì¬ ì–¸ì–´ì— ë”°ë¥¸ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
texts = system_texts[st.session_state.language]

# í—¤ë” í‘œì‹œ
st.markdown(f'<div class="main-header">{texts["title"]}</div>', unsafe_allow_html=True)

# íƒ­ ì„¤ì •
tabs = st.tabs([texts["tab_overview"], texts["tab_phase1"], texts["tab_phase2"]])

# ------------------ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ------------------

# ë¹ˆë„*ê°•ë„ ê²°ê³¼ Tì— ë”°ë¥¸ ë“±ê¸‰ ê²°ì • í•¨ìˆ˜
def determine_grade(value):
    """ë¹ˆë„*ê°•ë„ ê²°ê³¼ Tì— ë”°ë¥¸ ë“±ê¸‰ ê²°ì • í•¨ìˆ˜."""
    if 16 <= value <= 25:
        return 'A'
    elif 10 <= value <= 15:
        return 'B'
    elif 5 <= value <= 9:
        return 'C'
    elif 3 <= value <= 4:
        return 'D'
    elif 1 <= value <= 2:
        return 'E'
    else:
        return 'ì•Œ ìˆ˜ ì—†ìŒ' if st.session_state.language == 'Korean' else 'Unknown'

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜
def load_data(selected_dataset_name):
    """ì„ íƒëœ ì´ë¦„ì— ëŒ€ì‘í•˜ëŠ” Excel ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°."""
    try:
        # ì‹¤ì œ Excel íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
        df = pd.read_excel(f"{selected_dataset_name}.xlsx")

        # ì „ì²˜ë¦¬
        if 'ì‚­ì œ Del' in df.columns:
            df = df.drop(['ì‚­ì œ Del'], axis=1)
        df = df.iloc[1:]
        df = df.rename(columns={df.columns[4]: 'ë¹ˆë„'})
        df = df.rename(columns={df.columns[5]: 'ê°•ë„'})

        df['T'] = pd.to_numeric(df.iloc[:, 4]) * pd.to_numeric(df.iloc[:, 5])
        df = df.iloc[:, :7]
        df.rename(
            columns={
                'ì‘ì—…í™œë™ ë° ë‚´ìš©\nWork & Contents': 'ì‘ì—…í™œë™ ë° ë‚´ìš©',
                'ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥\nHazard & Risk': 'ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥',
                'í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥\nDamage & Effect': 'í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥'
            },
            inplace=True
        )
        df = df.rename(columns={df.columns[6]: 'T'})
        df['ë“±ê¸‰'] = df['T'].apply(determine_grade)

        return df
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.write(f"ì‹œë„í•œ íŒŒì¼ ê²½ë¡œ: {selected_dataset_name}")
        
        # íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ëŒ€ë¹„í•œ ë”ë¯¸ ë°ì´í„° ìƒì„± (ë°ëª¨ìš©)
        st.warning("Excel íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì‹¤ì œ ìš´ì˜ ì‹œì—ëŠ” í•´ë‹¹ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        data = {
            "ì‘ì—…í™œë™ ë° ë‚´ìš©": ["Shoring Installation", "In and Out of materials", "Transport / Delivery", "Survey and Inspection"],
            "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥": ["Fall and collision due to unstable ground", "Overturning of transport vehicle", 
                                 "Collision between transport vehicle", "Personnel fall while inspecting"],
            "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥": ["Injury", "Equipment damage", "Collision injury", "Fall injury"],
            "ë¹ˆë„": [3, 3, 3, 2],
            "ê°•ë„": [2, 3, 5, 3]
        }
        
        df = pd.DataFrame(data)
        df['T'] = df['ë¹ˆë„'] * df['ê°•ë„']
        df['ë“±ê¸‰'] = df['T'].apply(determine_grade)
        
        return df

# OpenAI ì„ë² ë”© APIë¥¼ í†µí•´ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
def embed_texts_with_openai(texts, model="text-embedding-3-large", api_key=None):
    """OpenAI ì„ë² ë”© APIë¡œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©."""
    if api_key:
        openai.api_key = api_key
    
    embeddings = []
    progress_bar = st.progress(0)
    total = len(texts)

    for idx, text in enumerate(texts):
        try:
            text = str(text).replace("\n", " ")
            response = openai.Embedding.create(model=model, input=[text])
            embedding = response["data"][0]["embedding"]
            embeddings.append(embedding)
        except Exception as e:
            st.error(f"í…ìŠ¤íŠ¸ ì„ë² ë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            embeddings.append([0]*1536)
        
        progress_bar.progress((idx + 1) / total)
    
    return embeddings

# GPT ëª¨ë¸ì„ í†µí•´ ì˜ˆì¸¡ ê²°ê³¼ ìƒì„±
def generate_with_gpt(prompt, api_key=None, model="gpt-4o", language="Korean"):
    """GPT ëª¨ë¸ë¡œë¶€í„° ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ëŠ” í•¨ìˆ˜."""
    if api_key:
        openai.api_key = api_key
        
    # ì–¸ì–´ì— ë”°ë¥¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    system_prompts = {
        "Korean": "ìœ„í—˜ì„± í‰ê°€ ë° ê°œì„ ëŒ€ì±… ìƒì„±ì„ ë•ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ì„¸ìš”.",
        "English": "I am an assistant helping with risk assessment and improvement measures. Please respond in English.",
        "Chinese": "æˆ‘æ˜¯ä¸€ä¸ªååŠ©è¿›è¡Œé£é™©è¯„ä¼°å’Œæ”¹è¿›æªæ–½çš„åŠ©æ‰‹ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"
    }
    
    system_prompt = system_prompts.get(language, system_prompts["Korean"])
        
    try:
        response = openai.ChatCompletion.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            temperature=0.0,
            max_tokens=250
        )
        return response['choices'][0]['message']['content'].strip()
    except Exception as e:
        st.error(f"GPT API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

# ----- Phase 1: ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡ ê´€ë ¨ í•¨ìˆ˜ -----

# ê²€ìƒ‰ëœ ë¬¸ì„œë¡œ GPT í”„ë¡¬í”„íŠ¸ ìƒì„± (Phase 1 - ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡)
def construct_prompt_phase1_hazard(retrieved_docs, activity_text, language="Korean"):
    """ì‘ì—…í™œë™ìœ¼ë¡œë¶€í„° ìœ í•´ìœ„í—˜ìš”ì¸ì„ ì˜ˆì¸¡í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±."""
    # ì–¸ì–´ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    prompt_templates = {
        "Korean": {
            "intro": "ë‹¤ìŒì€ ê±´ì„¤ í˜„ì¥ì˜ ì‘ì—…í™œë™ê³¼ ê·¸ì— ë”°ë¥¸ ìœ í•´ìœ„í—˜ìš”ì¸ì˜ ì˜ˆì‹œì…ë‹ˆë‹¤:\n\n",
            "example_format": "ì˜ˆì‹œ {i}:\nì‘ì—…í™œë™: {activity}\nìœ í•´ìœ„í—˜ìš”ì¸: {hazard}\n\n",
            "query_format": "ì´ì œ ë‹¤ìŒ ì‘ì—…í™œë™ì— ëŒ€í•œ ìœ í•´ìœ„í—˜ìš”ì¸ì„ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”:\nì‘ì—…í™œë™: {activity}\nìœ í•´ìœ„í—˜ìš”ì¸: "
        },
        "English": {
            "intro": "The following are examples of work activities at construction sites and their associated hazards:\n\n",
            "example_format": "Example {i}:\nWork Activity: {activity}\nHazard: {hazard}\n\n",
            "query_format": "Now, please predict the hazard for the following work activity:\nWork Activity: {activity}\nHazard: "
        },
        "Chinese": {
            "intro": "ä»¥ä¸‹æ˜¯å»ºç­‘å·¥åœ°çš„å·¥ä½œæ´»åŠ¨åŠå…¶ç›¸å…³å±å®³çš„ä¾‹å­:\n\n",
            "example_format": "ä¾‹å­ {i}:\nå·¥ä½œæ´»åŠ¨: {activity}\nå±å®³: {hazard}\n\n",
            "query_format": "ç°åœ¨ï¼Œè¯·é¢„æµ‹ä»¥ä¸‹å·¥ä½œæ´»åŠ¨çš„å±å®³:\nå·¥ä½œæ´»åŠ¨: {activity}\nå±å®³: "
        }
    }
    
    # í˜„ì¬ ì–¸ì–´ì˜ í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°
    template = prompt_templates.get(language, prompt_templates["Korean"])
    
    retrieved_examples = []
    for _, doc in retrieved_docs.iterrows():
        try:
            activity = doc['ì‘ì—…í™œë™ ë° ë‚´ìš©']
            hazard = doc['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥']
            retrieved_examples.append((activity, hazard))
        except:
            continue
    
    prompt = template["intro"]
    for i, (activity, hazard) in enumerate(retrieved_examples, 1):
        prompt += template["example_format"].format(i=i, activity=activity, hazard=hazard)
    
    prompt += template["query_format"].format(activity=activity_text)
    
    return prompt

# ë¹ˆë„ì™€ ê°•ë„ ì˜ˆì¸¡ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± (Phase 1)
def construct_prompt_phase1_risk(retrieved_docs, activity_text, hazard_text, language="Korean"):
    """ì‘ì—…í™œë™ê³¼ ìœ í•´ìœ„í—˜ìš”ì¸ì„ ë°”íƒ•ìœ¼ë¡œ ë¹ˆë„ì™€ ê°•ë„ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±."""
    # ì–¸ì–´ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    prompt_templates = {
        "Korean": {
            "example_format": "ì˜ˆì‹œ {i}:\nì…ë ¥: {input}\nì¶œë ¥: {output}\n\n",
            "query_format": "ì…ë ¥: {activity} - {hazard}\nìœ„ ì…ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ë¹ˆë„ì™€ ê°•ë„ë¥¼ ì˜ˆì¸¡í•˜ì„¸ìš”. ë¹ˆë„ëŠ” 1ì—ì„œ 5 ì‚¬ì´ì˜ ì •ìˆ˜ì…ë‹ˆë‹¤. ê°•ë„ëŠ” 1ì—ì„œ 5 ì‚¬ì´ì˜ ì •ìˆ˜ì…ë‹ˆë‹¤. TëŠ” ë¹ˆë„ì™€ ê°•ë„ë¥¼ ê³±í•œ ê°’ì…ë‹ˆë‹¤.\në‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:\n{json_format}\nì¶œë ¥:\n"
        },
        "English": {
            "example_format": "Example {i}:\nInput: {input}\nOutput: {output}\n\n",
            "query_format": "Input: {activity} - {hazard}\nBased on the above input, predict frequency and intensity. Frequency is an integer between 1 and 5. Intensity is an integer between 1 and 5. T is the product of frequency and intensity.\nOutput in the following JSON format:\n{json_format}\nOutput:\n"
        },
        "Chinese": {
            "example_format": "ç¤ºä¾‹ {i}:\nè¾“å…¥: {input}\nè¾“å‡º: {output}\n\n",
            "query_format": "è¾“å…¥: {activity} - {hazard}\næ ¹æ®ä¸Šè¿°è¾“å…¥ï¼Œé¢„æµ‹é¢‘ç‡å’Œå¼ºåº¦ã€‚é¢‘ç‡æ˜¯1åˆ°5ä¹‹é—´çš„æ•´æ•°ã€‚å¼ºåº¦æ˜¯1åˆ°5ä¹‹é—´çš„æ•´æ•°ã€‚Tæ˜¯é¢‘ç‡å’Œå¼ºåº¦çš„ä¹˜ç§¯ã€‚\nè¯·ä»¥ä¸‹åˆ—JSONæ ¼å¼è¾“å‡º:\n{json_format}\nè¾“å‡º:\n"
        }
    }
    
    # JSON í˜•ì‹ ì–¸ì–´ë³„ ì •ì˜
    json_formats = {
        "Korean": '{"ë¹ˆë„": ìˆ«ì, "ê°•ë„": ìˆ«ì, "T": ìˆ«ì}',
        "English": '{"frequency": number, "intensity": number, "T": number}',
        "Chinese": '{"é¢‘ç‡": æ•°å­—, "å¼ºåº¦": æ•°å­—, "T": æ•°å­—}'
    }
    
    # í˜„ì¬ ì–¸ì–´ì˜ í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°
    template = prompt_templates.get(language, prompt_templates["Korean"])
    json_format = json_formats.get(language, json_formats["Korean"])
    
    retrieved_examples = []
    for _, doc in retrieved_docs.iterrows():
        try:
            example_input = f"{doc['ì‘ì—…í™œë™ ë° ë‚´ìš©']} - {doc['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥']}"
            frequency = int(doc['ë¹ˆë„'])
            intensity = int(doc['ê°•ë„'])
            T_value = frequency * intensity
            
            # ì–¸ì–´ë³„ JSON ì¶œë ¥ í˜•ì‹
            if language == "Korean":
                example_output = f'{{"ë¹ˆë„": {frequency}, "ê°•ë„": {intensity}, "T": {T_value}}}'
            elif language == "English":
                example_output = f'{{"frequency": {frequency}, "intensity": {intensity}, "T": {T_value}}}'
            elif language == "Chinese":
                example_output = f'{{"é¢‘ç‡": {frequency}, "å¼ºåº¦": {intensity}, "T": {T_value}}}'
            else:
                example_output = f'{{"ë¹ˆë„": {frequency}, "ê°•ë„": {intensity}, "T": {T_value}}}'
                
            retrieved_examples.append((example_input, example_output))
        except:
            continue
    
    prompt = ""
    for i, (example_input, example_output) in enumerate(retrieved_examples, 1):
        prompt += template["example_format"].format(i=i, input=example_input, output=example_output)
    
    prompt += template["query_format"].format(
        activity=activity_text, 
        hazard=hazard_text,
        json_format=json_format
    )
    
    return prompt

# GPT ì¶œë ¥ íŒŒì‹± (Phase 1)
def parse_gpt_output_phase1(gpt_output, language="Korean"):
    """GPT ì¶œë ¥ì—ì„œ {ë¹ˆë„, ê°•ë„, T}ë¥¼ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¶”ì¶œ."""
    # ì–¸ì–´ë³„ JSON íŒ¨í„´
    json_patterns = {
        "Korean": r'\{"ë¹ˆë„":\s*([1-5]),\s*"ê°•ë„":\s*([1-5]),\s*"T":\s*([0-9]+)\}',
        "English": r'\{"frequency":\s*([1-5]),\s*"intensity":\s*([1-5]),\s*"T":\s*([0-9]+)\}',
        "Chinese": r'\{"é¢‘ç‡":\s*([1-5]),\s*"å¼ºåº¦":\s*([1-5]),\s*"T":\s*([0-9]+)\}'
    }
    
    pattern = json_patterns.get(language, json_patterns["Korean"])
    match = re.search(pattern, gpt_output)
    
    if match:
        pred_frequency = int(match.group(1))
        pred_intensity = int(match.group(2))
        pred_T = int(match.group(3))
        return pred_frequency, pred_intensity, pred_T
    else:
        # ë‹¤ë¥¸ íŒ¨í„´ë„ ì‹œë„ (GPTê°€ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ì‘ë‹µí–ˆì„ ê²½ìš°)
        for lang, pattern in json_patterns.items():
            if lang != language:  # ì´ë¯¸ ì‹œë„í•œ ì–¸ì–´ëŠ” ê±´ë„ˆë›´ë‹¤
                match = re.search(pattern, gpt_output)
                if match:
                    pred_frequency = int(match.group(1))
                    pred_intensity = int(match.group(2))
                    pred_T = int(match.group(3))
                    return pred_frequency, pred_intensity, pred_T
        
        return None

# ----- Phase 2: ê°œì„ ëŒ€ì±… ìƒì„± ê´€ë ¨ í•¨ìˆ˜ -----

# ìœ„í—˜ ê°ì†Œìœ¨(RRR) ê³„ì‚° í•¨ìˆ˜
def compute_rrr(T_before, T_after):
    """ìœ„í—˜ ê°ì†Œìœ¨(Risk Reduction Rate) ê³„ì‚°"""
    if T_before == 0:
        return 0.0
    return ((T_before - T_after) / T_before) * 100.0

# ê°œì„ ëŒ€ì±… ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (Phase 2)
def construct_prompt_phase2(retrieved_docs, activity_text, hazard_text, freq, intensity, T, target_language="Korean"):
    """
    ê°œì„ ëŒ€ì±… ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    """
    # ì˜ˆì‹œ ì„¹ì…˜ êµ¬ì„±
    example_section = ""
    examples_added = 0
    
    # ì–¸ì–´ë³„ í•„ë“œëª…
    field_names = {
        "Korean": {
            "improvement_fields": ['ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ', 'ê°œì„ ëŒ€ì±…', 'ê°œì„ ë°©ì•ˆ', 'Corrective Action'],
            "activity": "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "hazard": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥",
            "freq": "ë¹ˆë„",
            "intensity": "ê°•ë„",
            "example_intro": "Example:",
            "input_activity": "Input (Activity): ",
            "input_hazard": "Input (Hazard): ",
            "input_freq": "Input (Original Frequency): ",
            "input_intensity": "Input (Original Intensity): ",
            "input_t": "Input (Original T): ",
            "output_intro": "Output (Improvement Plan and Risk Reduction) in JSON:",
            "improvement": "ê°œì„ ëŒ€ì±…",
            "improved_freq": "ê°œì„  í›„ ë¹ˆë„",
            "improved_intensity": "ê°œì„  í›„ ê°•ë„",
            "improved_t": "ê°œì„  í›„ T",
            "reduction_rate": "T ê°ì†Œìœ¨"
        },
        "English": {
            "improvement_fields": ['Improvement Measures', 'Improvement Plan', 'Countermeasures'],
            "activity": "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "hazard": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥",
            "freq": "ë¹ˆë„",
            "intensity": "ê°•ë„",
            "example_intro": "Example:",
            "input_activity": "Input (Activity): ",
            "input_hazard": "Input (Hazard): ",
            "input_freq": "Input (Original Frequency): ",
            "input_intensity": "Input (Original Intensity): ",
            "input_t": "Input (Original T): ",
            "output_intro": "Output (Improvement Plan and Risk Reduction) in JSON:",
            "improvement": "improvement_plan",
            "improved_freq": "improved_frequency",
            "improved_intensity": "improved_intensity",
            "improved_t": "improved_T",
            "reduction_rate": "reduction_rate"
        },
        "Chinese": {
            "improvement_fields": ['æ”¹è¿›æªæ–½', 'æ”¹è¿›è®¡åˆ’', 'å¯¹ç­–'],
            "activity": "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "hazard": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥",
            "freq": "ë¹ˆë„",
            "intensity": "ê°•ë„",
            "example_intro": "ç¤ºä¾‹:",
            "input_activity": "è¾“å…¥ (å·¥ä½œæ´»åŠ¨): ",
            "input_hazard": "è¾“å…¥ (å±å®³): ",
            "input_freq": "è¾“å…¥ (åŸé¢‘ç‡): ",
            "input_intensity": "è¾“å…¥ (åŸå¼ºåº¦): ",
            "input_t": "è¾“å…¥ (åŸTå€¼): ",
            "output_intro": "è¾“å‡º (æ”¹è¿›è®¡åˆ’å’Œé£é™©é™ä½) ä»¥JSONæ ¼å¼:",
            "improvement": "æ”¹è¿›æªæ–½",
            "improved_freq": "æ”¹è¿›åé¢‘ç‡",
            "improved_intensity": "æ”¹è¿›åå¼ºåº¦",
            "improved_t": "æ”¹è¿›åTå€¼",
            "reduction_rate": "Tå€¼é™ä½ç‡"
        }
    }
    
    # í˜„ì¬ ì–¸ì–´ì— ë§ëŠ” í•„ë“œëª… ê°€ì ¸ì˜¤ê¸°
    fields = field_names.get(target_language, field_names["Korean"])
    
    for _, row in retrieved_docs.iterrows():
        try:
            # Phase2 ë°ì´í„°ì…‹ì— ìˆëŠ” ê°œì„ ëŒ€ì±… í•„ë“œ ì‚¬ìš© ì‹œë„
            improvement_plan = ""
            for field in fields["improvement_fields"]:
                if field in row and pd.notna(row[field]):
                    improvement_plan = row[field]
                    break
            
            if not improvement_plan:
                continue  # ê°œì„ ëŒ€ì±…ì´ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                
            original_freq = int(row[fields["freq"]]) if fields["freq"] in row else 3
            original_intensity = int(row[fields["intensity"]]) if fields["intensity"] in row else 3
            original_T = original_freq * original_intensity
                
            # ê°œì„  í›„ ë°ì´í„° ì‹œë„
            improved_freq = 1
            improved_intensity = 1
            improved_T = 1
            
            for field_pattern in [('ê°œì„  í›„ ë¹ˆë„', 'ê°œì„  í›„ ê°•ë„', 'ê°œì„  í›„ T'), ('ê°œì„ ë¹ˆë„', 'ê°œì„ ê°•ë„', 'ê°œì„ T')]:
                if all(field in row for field in field_pattern):
                    improved_freq = int(row[field_pattern[0]])
                    improved_intensity = int(row[field_pattern[1]])
                    improved_T = int(row[field_pattern[2]])
                    break
            
            example_section += (
                f"{fields['example_intro']}\n"
                f"{fields['input_activity']}{row[fields['activity']]}\n"
                f"{fields['input_hazard']}{row[fields['hazard']]}\n"
                f"{fields['input_freq']}{original_freq}\n"
                f"{fields['input_intensity']}{original_intensity}\n"
                f"{fields['input_t']}{original_T}\n"
                f"{fields['output_intro']}\n"
                "{\n"
                f'  "{fields["improvement"]}": "{improvement_plan}",\n'
                f'  "{fields["improved_freq"]}": {improved_freq},\n'
                f'  "{fields["improved_intensity"]}": {improved_intensity},\n'
                f'  "{fields["improved_t"]}": {improved_T},\n'
                f'  "{fields["reduction_rate"]}": {compute_rrr(original_T, improved_T):.2f}\n'
                "}\n\n"
            )
            
            examples_added += 1
            if examples_added >= 3:  # ìµœëŒ€ 3ê°œ ì˜ˆì‹œë§Œ ì‚¬ìš©
                break
                
        except Exception as e:
            # ì—ëŸ¬ ë°œìƒ ì‹œ í•´ë‹¹ ì˜ˆì‹œ ê±´ë„ˆë›°ê¸°
            continue
    
    # ì˜ˆì‹œê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì˜ˆì‹œ ì¶”ê°€ (ì–¸ì–´ë³„)
    if examples_added == 0:
        # í•œêµ­ì–´ ê¸°ë³¸ ì˜ˆì‹œ
        if target_language == "Korean":
            example_section = """
Example:
Input (Activity): Excavation and backfilling
Input (Hazard): Collapse of excavation wall due to improper sloping
Input (Original Frequency): 3
Input (Original Intensity): 4
Input (Original T): 12
Output (Improvement Plan and Risk Reduction) in JSON:
{
  "ê°œì„ ëŒ€ì±…": "1) í† ì–‘ ë¶„ë¥˜ì— ë”°ë¥¸ ì ì ˆí•œ ê²½ì‚¬ ìœ ì§€ 2) êµ´ì°© ë²½ë©´ ë³´ê°• 3) ì •ê¸°ì ì¸ ì§€ë°˜ ìƒíƒœ ê²€ì‚¬ ì‹¤ì‹œ",
  "ê°œì„  í›„ ë¹ˆë„": 1,
  "ê°œì„  í›„ ê°•ë„": 2,
  "ê°œì„  í›„ T": 2,
  "T ê°ì†Œìœ¨": 83.33
}

Example:
Input (Activity): Lifting operation
Input (Hazard): Material fall due to improper rigging
Input (Original Frequency): 2
Input (Original Intensity): 5
Input (Original T): 10
Output (Improvement Plan and Risk Reduction) in JSON:
{
  "ê°œì„ ëŒ€ì±…": "1) ë¦¬ê¹… ì „ë¬¸ê°€ ì‘ì—… ì°¸ì—¬ 2) ë¦¬ê¹… ì¥ë¹„ ì‚¬ì „ ì ê²€ 3) ì•ˆì „ êµ¬ì—­ ì„¤ì • ë° ì ‘ê·¼ í†µì œ",
  "ê°œì„  í›„ ë¹ˆë„": 1,
  "ê°œì„  í›„ ê°•ë„": 2,
  "ê°œì„  í›„ T": 2,
  "T ê°ì†Œìœ¨": 80.00
}
"""
        # ì˜ì–´ ê¸°ë³¸ ì˜ˆì‹œ
        elif target_language == "English":
            example_section = """
Example:
Input (Activity): Excavation and backfilling
Input (Hazard): Collapse of excavation wall due to improper sloping
Input (Original Frequency): 3
Input (Original Intensity): 4
Input (Original T): 12
Output (Improvement Plan and Risk Reduction) in JSON:
{
  "improvement_plan": "1) Maintain proper slope according to soil classification 2) Reinforce excavation walls 3) Conduct regular ground condition inspections",
  "improved_frequency": 1,
  "improved_intensity": 2,
  "improved_T": 2,
  "reduction_rate": 83.33
}

Example:
Input (Activity): Lifting operation
Input (Hazard): Material fall due to improper rigging
Input (Original Frequency): 2
Input (Original Intensity): 5
Input (Original T): 10
Output (Improvement Plan and Risk Reduction) in JSON:
{
  "improvement_plan": "1) Involve rigging experts in operations 2) Pre-inspect rigging equipment 3) Set up safety zones and control access",
  "improved_frequency": 1,
  "improved_intensity": 2,
  "improved_T": 2,
  "reduction_rate": 80.00
}
"""
        # ì¤‘êµ­ì–´ ê¸°ë³¸ ì˜ˆì‹œ
        elif target_language == "Chinese":
            example_section = """
ç¤ºä¾‹:
è¾“å…¥ (å·¥ä½œæ´»åŠ¨): Excavation and backfilling
è¾“å…¥ (å±å®³): Collapse of excavation wall due to improper sloping
è¾“å…¥ (åŸé¢‘ç‡): 3
è¾“å…¥ (åŸå¼ºåº¦): 4
è¾“å…¥ (åŸTå€¼): 12
è¾“å‡º (æ”¹è¿›è®¡åˆ’å’Œé£é™©é™ä½) ä»¥JSONæ ¼å¼:
{
  "æ”¹è¿›æªæ–½": "1) æ ¹æ®åœŸå£¤åˆ†ç±»ç»´æŒé€‚å½“çš„æ–œå¡ 2) åŠ å›ºæŒ–æ˜å¢™å£ 3) å®šæœŸè¿›è¡Œåœ°é¢çŠ¶å†µæ£€æŸ¥",
  "æ”¹è¿›åé¢‘ç‡": 1,
  "æ”¹è¿›åå¼ºåº¦": 2,
  "æ”¹è¿›åTå€¼": 2,
  "Tå€¼é™ä½ç‡": 83.33
}

ç¤ºä¾‹:
è¾“å…¥ (å·¥ä½œæ´»åŠ¨): Lifting operation
è¾“å…¥ (å±å®³): Material fall due to improper rigging
è¾“å…¥ (åŸé¢‘ç‡): 2
è¾“å…¥ (åŸå¼ºåº¦): 5
è¾“å…¥ (åŸTå€¼): 10
è¾“å‡º (æ”¹è¿›è®¡åˆ’å’Œé£é™©é™ä½) ä»¥JSONæ ¼å¼:
{
  "æ”¹è¿›æªæ–½": "1) åŠè£…ä¸“å®¶å‚ä¸ä½œä¸š 2) é¢„æ£€æŸ¥åŠè£…è®¾å¤‡ 3) è®¾ç½®å®‰å…¨åŒºåŸŸå¹¶æ§åˆ¶è¿›å…¥",
  "æ”¹è¿›åé¢‘ç‡": 1,
  "æ”¹è¿›åå¼ºåº¦": 2,
  "æ”¹è¿›åTå€¼": 2,
  "Tå€¼é™ä½ç‡": 80.00
}
"""
    
    # ì–¸ì–´ë³„ JSON ì¶œë ¥ í‚¤ ì´ë¦„
    json_keys = {
        "Korean": {
            "improvement": "ê°œì„ ëŒ€ì±…",
            "improved_freq": "ê°œì„  í›„ ë¹ˆë„",
            "improved_intensity": "ê°œì„  í›„ ê°•ë„",
            "improved_t": "ê°œì„  í›„ T",
            "reduction_rate": "T ê°ì†Œìœ¨"
        },
        "English": {
            "improvement": "improvement_plan",
            "improved_freq": "improved_frequency",
            "improved_intensity": "improved_intensity",
            "improved_t": "improved_T",
            "reduction_rate": "reduction_rate"
        },
        "Chinese": {
            "improvement": "æ”¹è¿›æªæ–½",
            "improved_freq": "æ”¹è¿›åé¢‘ç‡",
            "improved_intensity": "æ”¹è¿›åå¼ºåº¦", 
            "improved_t": "æ”¹è¿›åTå€¼",
            "reduction_rate": "Tå€¼é™ä½ç‡"
        }
    }
    
    # ê° ì–¸ì–´ë³„ ì•ˆë‚´ ë©”ì‹œì§€
    instructions = {
        "Korean": {
            "new_input": "ë‹¤ìŒì€ ìƒˆë¡œìš´ ì…ë ¥ì…ë‹ˆë‹¤:",
            "input_activity": "ì…ë ¥ (ì‘ì—…í™œë™): ",
            "input_hazard": "ì…ë ¥ (ìœ í•´ìœ„í—˜ìš”ì¸): ",
            "input_freq": "ì…ë ¥ (ì›ë˜ ë¹ˆë„): ",
            "input_intensity": "ì…ë ¥ (ì›ë˜ ê°•ë„): ",
            "input_t": "ì…ë ¥ (ì›ë˜ T): ",
            "output_format": "ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ì„ ì œê³µí•˜ì„¸ìš”:",
            "improvement_write": "ê°œì„ ëŒ€ì±…(ê°œì„ ëŒ€ì±…)ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.",
            "provide_measures": "ìµœì†Œ 3ê°œì˜ êµ¬ì²´ì ì¸ ê°œì„  ì¡°ì¹˜ë¥¼ ë²ˆí˜¸ê°€ ë§¤ê²¨ì§„ ëª©ë¡ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”.",
            "valid_json": "ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•˜ë„ë¡ í•˜ì„¸ìš”.",
            "output": "ì¶œë ¥:"
        },
        "English": {
            "new_input": "Now here is a new input:",
            "input_activity": "Input (Activity): ",
            "input_hazard": "Input (Hazard): ",
            "input_freq": "Input (Original Frequency): ",
            "input_intensity": "Input (Original Intensity): ",
            "input_t": "Input (Original T): ",
            "output_format": "Please provide the output in JSON format with these keys:",
            "improvement_write": "Please write the improvement measures (improvement_plan) in English.",
            "provide_measures": "Provide at least 3 specific improvement measures as a numbered list.",
            "valid_json": "Make sure to return only valid JSON.",
            "output": "Output:"
        },
        "Chinese": {
            "new_input": "ä»¥ä¸‹æ˜¯æ–°çš„è¾“å…¥:",
            "input_activity": "è¾“å…¥ (å·¥ä½œæ´»åŠ¨): ",
            "input_hazard": "è¾“å…¥ (å±å®³): ",
            "input_freq": "è¾“å…¥ (åŸé¢‘ç‡): ",
            "input_intensity": "è¾“å…¥ (åŸå¼ºåº¦): ",
            "input_t": "è¾“å…¥ (åŸTå€¼): ",
            "output_format": "è¯·ä»¥ä»¥ä¸‹JSONæ ¼å¼æä¾›è¾“å‡º:",
            "improvement_write": "è¯·ç”¨ä¸­æ–‡ç¼–å†™æ”¹è¿›æªæ–½(æ”¹è¿›æªæ–½)ã€‚",
            "provide_measures": "æä¾›è‡³å°‘3é¡¹å…·ä½“çš„æ”¹è¿›æªæ–½ï¼Œåˆ—ä¸ºç¼–å·åˆ—è¡¨ã€‚",
            "valid_json": "è¯·ç¡®ä¿åªè¿”å›æœ‰æ•ˆçš„JSONã€‚",
            "output": "è¾“å‡º:"
        }
    }
    
    # í˜„ì¬ ì–¸ì–´ì˜ í‚¤ì™€ ì•ˆë‚´ ë©”ì‹œì§€
    keys = json_keys.get(target_language, json_keys["Korean"])
    instr = instructions.get(target_language, instructions["Korean"])
    
    # ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = (
        f"{example_section}"
        f"{instr['new_input']}\n"
        f"{instr['input_activity']}{activity_text}\n"
        f"{instr['input_hazard']}{hazard_text}\n"
        f"{instr['input_freq']}{freq}\n"
        f"{instr['input_intensity']}{intensity}\n"
        f"{instr['input_t']}{T}\n\n"
        f"{instr['output_format']}\n"
        "{\n"
        f'  "{keys["improvement"]}": "í•­ëª©ë³„ ê°œì„ ëŒ€ì±… ë¦¬ìŠ¤íŠ¸", \n'
        f'  "{keys["improved_freq"]}": (an integer in [1..5]),\n'
        f'  "{keys["improved_intensity"]}": (an integer in [1..5]),\n'
        f'  "{keys["improved_t"]}": (Improved Frequency * Improved Severity),\n'
        f'  "{keys["reduction_rate"]}": (percentage of risk reduction)\n'
        "}\n\n"
        f"{instr['improvement_write']}\n"
        f"{instr['provide_measures']}\n"
        f"{instr['valid_json']}\n"
        f"{instr['output']}\n"
    )
    
    return prompt

# GPT ì‘ë‹µ íŒŒì‹± (Phase 2)
def parse_gpt_output_phase2(gpt_output, language="Korean"):
    """GPT ì‘ë‹µì—ì„œ JSON ë°ì´í„°ë¥¼ ì¶”ì¶œ"""
    try:
        # JSON ë¸”ë¡ì´ ìˆëŠ” ê²½ìš° ì¶”ì¶œ ì‹œë„
        pattern = re.compile(r"```json(.*?)```", re.DOTALL)
        match = pattern.search(gpt_output)

        if match:
            json_str = match.group(1).strip()
        else:
            # JSON ë¸”ë¡ í‘œì‹œê°€ ì—†ëŠ” ê²½ìš° ì›ë¬¸ì„ JSONìœ¼ë¡œ íŒŒì‹± ì‹œë„
            json_str = gpt_output.replace("```", "").strip()

        import json
        result = json.loads(json_str)
        
        # ì–¸ì–´ë³„ í‚¤ ë§¤í•‘
        key_mappings = {
            "Korean": {
                "improvement": ["ê°œì„ ëŒ€ì±…"],
                "improved_freq": ["ê°œì„  í›„ ë¹ˆë„", "ê°œì„ ë¹ˆë„"],
                "improved_intensity": ["ê°œì„  í›„ ê°•ë„", "ê°œì„ ê°•ë„"],
                "improved_t": ["ê°œì„  í›„ T", "ê°œì„ T", "ê°œì„  í›„ t"],
                "reduction_rate": ["T ê°ì†Œìœ¨", "ê°ì†Œìœ¨", "ìœ„í—˜ ê°ì†Œìœ¨"]
            },
            "English": {
                "improvement": ["improvement_plan", "improvement_measures", "improvements"],
                "improved_freq": ["improved_frequency", "new_frequency", "frequency_after"],
                "improved_intensity": ["improved_intensity", "new_intensity", "intensity_after"],
                "improved_t": ["improved_T", "new_T", "T_after"],
                "reduction_rate": ["reduction_rate", "risk_reduction_rate", "rrr"]
            },
            "Chinese": {
                "improvement": ["æ”¹è¿›æªæ–½", "æ”¹è¿›è®¡åˆ’", "æ”¹å–„æªæ–½"],
                "improved_freq": ["æ”¹è¿›åé¢‘ç‡", "æ–°é¢‘ç‡", "é¢‘ç‡æ”¹è¿›å"],
                "improved_intensity": ["æ”¹è¿›åå¼ºåº¦", "æ–°å¼ºåº¦", "å¼ºåº¦æ”¹è¿›å"],
                "improved_t": ["æ”¹è¿›åTå€¼", "æ–°Tå€¼", "Tå€¼æ”¹è¿›å"],
                "reduction_rate": ["Tå€¼é™ä½ç‡", "é£é™©é™ä½ç‡", "é™ä½ç‡"]
            }
        }
        
        # ê²°ê³¼ ë§¤í•‘
        mapped_result = {}
        
        # í˜„ì¬ ì–¸ì–´ì˜ í‚¤ ë§¤í•‘ ê°€ì ¸ì˜¤ê¸°
        mappings = key_mappings.get(language, key_mappings["Korean"])
        
        # ê°œì„ ëŒ€ì±… í‚¤ ë§¤í•‘
        for result_key, possible_keys in mappings.items():
            for key in possible_keys:
                if key in result:
                    mapped_result[result_key] = result[key]
                    break
            
        return mapped_result
    except Exception as e:
        st.error(f"JSON íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.write("ì›ë³¸ GPT ì‘ë‹µ:", gpt_output)
        return None

# ------------------ ë°ì´í„°ì…‹ ë° ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„ ------------------

# ë°ì´í„°ì…‹ ì˜µì…˜
dataset_options = {
    "SWRO ê±´ì¶•ê³µì • (ê±´ì¶•)": "SWRO ê±´ì¶•ê³µì • (ê±´ì¶•)",
    "Civil (í† ëª©)": "Civil (í† ëª©)",
    "Marine (í† ëª©)": "Marine (í† ëª©)",
    "SWRO ê¸°ê³„ê³µì‚¬ (í”ŒëœíŠ¸)": "SWRO ê¸°ê³„ê³µì‚¬ (í”ŒëœíŠ¸)",
    "SWRO ì „ê¸°ì‘ì—…í‘œì¤€ (í”ŒëœíŠ¸)": "SWRO ì „ê¸°ì‘ì—…í‘œì¤€ (í”ŒëœíŠ¸)"
}

# ----- ì‹œìŠ¤í…œ ê°œìš” íƒ­ -----
with tabs[0]:
    st.markdown(f'<div class="sub-header">{texts["overview_header"]}</div>', unsafe_allow_html=True)
    
    col1, col2 = st.columns([3, 2])
    
    with col1:
        st.markdown(f"""
        <div class="info-text">
        {texts["overview_text"]}
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        # AI ìœ„í—˜ì„±í‰ê°€ í”„ë¡œì„¸ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨
        st.markdown(f'<div style="text-align: center; margin-bottom: 10px;"><b>{texts["process_title"]}</b></div>', unsafe_allow_html=True)
        
        steps = texts["process_steps"]
        
        for i, step in enumerate(steps):
            phase_badge = '<span class="phase-badge">Phase 1</span>' if i < 4 else '<span class="phase-badge">Phase 2</span>'
            st.markdown(f"**{i+1}. {step}** {phase_badge}" + (" â†’ " if i < len(steps)-1 else ""), unsafe_allow_html=True)
    
    # ì‹œìŠ¤í…œ íŠ¹ì§•
    st.markdown(f'<div class="sub-header">{texts["features_title"]}</div>', unsafe_allow_html=True)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown(texts["phase1_features"], unsafe_allow_html=True)
    
    with col2:
        st.markdown(texts["phase2_features"], unsafe_allow_html=True)

# ----- Phase 1: ìœ„í—˜ì„± í‰ê°€ íƒ­ -----
with tabs[1]:
    st.markdown(f'<div class="sub-header">{texts["phase1_header"]}</div>', unsafe_allow_html=True)
    
    # API í‚¤ ì…ë ¥
    api_key = st.text_input(texts["api_key_label"], type="password", key="api_key_phase1")
    
    # ë°ì´í„°ì…‹ ì„ íƒ
    selected_dataset_name = st.selectbox(
        texts["dataset_label"],
        options=list(dataset_options.keys()),
        key="dataset_selector_phase1"
    )
    
    # ì¸ë±ìŠ¤ êµ¬ì„± ì„¹ì…˜
    st.markdown(f"### {texts['load_data_label']}")
    
    if st.button(texts["load_data_btn"], key="load_data_phase1"):
        if not api_key:
            st.warning(texts["api_key_warning"])
        else:
            with st.spinner(texts["data_loading"]):
                # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
                df = load_data(dataset_options[selected_dataset_name])
                
                if df is not None:
                    # Train/Test ë¶„í• 
                    train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)
                    
                    # ë¦¬íŠ¸ë¦¬ë²„ í’€ êµ¬ì„±
                    retriever_pool_df = train_df.copy()
                    retriever_pool_df['content'] = retriever_pool_df.apply(
                        lambda row: ' '.join(row.values.astype(str)), axis=1
                    )
                    texts_to_embed = retriever_pool_df['content'].tolist()
                    
                    # ì„ë² ë”© ìƒì„± (ë°ëª¨ì—ì„œëŠ” ì ì€ ìˆ˜ë§Œ ì²˜ë¦¬, ì‹¤ì œë¡œëŠ” ì „ì²´ ì²˜ë¦¬)
                    max_texts = min(len(texts_to_embed), 10)  # ë°ëª¨ì—ì„œëŠ” ìµœëŒ€ 10ê°œë§Œ ì²˜ë¦¬
                    st.info("í…ìŠ¤íŠ¸ ì„ë² ë”© ì¤‘ì…ë‹ˆë‹¤...")
                    
                    openai.api_key = api_key
                    embeddings = embed_texts_with_openai(texts_to_embed[:max_texts], api_key=api_key)
                    
                    # FAISS ì¸ë±ìŠ¤ êµ¬ì„±
                    embeddings_array = np.array(embeddings, dtype='float32')
                    dimension = embeddings_array.shape[1]
                    faiss_index = faiss.IndexFlatL2(dimension)
                    faiss_index.add(embeddings_array)
                    
                    # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                    st.session_state.index = faiss_index
                    st.session_state.embeddings = embeddings_array
                    st.session_state.retriever_pool_df = retriever_pool_df.iloc[:max_texts]  # ì„ë² ë”©ëœ ë¶€ë¶„ë§Œ ì €ì¥
                    
                    st.success("ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„± ì™„ë£Œ!")
                    st.session_state.test_df = test_df
    
    # ì‚¬ìš©ì ì…ë ¥ ì˜ˆì¸¡ ì„¹ì…˜
    st.markdown(f"### {texts['hazard_prediction_header']}")
    
    if st.session_state.index is None:
        st.warning(texts["load_first_warning"])
    else:
        with st.form("user_input_form"):
            user_work = st.text_input(texts["activity_label"], key="form_user_work")
            submitted = st.form_submit_button(texts["predict_hazard_btn"])
            
        if submitted:
            if not user_work:
                st.warning(texts["activity_warning"])
            else:
                with st.spinner(texts["predicting_hazard"]):
                    # ì¿¼ë¦¬ ì„ë² ë”©
                    query_embedding = embed_texts_with_openai([user_work], api_key=api_key)[0]
                    query_embedding_array = np.array([query_embedding], dtype='float32')
                    
                    # ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
                    k_similar = min(3, len(st.session_state.retriever_pool_df))
                    distances, indices = st.session_state.index.search(query_embedding_array, k_similar)
                    retrieved_docs = st.session_state.retriever_pool_df.iloc[indices[0]]
                    
                    # ìœ ì‚¬í•œ ì‚¬ë¡€ í‘œì‹œ
                    st.markdown(f"#### {texts['similar_cases_header']}")
                    for i, (_, doc) in enumerate(retrieved_docs.iterrows(), 1):
                        st.markdown(
                            texts["similar_case_text"].format(
                                i=i,
                                activity=doc['ì‘ì—…í™œë™ ë° ë‚´ìš©'],
                                hazard=doc['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥'],
                                freq=doc['ë¹ˆë„'],
                                intensity=doc['ê°•ë„'],
                                t_value=doc['T'],
                                grade=doc['ë“±ê¸‰']
                            ), 
                            unsafe_allow_html=True
                        )
                    
                    # GPT í”„ë¡¬í”„íŠ¸ ìƒì„± & í˜¸ì¶œ (ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡)
                    hazard_prompt = construct_prompt_phase1_hazard(retrieved_docs, user_work, language=st.session_state.language)
                    hazard_prediction = generate_with_gpt(hazard_prompt, api_key=api_key, language=st.session_state.language)
                    
                    # ë¹ˆë„ì™€ ê°•ë„ ì˜ˆì¸¡ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± & í˜¸ì¶œ
                    risk_prompt = construct_prompt_phase1_risk(retrieved_docs, user_work, hazard_prediction, language=st.session_state.language)
                    risk_prediction = generate_with_gpt(risk_prompt, api_key=api_key, language=st.session_state.language)
                    
                    # ê²°ê³¼ í‘œì‹œ
                    st.markdown(f"#### {texts['prediction_result_header']}")
                    st.markdown(texts["activity_result"].format(activity=user_work))
                    st.markdown(texts["hazard_result"].format(hazard=hazard_prediction))
                    
                    parse_result = parse_gpt_output_phase1(risk_prediction, language=st.session_state.language)
                    if parse_result is not None:
                        f_val, i_val, t_val = parse_result
                        grade = determine_grade(t_val)
                        
                        # ê²°ê³¼ë¥¼ í‘œë¡œ í‘œì‹œ
                        result_df = pd.DataFrame({
                            texts["result_table_columns"][0]: texts["result_table_rows"],
                            texts["result_table_columns"][1]: [f_val, i_val, t_val, grade]
                        })
                        
                        st.markdown('<div class="result-box">', unsafe_allow_html=True)
                        st.table(result_df)
                        st.markdown('</div>', unsafe_allow_html=True)
                        
                        # ì„¸ì…˜ ìƒíƒœì— ê²°ê³¼ ì €ì¥ (Phase 2ì—ì„œ ì‚¬ìš©)
                        st.session_state.last_assessment = {
                            'activity': user_work,
                            'hazard': hazard_prediction,
                            'frequency': f_val,
                            'intensity': i_val,
                            'T': t_val,
                            'grade': grade
                        }
                    else:
                        st.error(texts["parsing_error"])
                        st.write(texts["gpt_response"].format(response=risk_prediction))

# ----- Phase 2: ê°œì„ ëŒ€ì±… ìƒì„± íƒ­ -----
with tabs[2]:
    st.markdown(f'<div class="sub-header">{texts["phase2_header"]}</div>', unsafe_allow_html=True)
    
    # API í‚¤ ì…ë ¥
    api_key_phase2 = st.text_input(texts["api_key_label"], type="password", key="api_key_phase2")
    
    # ê°œì„ ëŒ€ì±… ì–¸ì–´ ì„ íƒ
    target_language = st.selectbox(
        texts["language_select_label"],
        options=list(system_texts.keys()),
        index=list(system_texts.keys()).index(st.session_state.language),
        key="target_language"
    )
    
    # ì…ë ¥ ë°©ì‹ ì„ íƒ (Phase 1 ê²°ê³¼ ì‚¬ìš© ë˜ëŠ” ì§ì ‘ ì…ë ¥)
    input_method = st.radio(
        texts["input_method_label"],
        options=texts["input_methods"],
        index=0,
        key="input_method"
    )
    
    if input_method == texts["input_methods"][0]:  # Phase 1 í‰ê°€ ê²°ê³¼ ì‚¬ìš©
        # Phase 1 ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
        if hasattr(st.session_state, 'last_assessment'):
            last_assessment = st.session_state.last_assessment
            
            st.markdown(f"### {texts['phase1_results_header']}")
            st.markdown(f"**{texts['activity_label'].strip(':')}** {last_assessment['activity']}")
            st.markdown(f"**{texts['hazard_label'].strip(':')}** {last_assessment['hazard']}")
            st.markdown(
                texts["risk_level_text"].format(
                    freq=last_assessment['frequency'],
                    intensity=last_assessment['intensity'],
                    t_value=last_assessment['T'],
                    grade=last_assessment['grade']
                )
            )
            
            activity_text = last_assessment['activity']
            hazard_text = last_assessment['hazard']
            frequency = last_assessment['frequency']
            intensity = last_assessment['intensity']
            T_value = last_assessment['T']
            
        else:
            st.warning(texts["phase1_first_warning"])
            activity_text = hazard_text = None
            frequency = intensity = T_value = None
    else:
        # ì§ì ‘ ì…ë ¥í•˜ëŠ” ê²½ìš°
        col1, col2 = st.columns(2)
        
        with col1:
            activity_text = st.text_input(texts["activity_label"], key="direct_activity")
            hazard_text = st.text_input(texts["hazard_label"], key="direct_hazard")
        
        with col2:
            frequency = st.number_input(texts["frequency_label"], min_value=1, max_value=5, value=3, key="direct_freq")
            intensity = st.number_input(texts["intensity_label"], min_value=1, max_value=5, value=3, key="direct_intensity")
            T_value = frequency * intensity
            st.markdown(texts["t_value_text"].format(t_value=T_value, grade=determine_grade(T_value)))
    
    # ê°œì„ ëŒ€ì±… ìƒì„± ì„¹ì…˜
    if st.button(texts["generate_improvement_btn"], key="generate_improvement") and activity_text and hazard_text and frequency and intensity and T_value:
        if not api_key_phase2:
            st.warning(texts["api_key_warning"])
        else:
            with st.spinner(texts["generating_improvement"]):
                # ë¦¬íŠ¸ë¦¬ë²„ í’€ê³¼ ì¸ë±ìŠ¤ í™•ì¸
                if st.session_state.retriever_pool_df is None or st.session_state.index is None:
                    st.warning(texts["no_data_warning"])
                    # ê¸°ë³¸ ê³µí†µ ë°ì´í„°ì…‹ ë¡œë“œ
                    df = load_data("Civil (í† ëª©)")
                    retriever_pool_df = df.sample(min(5, len(df)))  # ìµœëŒ€ 5ê°œ ìƒ˜í”Œ
                    
                    # ìœ ì‚¬ ë¬¸ì„œëŠ” ëœë¤ ìƒ˜í”Œë§
                    retrieved_docs = retriever_pool_df.sample(min(3, len(retriever_pool_df)))
                else:
                    # Phase 1ì—ì„œ êµ¬ì„±ëœ ë¦¬íŠ¸ë¦¬ë²„ í’€ ë° ì¸ë±ìŠ¤ ì‚¬ìš©
                    retriever_pool_df = st.session_state.retriever_pool_df
                    
                    # ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (ì‹¤ì œ ë˜ëŠ” ëª¨ì˜)
                    k_similar = min(3, len(retriever_pool_df))
                    query_text = f"{activity_text} {hazard_text}"
                    query_embedding = embed_texts_with_openai([query_text], api_key=api_key_phase2)[0]
                    query_embedding_array = np.array([query_embedding], dtype='float32')
                    distances, indices = st.session_state.index.search(query_embedding_array, k_similar)
                    retrieved_docs = retriever_pool_df.iloc[indices[0]]
                    
                    # ìœ ì‚¬ ì‚¬ë¡€ í‘œì‹œ
                    st.markdown(f"#### {texts['similar_cases_header']}")
                    for i, (_, doc) in enumerate(retrieved_docs.iterrows(), 1):
                        # ê°œì„ ëŒ€ì±… ì •ë³´ ì°¾ê¸°
                        improvement_plan = ""
                        for field in ['ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ', 'ê°œì„ ëŒ€ì±…', 'ê°œì„ ë°©ì•ˆ', 'Corrective Action']:
                            if field in doc and pd.notna(doc[field]):
                                improvement_plan = doc[field]
                                break
                        
                        improvement_section = ""
                        if improvement_plan:
                            improvement_title = "ê°œì„ ëŒ€ì±…" if st.session_state.language == "Korean" else "Improvement Plan" if st.session_state.language == "English" else "æ”¹è¿›æªæ–½"
                            improvement_section = f"<strong>{improvement_title}:</strong> {improvement_plan}<br>"
                        
                        st.markdown(f"""
                        <div class="similar-case">
                            <strong>ì‚¬ë¡€ {i}</strong><br>
                            <strong>ì‘ì—…í™œë™:</strong> {doc['ì‘ì—…í™œë™ ë° ë‚´ìš©']}<br>
                            <strong>ìœ í•´ìœ„í—˜ìš”ì¸:</strong> {doc['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥']}<br>
                            <strong>ìœ„í—˜ë„:</strong> ë¹ˆë„ {doc['ë¹ˆë„']}, ê°•ë„ {doc['ê°•ë„']}, Tê°’ {doc['T']} (ë“±ê¸‰ {doc['ë“±ê¸‰']})<br>
                            {improvement_section}
                        </div>
                        """, unsafe_allow_html=True)
                
                # ê°œì„ ëŒ€ì±… ìƒì„± í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                prompt = construct_prompt_phase2(
                    retrieved_docs, 
                    activity_text, 
                    hazard_text, 
                    frequency, 
                    intensity, 
                    T_value, 
                    target_language
                )
                
                # GPT í˜¸ì¶œ
                generated_output = generate_with_gpt(prompt, api_key=api_key_phase2, language=target_language)
                
                # ê²°ê³¼ íŒŒì‹±
                parsed_result = parse_gpt_output_phase2(generated_output, language=target_language)
                
                if parsed_result:
                    # í‚¤ ì´ë¦„ ë§¤í•‘
                    key_mappings = {
                        "improvement": "ê°œì„ ëŒ€ì±…" if target_language == "Korean" else "improvement_plan" if target_language == "English" else "æ”¹è¿›æªæ–½",
                        "improved_freq": "ê°œì„  í›„ ë¹ˆë„" if target_language == "Korean" else "improved_frequency" if target_language == "English" else "æ”¹è¿›åé¢‘ç‡",
                        "improved_intensity": "ê°œì„  í›„ ê°•ë„" if target_language == "Korean" else "improved_intensity" if target_language == "English" else "æ”¹è¿›åå¼ºåº¦",
                        "improved_t": "ê°œì„  í›„ T" if target_language == "Korean" else "improved_T" if target_language == "English" else "æ”¹è¿›åTå€¼",
                        "reduction_rate": "T ê°ì†Œìœ¨" if target_language == "Korean" else "reduction_rate" if target_language == "English" else "Tå€¼é™ä½ç‡"
                    }
                    
                    # ê²°ê³¼ í‘œì‹œ
                    improvement_plan = parsed_result.get("improvement", "")
                    improved_freq = parsed_result.get("improved_freq", 1)
                    improved_intensity = parsed_result.get("improved_intensity", 1)
                    improved_T = parsed_result.get("improved_t", improved_freq * improved_intensity)
                    rrr = parsed_result.get("reduction_rate", compute_rrr(T_value, improved_T))
                    
                    st.markdown('<div class="result-box">', unsafe_allow_html=True)
                    st.markdown(f"#### {texts['improvement_result_header']}")
                    
                    col1, col2 = st.columns([3, 2])
                    
                    with col1:
                        st.markdown(f"##### {texts['improvement_plan_header']}")
                        st.markdown(improvement_plan)
                    
                    with col2:
                        st.markdown(f"##### {texts['risk_improvement_header']}")
                        
                        # ê°œì„  ì „í›„ ìœ„í—˜ë„ ë¹„êµí‘œ
                        comparison_df = pd.DataFrame({
                            texts["comparison_columns"][0]: texts["result_table_rows"],
                            texts["comparison_columns"][1]: [frequency, intensity, T_value, determine_grade(T_value)],
                            texts["comparison_columns"][2]: [improved_freq, improved_intensity, improved_T, determine_grade(improved_T)]
                        })
                        st.table(comparison_df)
                        
                        # ìœ„í—˜ ê°ì†Œìœ¨ í‘œì‹œ
                        st.metric(
                            label=texts["risk_reduction_label"],
                            value=f"{rrr:.2f}%"
                        )
                    
                    st.markdown('</div>', unsafe_allow_html=True)
                    
                    # ìœ„í—˜ë„ ê·¸ë˜í”„ë¡œ í‘œí˜„ (ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‹œê°í™”)
                    st.markdown(f"#### {texts['t_value_change_header']}")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown(f"**{texts['before_improvement']}**")
                        st.progress(T_value / 25)  # 25ëŠ” ìµœëŒ€ Tê°’
                    
                    with col2:
                        st.markdown(f"**{texts['after_improvement']}**")
                        st.progress(improved_T / 25)
                else:
                    st.error(texts["parsing_error_improvement"])
                    st.write(texts["gpt_response"].format(response=generated_output))

# ----- í‘¸í„° ì„¹ì…˜: ë¡œê³  ì´ë¯¸ì§€ í‘œì‹œ -----
st.markdown('<hr style="margin-top: 50px;">', unsafe_allow_html=True)
st.markdown('<div style="display: flex; justify-content: space-between; align-items: center;">', unsafe_allow_html=True)

col1, col2 = st.columns(2)

with col1:
    if os.path.exists("cau.png"):
        cau_logo = Image.open("cau.png")
        st.image(cau_logo, width=150)
    else:
        st.warning("cau.png íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

with col2:
    if os.path.exists("doosan.png"):
        doosan_logo = Image.open("doosan.png")
        st.image(doosan_logo, width=180)
    else:
        st.warning("doosan.png íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

st.markdown('</div>', unsafe_allow_html=True)
