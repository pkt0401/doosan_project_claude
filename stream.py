import streamlit as st
import pandas as pd
import numpy as np
import faiss
import openai
import re
import os
from PIL import Image
from sklearn.model_selection import train_test_split

# ì–¸ì–´ ì„¤ì • í…ìŠ¤íŠ¸ ì •ì˜
system_texts = {
    "Korean": {
        "title": "Artificial Intelligence Risk Assessment",
        "phase1_header": "ìœ„í—˜ì„± í‰ê°€ ìë™í™” (Phase 1)",
        "api_key_label": "OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
        "dataset_label": "ë°ì´í„°ì…‹ ì„ íƒ",
        "load_data_label": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±",
        "load_data_btn": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±",
        "api_key_warning": "ê³„ì†í•˜ë ¤ë©´ OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.",
        "data_loading": "ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì¸ë±ìŠ¤ë¥¼ êµ¬ì„±í•˜ëŠ” ì¤‘...",
        "demo_limit_info": "ë°ëª¨ ëª©ì ìœ¼ë¡œ {max_texts}ê°œì˜ í…ìŠ¤íŠ¸ë§Œ ì„ë² ë”©í•©ë‹ˆë‹¤. ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.",
        "data_load_success": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„± ì™„ë£Œ! (ì´ {max_texts}ê°œ í•­ëª© ì²˜ë¦¬)",
        "hazard_prediction_header": "ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡",
        "load_first_warning": "ë¨¼ì € [ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±] ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.",
        "activity_label": "ì‘ì—…í™œë™:",
        "predict_hazard_btn": "ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡í•˜ê¸°",
        "activity_warning": "ì‘ì—…í™œë™ì„ ì…ë ¥í•˜ì„¸ìš”.",
        "predicting_hazard": "ìœ í•´ìœ„í—˜ìš”ì¸ì„ ì˜ˆì¸¡í•˜ëŠ” ì¤‘...",
        "similar_cases_header": "ìœ ì‚¬í•œ ì‚¬ë¡€",
        "similar_case_text": """
        <div class="similar-case">
            <strong>ì‚¬ë¡€ {i}</strong><br>
            <strong>ì‘ì—…í™œë™:</strong> {activity}<br>
            <strong>ìœ í•´ìœ„í—˜ìš”ì¸:</strong> {hazard}<br>
            <strong>ìœ„í—˜ë„:</strong> ë¹ˆë„ {freq}, ê°•ë„ {intensity}, Tê°’ {t_value} (ë“±ê¸‰ {grade})
        </div>
        """,
        "prediction_result_header": "ì˜ˆì¸¡ ê²°ê³¼",
        "activity_result": "ì‘ì—…í™œë™: {activity}",
        "hazard_result": "ì˜ˆì¸¡ëœ ìœ í•´ìœ„í—˜ìš”ì¸: {hazard}",
        "result_table_columns": ["í•­ëª©", "ê°’"],
        "result_table_rows": ["ë¹ˆë„", "ê°•ë„", "T ê°’", "ìœ„í—˜ë“±ê¸‰"],
        "parsing_error": "ìœ„í—˜ì„± í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        "gpt_response": "GPT ì›ë¬¸ ì‘ë‹µ: {response}",
        "phase2_header": "ê°œì„ ëŒ€ì±… ìë™ ìƒì„± (Phase 2)",
        "language_select_label": "ê°œì„ ëŒ€ì±… ì–¸ì–´ ì„ íƒ:",
        "input_method_label": "ì…ë ¥ ë°©ì‹ ì„ íƒ:",
        "input_methods": ["Phase 1 í‰ê°€ ê²°ê³¼ ì‚¬ìš©", "ì§ì ‘ ì…ë ¥"],
        "phase1_results_header": "Phase 1 í‰ê°€ ê²°ê³¼",
        "risk_level_text": "ìœ„í—˜ë„: ë¹ˆë„ {freq}, ê°•ë„ {intensity}, Tê°’ {t_value} (ë“±ê¸‰ {grade})",
        "phase1_first_warning": "ë¨¼ì € Phase 1ì—ì„œ ìœ„í—˜ì„± í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”.",
        "hazard_label": "ìœ í•´ìœ„í—˜ìš”ì¸:",
        "frequency_label": "ë¹ˆë„ (1-5):",
        "intensity_label": "ê°•ë„ (1-5):",
        "t_value_text": "Tê°’: {t_value} (ë“±ê¸‰: {grade})",
        "generate_improvement_btn": "ê°œì„ ëŒ€ì±… ìƒì„±",
        "generating_improvement": "ê°œì„ ëŒ€ì±…ì„ ìƒì„±í•˜ëŠ” ì¤‘...",
        "no_data_warning": "Phase 1ì—ì„œ ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±ì„ ì™„ë£Œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì˜ˆì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
        "improvement_result_header": "ê°œì„ ëŒ€ì±… ìƒì„± ê²°ê³¼",
        "improvement_plan_header": "ê°œì„ ëŒ€ì±…",
        "risk_improvement_header": "ìœ„í—˜ë„ ê°œì„  ê²°ê³¼",
        "comparison_columns": ["í•­ëª©", "ê°œì„  ì „", "ê°œì„  í›„"],
        "risk_reduction_label": "ìœ„í—˜ ê°ì†Œìœ¨ (RRR)",
        "t_value_change_header": "ìœ„í—˜ë„(Tê°’) ë³€í™”",
        "before_improvement": "ê°œì„  ì „ Tê°’:",
        "after_improvement": "ê°œì„  í›„ Tê°’:",
        "parsing_error_improvement": "ê°œì„ ëŒ€ì±… ìƒì„± ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    }
  
}

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Artificial Intelligence Risk Assessment",
    page_icon="ğŸ› ï¸",
    layout="wide"
)

# ìŠ¤íƒ€ì¼ ì ìš©
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E88E5;
        text-align: center;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.8rem;
        color: #0D47A1;
        margin-top: 2rem;
        margin-bottom: 1rem;
    }
    .metric-container {
        background-color: #f0f2f6;
        border-radius: 10px;
        padding: 20px;
        box-shadow: 2px 2px 5px rgba(0,0,0,0.1);
    }
    .info-text {
        font-size: 1rem;
        color: #424242;
        margin-bottom: 1rem;
    }
    .highlight {
        background-color: #e3f2fd;
        padding: 5px;
        border-radius: 5px;
    }
    .result-box {
        background-color: #f8f9fa;
        border-radius: 10px;
        padding: 15px;
        margin-top: 10px;
        margin-bottom: 10px;
        border-left: 5px solid #4CAF50;
    }
    .phase-badge {
        background-color: #4CAF50;
        color: white;
        padding: 5px 10px;
        border-radius: 15px;
        font-size: 0.8rem;
        margin-right: 10px;
    }
    .similar-case {
        background-color: #f1f8e9;
        border-radius: 8px;
        padding: 12px;
        margin-bottom: 8px;
        border-left: 4px solid #689f38;
    }
    .language-selector {
        position: absolute;
        top: 10px;
        right: 10px;
        z-index: 1000;
    }
</style>
""", unsafe_allow_html=True)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "index" not in st.session_state:
    st.session_state.index = None
if "embeddings" not in st.session_state:
    st.session_state.embeddings = None
if "retriever_pool_df" not in st.session_state:
    st.session_state.retriever_pool_df = None
if "language" not in st.session_state:
    st.session_state.language = "Korean"

# ìƒë‹¨ì— ì–¸ì–´ ì„ íƒê¸° ì¶”ê°€
col1, col2 = st.columns([6, 1])
with col2:
    selected_language = st.selectbox(
        "",
        options=list(system_texts.keys()),
        index=list(system_texts.keys()).index(st.session_state.language) if st.session_state.language in system_texts else 0,
        key="language_selector"
    )
    st.session_state.language = selected_language

# í˜„ì¬ ì–¸ì–´ì— ë”°ë¥¸ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
texts = system_texts[st.session_state.language]

# í—¤ë” í‘œì‹œ
st.markdown(f'<div class="main-header">{texts["title"]}</div>', unsafe_allow_html=True)

# íƒ­ ì„¤ì •
tabs = st.tabs([texts["tab_overview"], texts["tab_phase1"], texts["tab_phase2"]])

# ------------------ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ------------------

# ë¹ˆë„*ê°•ë„ ê²°ê³¼ Tì— ë”°ë¥¸ ë“±ê¸‰ ê²°ì • í•¨ìˆ˜
def determine_grade(value):
    """ë¹ˆë„*ê°•ë„ ê²°ê³¼ Tì— ë”°ë¥¸ ë“±ê¸‰ ê²°ì • í•¨ìˆ˜."""
    if 16 <= value <= 25:
        return 'A'
    elif 10 <= value <= 15:
        return 'B'
    elif 5 <= value <= 9:
        return 'C'
    elif 3 <= value <= 4:
        return 'D'
    elif 1 <= value <= 2:
        return 'E'
    else:
        return 'ì•Œ ìˆ˜ ì—†ìŒ' if st.session_state.language == 'Korean' else 'Unknown'

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜
def load_data(selected_dataset_name):
    """ì„ íƒëœ ì´ë¦„ì— ëŒ€ì‘í•˜ëŠ” Excel ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°."""
    try:
        # ì‹¤ì œ Excel íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
        df = pd.read_excel(f"{selected_dataset_name}.xlsx")

        # ì „ì²˜ë¦¬
        if 'ì‚­ì œ Del' in df.columns:
            df = df.drop(['ì‚­ì œ Del'], axis=1)
        df = df.iloc[1:]
        df = df.rename(columns={df.columns[4]: 'ë¹ˆë„'})
        df = df.rename(columns={df.columns[5]: 'ê°•ë„'})

        df['T'] = pd.to_numeric(df.iloc[:, 4]) * pd.to_numeric(df.iloc[:, 5])
        df = df.iloc[:, :7]
        df.rename(
            columns={
                'ì‘ì—…í™œë™ ë° ë‚´ìš©\nWork & Contents': 'ì‘ì—…í™œë™ ë° ë‚´ìš©',
                'ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥\nHazard & Risk': 'ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥',
                'í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥\nDamage & Effect': 'í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥'
            },
            inplace=True
        )
        df = df.rename(columns={df.columns[6]: 'T'})
        df['ë“±ê¸‰'] = df['T'].apply(determine_grade)

        return df
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.write(f"ì‹œë„í•œ íŒŒì¼ ê²½ë¡œ: {selected_dataset_name}")
        
        # íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ëŒ€ë¹„í•œ ë”ë¯¸ ë°ì´í„° ìƒì„± (ë°ëª¨ìš©)
        st.warning("Excel íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì‹¤ì œ ìš´ì˜ ì‹œì—ëŠ” í•´ë‹¹ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        data = {
            "ì‘ì—…í™œë™ ë° ë‚´ìš©": ["Shoring Installation", "In and Out of materials", "Transport / Delivery", "Survey and Inspection"],
            "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥": ["Fall and collision due to unstable ground", "Overturning of transport vehicle", 
                                 "Collision between transport vehicle", "Personnel fall while inspecting"],
            "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥": ["Injury", "Equipment damage", "Collision injury", "Fall injury"],
            "ë¹ˆë„": [3, 3, 3, 2],
            "ê°•ë„": [2, 3, 5, 3]
        }
        
        df = pd.DataFrame(data)
        df['T'] = df['ë¹ˆë„'] * df['ê°•ë„']
        df['ë“±ê¸‰'] = df['T'].apply(determine_grade)
        
        return df

# OpenAI ì„ë² ë”© APIë¥¼ í†µí•´ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
def embed_texts_with_openai(texts, model="text-embedding-3-large", api_key=None):
    """OpenAI ì„ë² ë”© APIë¡œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©."""
    if api_key:
        openai.api_key = api_key
    
    embeddings = []
    progress_bar = st.progress(0)
    total = len(texts)

    for idx, text in enumerate(texts):
        try:
            text = str(text).replace("\n", " ")
            response = openai.Embedding.create(model=model, input=[text])
            embedding = response["data"][0]["embedding"]
            embeddings.append(embedding)
        except Exception as e:
            st.error(f"í…ìŠ¤íŠ¸ ì„ë² ë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            embeddings.append([0]*1536)
        
        progress_bar.progress((idx + 1) / total)
    
    return embeddings

# GPT ëª¨ë¸ì„ í†µí•´ ì˜ˆì¸¡ ê²°ê³¼ ìƒì„±
def generate_with_gpt(prompt, api_key=None, model="gpt-4o", language="Korean"):
    """GPT ëª¨ë¸ë¡œë¶€í„° ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ëŠ” í•¨ìˆ˜."""
    if api_key:
        openai.api_key = api_key
        
    # ì–¸ì–´ì— ë”°ë¥¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    system_prompts = {
        "Korean": "ìœ„í—˜ì„± í‰ê°€ ë° ê°œì„ ëŒ€ì±… ìƒì„±ì„ ë•ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ì„¸ìš”.",
    }
    
    system_prompt = system_prompts.get(language, system_prompts["Korean"])
        
    try:
        response = openai.ChatCompletion.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            temperature=0.0,
            max_tokens=250
        )
        return response['choices'][0]['message']['content'].strip()
    except Exception as e:
        st.error(f"GPT API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

# ----- Phase 1: ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡ ê´€ë ¨ í•¨ìˆ˜ -----

# ê²€ìƒ‰ëœ ë¬¸ì„œë¡œ GPT í”„ë¡¬í”„íŠ¸ ìƒì„± (Phase 1 - ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡)
def construct_prompt_phase1_hazard(retrieved_docs, activity_text, language="Korean"):
    """ì‘ì—…í™œë™ìœ¼ë¡œë¶€í„° ìœ í•´ìœ„í—˜ìš”ì¸ì„ ì˜ˆì¸¡í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±."""
    # ì–¸ì–´ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    prompt_templates = {
        "Korean": {
            "intro": "ë‹¤ìŒì€ ê±´ì„¤ í˜„ì¥ì˜ ì‘ì—…í™œë™ê³¼ ê·¸ì— ë”°ë¥¸ ìœ í•´ìœ„í—˜ìš”ì¸ì˜ ì˜ˆì‹œì…ë‹ˆë‹¤:\n\n",
            "example_format": "ì˜ˆì‹œ {i}:\nì‘ì—…í™œë™: {activity}\nìœ í•´ìœ„í—˜ìš”ì¸: {hazard}\n\n",
            "query_format": "ì´ì œ ë‹¤ìŒ ì‘ì—…í™œë™ì— ëŒ€í•œ ìœ í•´ìœ„í—˜ìš”ì¸ì„ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”:\nì‘ì—…í™œë™: {activity}\nìœ í•´ìœ„í—˜ìš”ì¸: "
        }
    }
    
    # í˜„ì¬ ì–¸ì–´ì˜ í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°
    template = prompt_templates.get(language, prompt_templates["Korean"])
    
    retrieved_examples = []
    for _, doc in retrieved_docs.iterrows():
        try:
            activity = doc['ì‘ì—…í™œë™ ë° ë‚´ìš©']
            hazard = doc['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥']
            retrieved_examples.append((activity, hazard))
        except:
            continue
    
    prompt = template["intro"]
    for i, (activity, hazard) in enumerate(retrieved_examples, 1):
        prompt += template["example_format"].format(i=i, activity=activity, hazard=hazard)
    
    prompt += template["query_format"].format(activity=activity_text)
    
    return prompt

# ë¹ˆë„ì™€ ê°•ë„ ì˜ˆì¸¡ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± (Phase 1)
def construct_prompt_phase1_risk(retrieved_docs, activity_text, hazard_text, language="Korean"):
    """ì‘ì—…í™œë™ê³¼ ìœ í•´ìœ„í—˜ìš”ì¸ì„ ë°”íƒ•ìœ¼ë¡œ ë¹ˆë„ì™€ ê°•ë„ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±."""
    # ì–¸ì–´ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    prompt_templates = {
        "Korean": {
            "example_format": "ì˜ˆì‹œ {i}:\nì…ë ¥: {input}\nì¶œë ¥: {output}\n\n",
            "query_format": "ì…ë ¥: {activity} - {hazard}\nìœ„ ì…ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ë¹ˆë„ì™€ ê°•ë„ë¥¼ ì˜ˆì¸¡í•˜ì„¸ìš”. ë¹ˆë„ëŠ” 1ì—ì„œ 5 ì‚¬ì´ì˜ ì •ìˆ˜ì…ë‹ˆë‹¤. ê°•ë„ëŠ” 1ì—ì„œ 5 ì‚¬ì´ì˜ ì •ìˆ˜ì…ë‹ˆë‹¤. TëŠ” ë¹ˆë„ì™€ ê°•ë„ë¥¼ ê³±í•œ ê°’ì…ë‹ˆë‹¤.\në‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:\n{json_format}\nì¶œë ¥:\n"
        }
        
    }
    
    # JSON í˜•ì‹ ì–¸ì–´ë³„ ì •ì˜
    json_formats = {
        "Korean": '{"ë¹ˆë„": ìˆ«ì, "ê°•ë„": ìˆ«ì, "T": ìˆ«ì}'
    }
    
    # í˜„ì¬ ì–¸ì–´ì˜ í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°
    template = prompt_templates.get(language, prompt_templates["Korean"])
    json_format = json_formats.get(language, json_formats["Korean"])
    
    retrieved_examples = []
    for _, doc in retrieved_docs.iterrows():
        try:
            example_input = f"{doc['ì‘ì—…í™œë™ ë° ë‚´ìš©']} - {doc['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥']}"
            frequency = int(doc['ë¹ˆë„'])
            intensity = int(doc['ê°•ë„'])
            T_value = frequency * intensity
            
            # ì–¸ì–´ë³„ JSON ì¶œë ¥ í˜•ì‹
            if language == "Korean":
                example_output = f'{{"ë¹ˆë„": {frequency}, "ê°•ë„": {intensity}, "T": {T_value}}}'
            else:
                example_output = f'{{"ë¹ˆë„": {frequency}, "ê°•ë„": {intensity}, "T": {T_value}}}'
                
            retrieved_examples.append((example_input, example_output))
        except:
            continue
    
    prompt = ""
    for i, (example_input, example_output) in enumerate(retrieved_examples, 1):
        prompt += template["example_format"].format(i=i, input=example_input, output=example_output)
    
    prompt += template["query_format"].format(
        activity=activity_text, 
        hazard=hazard_text,
        json_format=json_format
    )
    
    return prompt

# GPT ì¶œë ¥ íŒŒì‹± (Phase 1)
def parse_gpt_output_phase1(gpt_output, language="Korean"):
    """GPT ì¶œë ¥ì—ì„œ {ë¹ˆë„, ê°•ë„, T}ë¥¼ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¶”ì¶œ."""
    # ì–¸ì–´ë³„ JSON íŒ¨í„´
    json_patterns = {
        "Korean": r'\{"ë¹ˆë„":\s*([1-5]),\s*"ê°•ë„":\s*([1-5]),\s*"T":\s*([0-9]+)\}'
    }
    
    pattern = json_patterns.get(language, json_patterns["Korean"])
    match = re.search(pattern, gpt_output)
    
    if match:
        pred_frequency = int(match.group(1))
        pred_intensity = int(match.group(2))
        pred_T = int(match.group(3))
        return pred_frequency, pred_intensity, pred_T
    else:
        # ë‹¤ë¥¸ íŒ¨í„´ë„ ì‹œë„ (GPTê°€ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ì‘ë‹µí–ˆì„ ê²½ìš°)
        for lang, pattern in json_patterns.items():
            if lang != language:  # ì´ë¯¸ ì‹œë„í•œ ì–¸ì–´ëŠ” ê±´ë„ˆë›´ë‹¤
                match = re.search(pattern, gpt_output)
                if match:
                    pred_frequency = int(match.group(1))
                    pred_intensity = int(match.group(2))
                    pred_T = int(match.group(3))
                    return pred_frequency, pred_intensity, pred_T
        
        return None

# ----- Phase 2: ê°œì„ ëŒ€ì±… ìƒì„± ê´€ë ¨ í•¨ìˆ˜ -----

# ìœ„í—˜ ê°ì†Œìœ¨(RRR) ê³„ì‚° í•¨ìˆ˜
def compute_rrr(T_before, T_after):
    """ìœ„í—˜ ê°ì†Œìœ¨(Risk Reduction Rate) ê³„ì‚°"""
    if T_before == 0:
        return 0.0
    return ((T_before - T_after) / T_before) * 100.0

# ê°œì„ ëŒ€ì±… ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (Phase 2)
def construct_prompt_phase2(retrieved_docs, activity_text, hazard_text, freq, intensity, T, target_language="Korean"):
    """
    ê°œì„ ëŒ€ì±… ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    """
    # ì˜ˆì‹œ ì„¹ì…˜ êµ¬ì„±
    example_section = ""
    examples_added = 0
    
    # ì–¸ì–´ë³„ í•„ë“œëª…
    field_names = {
        "Korean": {
            "improvement_fields": ['ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ', 'ê°œì„ ëŒ€ì±…', 'ê°œì„ ë°©ì•ˆ'],
            "activity": "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "hazard": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥",
            "freq": "ë¹ˆë„",
            "intensity": "ê°•ë„",
            "example_intro": "Example:",
            "input_activity": "Input (Activity): ",
            "input_hazard": "Input (Hazard): ",
            "input_freq": "Input (Original Frequency): ",
            "input_intensity": "Input (Original Intensity): ",
            "input_t": "Input (Original T): ",
            "output_intro": "Output (Improvement Plan and Risk Reduction) in JSON:",
            "improvement": "ê°œì„ ëŒ€ì±…",
            "improved_freq": "ê°œì„  í›„ ë¹ˆë„",
            "improved_intensity": "ê°œì„  í›„ ê°•ë„",
            "improved_t": "ê°œì„  í›„ T",
            "reduction_rate": "T ê°ì†Œìœ¨"
        }
    }
    
    # í˜„ì¬ ì–¸ì–´ì— ë§ëŠ” í•„ë“œëª… ê°€ì ¸ì˜¤ê¸°
    fields = field_names.get(target_language, field_names["Korean"])
    
    for _, row in retrieved_docs.iterrows():
        try:
            # Phase2 ë°ì´í„°ì…‹ì— ìˆëŠ” ê°œì„ ëŒ€ì±… í•„ë“œ ì‚¬ìš© ì‹œë„
            improvement_plan = ""
            for field in fields["improvement_fields"]:
                if field in row and pd.notna(row[field]):
                    improvement_plan = row[field]
                    break
            
            if not improvement_plan:
                continue  # ê°œì„ ëŒ€ì±…ì´ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                
            original_freq = int(row[fields["freq"]]) if fields["freq"] in row else 3
            original_intensity = int(row[fields["intensity"]]) if fields["intensity"] in row else 3
            original_T = original_freq * original_intensity
                
            # ê°œì„  í›„ ë°ì´í„° ì‹œë„
            improved_freq = 1
            improved_intensity = 1
            improved_T = 1
            
            for field_pattern in [('ê°œì„  í›„ ë¹ˆë„', 'ê°œì„  í›„ ê°•ë„', 'ê°œì„  í›„ T'), ('ê°œì„ ë¹ˆë„', 'ê°œì„ ê°•ë„', 'ê°œì„ T')]:
                if all(field in row for field in field_pattern):
                    improved_freq = int(row[field_pattern[0]])
                    improved_intensity = int(row[field_pattern[1]])
                    improved_T = int(row[field_pattern[2]])
                    break
            
            example_section += (
                f"{fields['example_intro']}\n"
                f"{fields['input_activity']}{row[fields['activity']]}\n"
                f"{fields['input_hazard']}{row[fields['hazard']]}\n"
                f"{fields['input_freq']}{original_freq}\n"
                f"{fields['input_intensity']}{original_intensity}\n"
                f"{fields['input_t']}{original_T}\n"
                f"{fields['output_intro']}\n"
                "{\n"
                f'  "{fields["improvement"]}": "{improvement_plan}",\n'
                f'  "{fields["improved_freq"]}": {improved_freq},\n'
                f'  "{fields["improved_intensity"]}": {improved_intensity},\n'
                f'  "{fields["improved_t"]}": {improved_T},\n'
                f'  "{fields["reduction_rate"]}": {compute_rrr(original_T, improved_T):.2f}\n'
                "}\n\n"
            )
            
            examples_added += 1
            if examples_added >= 3:  # ìµœëŒ€ 3ê°œ ì˜ˆì‹œë§Œ ì‚¬ìš©
                break
                
        except Exception as e:
            # ì—ëŸ¬ ë°œìƒ ì‹œ í•´ë‹¹ ì˜ˆì‹œ ê±´ë„ˆë›°ê¸°
            continue
    
    # ì˜ˆì‹œê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì˜ˆì‹œ ì¶”ê°€ (ì–¸ì–´ë³„)
    if examples_added == 0:
        # í•œêµ­ì–´ ê¸°ë³¸ ì˜ˆì‹œ
        if target_language == "Korean":
            example_section = """
Example:
Input (Activity): Excavation and backfilling
Input (Hazard): Collapse of excavation wall due to improper sloping
Input (Original Frequency): 3
Input (Original Intensity): 4
Input (Original T): 12
Output (Improvement Plan and Risk Reduction) in JSON:
{
  "ê°œì„ ëŒ€ì±…": "1) í† ì–‘ ë¶„ë¥˜ì— ë”°ë¥¸ ì ì ˆí•œ ê²½ì‚¬ ìœ ì§€ 2) êµ´ì°© ë²½ë©´ ë³´ê°• 3) ì •ê¸°ì ì¸ ì§€ë°˜ ìƒíƒœ ê²€ì‚¬ ì‹¤ì‹œ",
  "ê°œì„  í›„ ë¹ˆë„": 1,
  "ê°œì„  í›„ ê°•ë„": 2,
  "ê°œì„  í›„ T": 2,
  "T ê°ì†Œìœ¨": 83.33
}

Example:
Input (Activity): Lifting operation
Input (Hazard): Material fall due to improper rigging
Input (Original Frequency): 2
Input (Original Intensity): 5
Input (Original T): 10
Output (Improvement Plan and Risk Reduction) in JSON:
{
  "ê°œì„ ëŒ€ì±…": "1) ë¦¬ê¹… ì „ë¬¸ê°€ ì‘ì—… ì°¸ì—¬ 2) ë¦¬ê¹… ì¥ë¹„ ì‚¬ì „ ì ê²€ 3) ì•ˆì „ êµ¬ì—­ ì„¤ì • ë° ì ‘ê·¼ í†µì œ",
  "ê°œì„  í›„ ë¹ˆë„": 1,
  "ê°œì„  í›„ ê°•ë„": 2,
  "ê°œì„  í›„ T": 2,
  "T ê°ì†Œìœ¨": 80.00
}
"""

    
    # ì–¸ì–´ë³„ JSON ì¶œë ¥ í‚¤ ì´ë¦„
    json_keys = {
        "Korean": {
            "improvement": "ê°œì„ ëŒ€ì±…",
            "improved_freq": "ê°œì„  í›„ ë¹ˆë„",
            "improved_intensity": "ê°œì„  í›„ ê°•ë„",
            "improved_t": "ê°œì„  í›„ T",
            "reduction_rate": "T ê°ì†Œìœ¨"
        }
    }
    
    # ê° ì–¸ì–´ë³„ ì•ˆë‚´ ë©”ì‹œì§€
    instructions = {
        "Korean": {
            "new_input": "ë‹¤ìŒì€ ìƒˆë¡œìš´ ì…ë ¥ì…ë‹ˆë‹¤:",
            "input_activity": "ì…ë ¥ (ì‘ì—…í™œë™): ",
            "input_hazard": "ì…ë ¥ (ìœ í•´ìœ„í—˜ìš”ì¸): ",
            "input_freq": "ì…ë ¥ (ì›ë˜ ë¹ˆë„): ",
            "input_intensity": "ì…ë ¥ (ì›ë˜ ê°•ë„): ",
            "input_t": "ì…ë ¥ (ì›ë˜ T): ",
            "output_format": "ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ì„ ì œê³µí•˜ì„¸ìš”:",
            "improvement_write": "ê°œì„ ëŒ€ì±…(ê°œì„ ëŒ€ì±…)ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.",
            "provide_measures": "ìµœì†Œ 3ê°œì˜ êµ¬ì²´ì ì¸ ê°œì„  ì¡°ì¹˜ë¥¼ ë²ˆí˜¸ê°€ ë§¤ê²¨ì§„ ëª©ë¡ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”.",
            "valid_json": "ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•˜ë„ë¡ í•˜ì„¸ìš”.",
            "output": "ì¶œë ¥:"
        }
    }
    
    # í˜„ì¬ ì–¸ì–´ì˜ í‚¤ì™€ ì•ˆë‚´ ë©”ì‹œì§€
    keys = json_keys.get(target_language, json_keys["Korean"])
    instr = instructions.get(target_language, instructions["Korean"])
    
    # ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = (
        f"{example_section}"
        f"{instr['new_input']}\n"
        f"{instr['input_activity']}{activity_text}\n"
        f"{instr['input_hazard']}{hazard_text}\n"
        f"{instr['input_freq']}{freq}\n"
        f"{instr['input_intensity']}{intensity}\n"
        f"{instr['input_t']}{T}\n\n"
        f"{instr['output_format']}\n"
        "{\n"
        f'  "{keys["improvement"]}": "í•­ëª©ë³„ ê°œì„ ëŒ€ì±… ë¦¬ìŠ¤íŠ¸", \n'
        f'  "{keys["improved_freq"]}": (an integer in [1..5]),\n'
        f'  "{keys["improved_intensity"]}": (an integer in [1..5]),\n'
        f'  "{keys["improved_t"]}": (Improved Frequency * Improved Severity),\n'
        f'  "{keys["reduction_rate"]}": (percentage of risk reduction)\n'
        "}\n\n"
        f"{instr['improvement_write']}\n"
        f"{instr['provide_measures']}\n"
        f"{instr['valid_json']}\n"
        f"{instr['output']}\n"
    )
    
    return prompt

# GPT ì‘ë‹µ íŒŒì‹± (Phase 2)
def parse_gpt_output_phase2(gpt_output, language="Korean"):
    """GPT ì‘ë‹µì—ì„œ JSON ë°ì´í„°ë¥¼ ì¶”ì¶œ"""
    try:
        # JSON ë¸”ë¡ì´ ìˆëŠ” ê²½ìš° ì¶”ì¶œ ì‹œë„
        pattern = re.compile(r"```json(.*?)```", re.DOTALL)
        match = pattern.search(gpt_output)

        if match:
            json_str = match.group(1).strip()
        else:
            # JSON ë¸”ë¡ í‘œì‹œê°€ ì—†ëŠ” ê²½ìš° ì›ë¬¸ì„ JSONìœ¼ë¡œ íŒŒì‹± ì‹œë„
            json_str = gpt_output.replace("```", "").strip()

        import json
        result = json.loads(json_str)
        
        # ì–¸ì–´ë³„ í‚¤ ë§¤í•‘
        key_mappings = {
            "Korean": {
                "improvement": ["ê°œì„ ëŒ€ì±…"],
                "improved_freq": ["ê°œì„  í›„ ë¹ˆë„", "ê°œì„ ë¹ˆë„"],
                "improved_intensity": ["ê°œì„  í›„ ê°•ë„", "ê°œì„ ê°•ë„"],
                "improved_t": ["ê°œì„  í›„ T", "ê°œì„ T", "ê°œì„  í›„ t"],
                "reduction_rate": ["T ê°ì†Œìœ¨", "ê°ì†Œìœ¨", "ìœ„í—˜ ê°ì†Œìœ¨"]
            }
        }
        
        # ê²°ê³¼ ë§¤í•‘
        mapped_result = {}
        
        # í˜„ì¬ ì–¸ì–´ì˜ í‚¤ ë§¤í•‘ ê°€ì ¸ì˜¤ê¸°
        mappings = key_mappings.get(language, key_mappings["Korean"])
        
        # ê°œì„ ëŒ€ì±… í‚¤ ë§¤í•‘
        for result_key, possible_keys in mappings.items():
            for key in possible_keys:
                if key in result:
                    mapped_result[result_key] = result[key]
                    break
            
        return mapped_result
    except Exception as e:
        st.error(f"JSON íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.write("ì›ë³¸ GPT ì‘ë‹µ:", gpt_output)
        return None

# ------------------ ë°ì´í„°ì…‹ ë° ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„ ------------------

# ë°ì´í„°ì…‹ ì˜µì…˜
dataset_options = {
    "SWRO ê±´ì¶•ê³µì • (ê±´ì¶•)": "SWRO ê±´ì¶•ê³µì • (ê±´ì¶•)",
    "Civil (í† ëª©)": "Civil (í† ëª©)",
    "Marine (í† ëª©)": "Marine (í† ëª©)",
    "SWRO ê¸°ê³„ê³µì‚¬ (í”ŒëœíŠ¸)": "SWRO ê¸°ê³„ê³µì‚¬ (í”ŒëœíŠ¸)",
    "SWRO ì „ê¸°ì‘ì—…í‘œì¤€ (í”ŒëœíŠ¸)": "SWRO ì „ê¸°ì‘ì—…í‘œì¤€ (í”ŒëœíŠ¸)"
}

# ----- ì‹œìŠ¤í…œ ê°œìš” íƒ­ -----
with tabs[0]:
    st.markdown(f'<div class="sub-header">{texts["overview_header"]}</div>', unsafe_allow_html=True)
    
    col1, col2 = st.columns([3, 2])
    
    with col1:
        st.markdown(f"""
        <div class="info-text">
        {texts["overview_text"]}
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        # AI ìœ„í—˜ì„±í‰ê°€ í”„ë¡œì„¸ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨
        st.markdown(f'<div style="text-align: center; margin-bottom: 10px;"><b>{texts["process_title"]}</b></div>', unsafe_allow_html=True)
        
        steps = texts["process_steps"]
        
        for i, step in enumerate(steps):
            phase_badge = '<span class="phase-badge">Phase 1</span>' if i < 4 else '<span class="phase-badge">Phase 2</span>'
            st.markdown(f"**{i+1}. {step}** {phase_badge}" + (" â†’ " if i < len(steps)-1 else ""), unsafe_allow_html=True)
    
    # ì‹œìŠ¤í…œ íŠ¹ì§•
    st.markdown(f'<div class="sub-header">{texts["features_title"]}</div>', unsafe_allow_html=True)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown(texts["phase1_features"], unsafe_allow_html=True)
    
    with col2:
        st.markdown(texts["phase2_features"], unsafe_allow_html=True)

# ----- Phase 1: ìœ„í—˜ì„± í‰ê°€ íƒ­ -----
with tabs[1]:
    st.markdown(f'<div class="sub-header">{texts["phase1_header"]}</div>', unsafe_allow_html=True)
    
    # API í‚¤ ì…ë ¥
    api_key = st.text_input(texts["api_key_label"], type="password", key="api_key_phase1")
    
    # ë°ì´í„°ì…‹ ì„ íƒ
    selected_dataset_name = st.selectbox(
        texts["dataset_label"],
        options=list(dataset_options.keys()),
        key="dataset_selector_phase1"
    )
    
    # ì¸ë±ìŠ¤ êµ¬ì„± ì„¹ì…˜
    st.markdown(f"### {texts['load_data_label']}")
    
    if st.button(texts["load_data_btn"], key="load_data_phase1"):
        if not api_key:
            st.warning(texts["api_key_warning"])
        else:
            with st.spinner(texts["data_loading"]):
                # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
                df = load_data(dataset_options[selected_dataset_name])
                
                if df is not None:
                    # Train/Test ë¶„í• 
                    train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)
                    
                    # ë¦¬íŠ¸ë¦¬ë²„ í’€ êµ¬ì„±
                    retriever_pool_df = train_df.copy()
                    retriever_pool_df['content'] = retriever_pool_df.apply(
                        lambda row: ' '.join(row.values.astype(str)), axis=1
                    )
                    texts_to_embed = retriever_pool_df['content'].tolist()
                    
                    # ì„ë² ë”© ìƒì„± (ë°ëª¨ì—ì„œëŠ” ì ì€ ìˆ˜ë§Œ ì²˜ë¦¬, ì‹¤ì œë¡œëŠ” ì „ì²´ ì²˜ë¦¬)
                    max_texts = min(len(texts_to_embed), 10)  # ë°ëª¨ì—ì„œëŠ” ìµœëŒ€ 10ê°œë§Œ ì²˜ë¦¬
                    st.info(texts["demo_limit_info"].format(max_texts=max_texts))
                    
                    openai.api_key = api_key
                    embeddings = embed_texts_with_openai(texts_to_embed[:max_texts], api_key=api_key)
                    
                    # FAISS ì¸ë±ìŠ¤ êµ¬ì„±
                    embeddings_array = np.array(embeddings, dtype='float32')
                    dimension = embeddings_array.shape[1]
                    faiss_index = faiss.IndexFlatL2(dimension)
                    faiss_index.add(embeddings_array)
                    
                    # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                    st.session_state.index = faiss_index
                    st.session_state.embeddings = embeddings_array
                    st.session_state.retriever_pool_df = retriever_pool_df.iloc[:max_texts]  # ì„ë² ë”©ëœ ë¶€ë¶„ë§Œ ì €ì¥
                    
                    st.success(texts["data_load_success"].format(max_texts=max_texts))
                    st.session_state.test_df = test_df
    
    # ì‚¬ìš©ì ì…ë ¥ ì˜ˆì¸¡ ì„¹ì…˜
    st.markdown(f"### {texts['hazard_prediction_header']}")
    
    if st.session_state.index is None:
        st.warning(texts["load_first_warning"])
    else:
        with st.form("user_input_form"):
            user_work = st.text_input(texts["activity_label"], key="form_user_work")
            submitted = st.form_submit_button(texts["predict_hazard_btn"])
            
        if submitted:
            if not user_work:
                st.warning(texts["activity_warning"])
            else:
                with st.spinner(texts["predicting_hazard"]):
                    # ì¿¼ë¦¬ ì„ë² ë”©
                    query_embedding = embed_texts_with_openai([user_work], api_key=api_key)[0]
                    query_embedding_array = np.array([query_embedding], dtype='float32')
                    
                    # ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
                    k_similar = min(3, len(st.session_state.retriever_pool_df))
                    distances, indices = st.session_state.index.search(query_embedding_array, k_similar)
                    retrieved_docs = st.session_state.retriever_pool_df.iloc[indices[0]]
                    
                    # ìœ ì‚¬í•œ ì‚¬ë¡€ í‘œì‹œ
                    st.markdown(f"#### {texts['similar_cases_header']}")
                    for i, (_, doc) in enumerate(retrieved_docs.iterrows(), 1):
                        st.markdown(
                            texts["similar_case_text"].format(
                                i=i,
                                activity=doc['ì‘ì—…í™œë™ ë° ë‚´ìš©'],
                                hazard=doc['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥'],
                                freq=doc['ë¹ˆë„'],
                                intensity=doc['ê°•ë„'],
                                t_value=doc['T'],
                                grade=doc['ë“±ê¸‰']
                            ), 
                            unsafe_allow_html=True
                        )
                    
                    # GPT í”„ë¡¬í”„íŠ¸ ìƒì„± & í˜¸ì¶œ (ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡)
                    hazard_prompt = construct_prompt_phase1_hazard(retrieved_docs, user_work, language=st.session_state.language)
                    hazard_prediction = generate_with_gpt(hazard_prompt, api_key=api_key, language=st.session_state.language)
                    
                    # ë¹ˆë„ì™€ ê°•ë„ ì˜ˆì¸¡ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± & í˜¸ì¶œ
                    risk_prompt = construct_prompt_phase1_risk(retrieved_docs, user_work, hazard_prediction, language=st.session_state.language)
                    risk_prediction = generate_with_gpt(risk_prompt, api_key=api_key, language=st.session_state.language)
                    
                    # ê²°ê³¼ í‘œì‹œ
                    st.markdown(f"#### {texts['prediction_result_header']}")
                    st.markdown(texts["activity_result"].format(activity=user_work))
                    st.markdown(texts["hazard_result"].format(hazard=hazard_prediction))
                    
                    parse_result = parse_gpt_output_phase1(risk_prediction, language=st.session_state.language)
                    if parse_result is not None:
                        f_val, i_val, t_val = parse_result
                        grade = determine_grade(t_val)
                        
                        # ê²°ê³¼ë¥¼ í‘œë¡œ í‘œì‹œ
                        result_df = pd.DataFrame({
                            texts["result_table_columns"][0]: texts["result_table_rows"],
                            texts["result_table_columns"][1]: [f_val, i_val, t_val, grade]
                        })
                        
                        st.markdown('<div class="result-box">', unsafe_allow_html=True)
                        st.table(result_df)
                        st.markdown('</div>', unsafe_allow_html=True)
                        
                        # ì„¸ì…˜ ìƒíƒœì— ê²°ê³¼ ì €ì¥ (Phase 2ì—ì„œ ì‚¬ìš©)
                        st.session_state.last_assessment = {
                            'activity': user_work,
                            'hazard': hazard_prediction,
                            'frequency': f_val,
                            'intensity': i_val,
                            'T': t_val,
                            'grade': grade
                        }
                    else:
                        st.error(texts["parsing_error"])
                        st.write(texts["gpt_response"].format(response=risk_prediction))

# ----- Phase 2: ê°œì„ ëŒ€ì±… ìƒì„± íƒ­ -----
with tabs[2]:
    st.markdown(f'<div class="sub-header">{texts["phase2_header"]}</div>', unsafe_allow_html=True)
    
    # API í‚¤ ì…ë ¥
    api_key_phase2 = st.text_input(texts["api_key_label"], type="password", key="api_key_phase2")
    
    # ê°œì„ ëŒ€ì±… ì–¸ì–´ ì„ íƒ
    target_language = st.selectbox(
        texts["language_select_label"],
        options=list(system_texts.keys()),
        index=list(system_texts.keys()).index(st.session_state.language),
        key="target_language"
    )
    
    # ì…ë ¥ ë°©ì‹ ì„ íƒ (Phase 1 ê²°ê³¼ ì‚¬ìš© ë˜ëŠ” ì§ì ‘ ì…ë ¥)
    input_method = st.radio(
        texts["input_method_label"],
        options=texts["input_methods"],
        index=0,
        key="input_method"
    )
    
    if input_method == texts["input_methods"][0]:  # Phase 1 í‰ê°€ ê²°ê³¼ ì‚¬ìš©
        # Phase 1 ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
        if hasattr(st.session_state, 'last_assessment'):
            last_assessment = st.session_state.last_assessment
            
            st.markdown(f"### {texts['phase1_results_header']}")
            st.markdown(f"**{texts['activity_label'].strip(':')}** {last_assessment['activity']}")
            st.markdown(f"**{texts['hazard_label'].strip(':')}** {last_assessment['hazard']}")
            st.markdown(
                texts["risk_level_text"].format(
                    freq=last_assessment['frequency'],
                    intensity=last_assessment['intensity'],
                    t_value=last_assessment['T'],
                    grade=last_assessment['grade']
                )
            )
            
            activity_text = last_assessment['activity']
            hazard_text = last_assessment['hazard']
            frequency = last_assessment['frequency']
            intensity = last_assessment['intensity']
            T_value = last_assessment['T']
            
        else:
            st.warning(texts["phase1_first_warning"])
            activity_text = hazard_text = None
            frequency = intensity = T_value = None
    else:
        # ì§ì ‘ ì…ë ¥í•˜ëŠ” ê²½ìš°
        col1, col2 = st.columns(2)
        
        with col1:
            activity_text = st.text_input(texts["activity_label"], key="direct_activity")
            hazard_text = st.text_input(texts["hazard_label"], key="direct_hazard")
        
        with col2:
            frequency = st.number_input(texts["frequency_label"], min_value=1, max_value=5, value=3, key="direct_freq")
            intensity = st.number_input(texts["intensity_label"], min_value=1, max_value=5, value=3, key="direct_intensity")
            T_value = frequency * intensity
            st.markdown(texts["t_value_text"].format(t_value=T_value, grade=determine_grade(T_value)))
    
    # ê°œì„ ëŒ€ì±… ìƒì„± ì„¹ì…˜
    if st.button(texts["generate_improvement_btn"], key="generate_improvement") and activity_text and hazard_text and frequency and intensity and T_value:
        if not api_key_phase2:
            st.warning(texts["api_key_warning"])
        else:
            with st.spinner(texts["generating_improvement"]):
                # ë¦¬íŠ¸ë¦¬ë²„ í’€ê³¼ ì¸ë±ìŠ¤ í™•ì¸
                if st.session_state.retriever_pool_df is None or st.session_state.index is None:
                    st.warning(texts["no_data_warning"])
                    # ê¸°ë³¸ ê³µí†µ ë°ì´í„°ì…‹ ë¡œë“œ
                    df = load_data("Civil (í† ëª©)")
                    retriever_pool_df = df.sample(min(5, len(df)))  # ìµœëŒ€ 5ê°œ ìƒ˜í”Œ
                    
                    # ìœ ì‚¬ ë¬¸ì„œëŠ” ëœë¤ ìƒ˜í”Œë§
                    retrieved_docs = retriever_pool_df.sample(min(3, len(retriever_pool_df)))
                else:
                    # Phase 1ì—ì„œ êµ¬ì„±ëœ ë¦¬íŠ¸ë¦¬ë²„ í’€ ë° ì¸ë±ìŠ¤ ì‚¬ìš©
                    retriever_pool_df = st.session_state.retriever_pool_df
                    
                    # ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (ì‹¤ì œ ë˜ëŠ” ëª¨ì˜)
                    k_similar = min(3, len(retriever_pool_df))
                    query_text = f"{activity_text} {hazard_text}"
                    query_embedding = embed_texts_with_openai([query_text], api_key=api_key_phase2)[0]
                    query_embedding_array = np.array([query_embedding], dtype='float32')
                    distances, indices = st.session_state.index.search(query_embedding_array, k_similar)
                    retrieved_docs = retriever_pool_df.iloc[indices[0]]
                    
                    # ìœ ì‚¬ ì‚¬ë¡€ í‘œì‹œ
                    st.markdown(f"#### {texts['similar_cases_header']}")
                    for i, (_, doc) in enumerate(retrieved_docs.iterrows(), 1):
                        st.markdown(
                            texts["similar_case_text"].format(
                                i=i,
                                activity=doc['ì‘ì—…í™œë™ ë° ë‚´ìš©'],
                                hazard=doc['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥'],
                                freq=doc['ë¹ˆë„'],
                                intensity=doc['ê°•ë„'],
                                t_value=doc['T'],
                                grade=doc['ë“±ê¸‰']
                            ), 
                            unsafe_allow_html=True
                        )
                
                # ê°œì„ ëŒ€ì±… ìƒì„± í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                prompt = construct_prompt_phase2(
                    retrieved_docs, 
                    activity_text, 
                    hazard_text, 
                    frequency, 
                    intensity, 
                    T_value, 
                    target_language
                )
                
                # GPT í˜¸ì¶œ
                generated_output = generate_with_gpt(prompt, api_key=api_key_phase2, language=target_language)
                
                # ê²°ê³¼ íŒŒì‹±
                parsed_result = parse_gpt_output_phase2(generated_output, language=target_language)
                
                if parsed_result:
                    # í‚¤ ì´ë¦„ ë§¤í•‘
                    key_mappings = {
                        "improvement": "ê°œì„ ëŒ€ì±…" if target_language == "Korean" else "improvement_plan",
                        "improved_freq": "ê°œì„  í›„ ë¹ˆë„" if target_language == "Korean" else "improved_frequency",
                        "improved_intensity": "ê°œì„  í›„ ê°•ë„" if target_language == "Korean" else "improved_intensity",
                        "improved_t": "ê°œì„  í›„ T" if target_language == "Korean" else "improved_T",
                        "reduction_rate": "T ê°ì†Œìœ¨" if target_language == "Korean" else "reduction_rate"
                    }
                    
                    # ê²°ê³¼ í‘œì‹œ
                    improvement_plan = parsed_result.get("improvement", "")
                    improved_freq = parsed_result.get("improved_freq", 1)
                    improved_intensity = parsed_result.get("improved_intensity", 1)
                    improved_T = parsed_result.get("improved_t", improved_freq * improved_intensity)
                    rrr = parsed_result.get("reduction_rate", compute_rrr(T_value, improved_T))
                    
                    st.markdown('<div class="result-box">', unsafe_allow_html=True)
                    st.markdown(f"#### {texts['improvement_result_header']}")
                    
                    col1, col2 = st.columns([3, 2])
                    
                    with col1:
                        st.markdown(f"##### {texts['improvement_plan_header']}")
                        st.markdown(improvement_plan)
                    
                    with col2:
                        st.markdown(f"##### {texts['risk_improvement_header']}")
                        
                        # ê°œì„  ì „í›„ ìœ„í—˜ë„ ë¹„êµí‘œ
                        comparison_df = pd.DataFrame({
                            texts["comparison_columns"][0]: texts["result_table_rows"],
                            texts["comparison_columns"][1]: [frequency, intensity, T_value, determine_grade(T_value)],
                            texts["comparison_columns"][2]: [improved_freq, improved_intensity, improved_T, determine_grade(improved_T)]
                        })
                        st.table(comparison_df)
                        
                        # ìœ„í—˜ ê°ì†Œìœ¨ í‘œì‹œ
                        st.metric(
                            label=texts["risk_reduction_label"],
                            value=f"{rrr:.2f}%"
                        )
                    
                    st.markdown('</div>', unsafe_allow_html=True)
                    
                    # ìœ„í—˜ë„ ê·¸ë˜í”„ë¡œ í‘œí˜„ (ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‹œê°í™”)
                    st.markdown(f"#### {texts['t_value_change_header']}")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown(f"**{texts['before_improvement']}**")
                        st.progress(T_value / 25)  # 25ëŠ” ìµœëŒ€ Tê°’
                    
                    with col2:
                        st.markdown(f"**{texts['after_improvement']}**")
                        st.progress(improved_T / 25)
                else:
                    st.error(texts["parsing_error_improvement"])
                    st.write(texts["gpt_response"].format(response=generated_output))

# ----- í‘¸í„° ì„¹ì…˜: ë¡œê³  ì´ë¯¸ì§€ í‘œì‹œ -----
st.markdown('<hr style="margin-top: 50px;">', unsafe_allow_html=True)
st.markdown('<div style="display: flex; justify-content: space-between; align-items: center;">', unsafe_allow_html=True)

col1, col2 = st.columns(2)

with col1:
    if os.path.exists("cau.png"):
        cau_logo = Image.open("cau.png")
        st.image(cau_logo, width=150)
    else:
        st.warning("cau.png íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

with col2:
    if os.path.exists("doosan.png"):
        doosan_logo = Image.open("doosan.png")
        st.image(doosan_logo, width=180)
    else:
        st.warning("doosan.png íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

st.markdown('</div>', unsafe_allow_html=True)
