import streamlit as st
import pandas as pd
import numpy as np
import faiss
import torch
import openai
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import re
import os
from tqdm import tqdm

# ----- ì „ì—­ ë³€ìˆ˜ (ë°ì´í„°ì…‹ ì„ íƒ ì˜µì…˜) -----
dataset_options = {
    "SWRO ê±´ì¶•ê³µì • (ê±´ì¶•)": "SWRO ê±´ì¶•ê³µì • (ê±´ì¶•)",
    "Civil (í† ëª©)": "Civil (í† ëª©)",
    "Marine (í† ëª©)": "Marine (í† ëª©)",
    "SWRO ê¸°ê³„ê³µì‚¬ (í”ŒëœíŠ¸)": "SWRO ê¸°ê³„ê³µì‚¬ (í”ŒëœíŠ¸)",
    "SWRO ì „ê¸°ì‘ì—…í‘œì¤€ (í”ŒëœíŠ¸)": "SWRO ì „ê¸°ì‘ì—…í‘œì¤€ (í”ŒëœíŠ¸)"
}

def check_files_exist():
    """í•„ìš”í•œ íŒŒì¼ë“¤ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
    missing_files = []
    for dataset_name in dataset_options.values():
        if not os.path.exists(f"{dataset_name}.xlsx"):
            missing_files.append(f"{dataset_name}.xlsx")
    
    if not os.path.exists("phase1_general_api_updated.index"):
        missing_files.append("phase1_general_api_updated.index")
    
    if not os.path.exists("cau.png"):
        missing_files.append("cau.png")
    
    if not os.path.exists("doosan.png"):
        missing_files.append("doosan.png")
    
    if missing_files:
        st.error(f"ë‹¤ìŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_files)}")
        st.info("ëª¨ë“  í•„ìš” íŒŒì¼ì´ í˜„ì¬ ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return False
    return True

def determine_grade(value):
    """ë¹ˆë„*ê°•ë„ ê²°ê³¼ Tì— ë”°ë¥¸ ë“±ê¸‰ ê²°ì • í•¨ìˆ˜."""
    if 16 <= value <= 25:
        return 'A'
    elif 10 <= value <= 15:
        return 'B'
    elif 5 <= value <= 9:
        return 'C'
    elif 3 <= value <= 4:
        return 'D'
    elif 1 <= value <= 2:
        return 'E'
    else:
        return 'ì•Œ ìˆ˜ ì—†ìŒ'

def load_data(selected_dataset_name):
    """ì„ íƒëœ ì´ë¦„ì— ëŒ€ì‘í•˜ëŠ” Excel ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°."""
    try:
        file_path = f"{selected_dataset_name}.xlsx"
        st.info(f"ë°ì´í„° íŒŒì¼ '{file_path}'ì„ ë¡œë“œí•˜ëŠ” ì¤‘...")
        df = pd.read_excel(file_path)

        # ì „ì²˜ë¦¬
        if 'ì‚­ì œ Del' in df.columns:
            df = df.drop(['ì‚­ì œ Del'], axis=1)
        df = df.iloc[1:]
        df = df.rename(columns={df.columns[4]: 'ë¹ˆë„'})
        df = df.rename(columns={df.columns[5]: 'ê°•ë„'})

        df['T'] = pd.to_numeric(df.iloc[:, 4]) * pd.to_numeric(df.iloc[:, 5])
        df = df.iloc[:, :7]
        df.rename(
            columns={
                'ì‘ì—…í™œë™ ë° ë‚´ìš©\nWork & Contents': 'ì‘ì—…í™œë™ ë° ë‚´ìš©',
                'ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥\nHazard & Risk': 'ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥',
                'í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥\nDamage & Effect': 'í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥'
            },
            inplace=True
        )
        df = df.rename(columns={df.columns[6]: 'T'})
        df['ë“±ê¸‰'] = df['T'].apply(determine_grade)

        st.success(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ í–‰ ë¡œë“œë¨")
        return df
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.write(f"ì‹œë„í•œ íŒŒì¼ ê²½ë¡œ: {selected_dataset_name}.xlsx")
        return None

def load_index_file(index_filename="phase1_general_api_updated.index"):
    """ë¯¸ë¦¬ ê³„ì‚°ëœ ì¸ë±ìŠ¤ íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜."""
    try:
        st.info(f"ì¸ë±ìŠ¤ íŒŒì¼ '{index_filename}'ì„ ë¡œë“œí•˜ëŠ” ì¤‘...")
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        faiss_index = faiss.read_index(index_filename)
        st.success(f"ì¸ë±ìŠ¤ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {faiss_index.ntotal}ê°œ ë²¡í„° í¬í•¨")
        return faiss_index
    except Exception as e:
        st.error(f"ì¸ë±ìŠ¤ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def validate_api_key(api_key):
    """API í‚¤ ìœ íš¨ì„± ê²€ì¦"""
    try:
        openai.api_key = api_key
        # ê°„ë‹¨í•œ API í˜¸ì¶œë¡œ ê²€ì¦
        openai.Embedding.create(
            model="text-embedding-3-small",
            input=["API í‚¤ ê²€ì¦ìš© í…ìŠ¤íŠ¸"]
        )
        return True
    except Exception as e:
        st.error(f"API í‚¤ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
        return False

def generate_with_gpt4(prompt):
    """GPT-4 ëª¨ë¸ë¡œë¶€í„° ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ëŠ” í•¨ìˆ˜."""
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ìœ„í—˜ì„± í‰ê°€ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.0,
            max_tokens=50
        )
        return response['choices'][0]['message']['content'].strip()
    except Exception as e:
        st.error(f"GPT-4 API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

# ì¸ë±ìŠ¤ ìë™ ë¡œë“œ (ì²˜ìŒ ì‹¤í–‰ì‹œ)
if not st.session_state.index_loaded:
    with st.spinner('ì¸ë±ìŠ¤ íŒŒì¼ì„ ìë™ìœ¼ë¡œ ë¡œë“œí•˜ëŠ” ì¤‘...'):
        try:
            faiss_index = load_index_file("phase1_general_api_updated.index")
            if faiss_index is not None:
                st.session_state.index = faiss_index
                st.session_state.index_loaded = True
                st.success(f"ì¸ë±ìŠ¤ íŒŒì¼ì´ ìë™ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤! ì°¨ì›: {faiss_index.d}")
                # ì„ë² ë”© ëª¨ë¸ì„ ì¸ë±ìŠ¤ ì°¨ì›ì— ë§ê²Œ ì„¤ì •
                if faiss_index.d == 1536:
                    st.session_state.embedding_model = "text-embedding-3-large"
                elif faiss_index.d == 768:
                    st.session_state.embedding_model = "text-embedding-ada-002"
                elif faiss_index.d == 1024:
                    st.session_state.embedding_model = "text-embedding-3-small"
                else:
                    st.warning(f"ì¸ë±ìŠ¤ ì°¨ì›({faiss_index.d})ì— ë§ëŠ” ì„ë² ë”© ëª¨ë¸ì„ ì‹ë³„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©.")
                    st.session_state.embedding_model = "text-embedding-3-large"
            else:
                st.error("ì¸ë±ìŠ¤ ìë™ ë¡œë“œ ì‹¤íŒ¨")
                return
        except Exception as e:
            st.error(f"ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return

# embed_texts_with_openai í•¨ìˆ˜ ìˆ˜ì •
def embed_texts_with_openai(texts, model=None):
    """OpenAI ì„ë² ë”© APIë¡œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©."""
    # ì„¸ì…˜ ìƒíƒœì—ì„œ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
    if model is None:
        model = st.session_state.get('embedding_model', "text-embedding-3-large")
        
    st.write(f"ì‚¬ìš© ì¤‘ì¸ ì„ë² ë”© ëª¨ë¸: {model}")
    
    embeddings = []
    progress_bar = st.progress(0)
    total = len(texts)

    for idx, text in enumerate(tqdm(texts, desc="ì„ë² ë”© ì§„í–‰ ì¤‘", unit="ê°œ")):
        try:
            text = text.replace("\n", " ")
            response = openai.Embedding.create(model=model, input=[text])
            embedding = response["data"][0]["embedding"]
            embeddings.append(embedding)
            
            # ì²« ë²ˆì§¸ ì„ë² ë”© í›„ ì°¨ì› í™•ì¸ ë° ì¶œë ¥
            if idx == 0:
                st.write(f"ìƒì„±ëœ ì„ë² ë”© ì°¨ì›: {len(embedding)}")
                if hasattr(st.session_state, 'index') and st.session_state.index is not None:
                    if len(embedding) != st.session_state.index.d:
                        st.error(f"ì„ë² ë”© ì°¨ì›({len(embedding)})ì´ ì¸ë±ìŠ¤ ì°¨ì›({st.session_state.index.d})ê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
        except Exception as e:
            st.error(f"í…ìŠ¤íŠ¸ ì„ë² ë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            embeddings.append([0]*st.session_state.index.d)  # ì¸ë±ìŠ¤ ì°¨ì›ì— ë§ê²Œ ì¡°ì •
        
        progress_bar.progress((idx + 1) / total)
    
    return embeddings

def construct_prompt(retrieved_docs, query_text):
    """ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ë¡œë¶€í„° ì˜ˆì‹œë¥¼ êµ¬ì„±í•´ GPT í”„ë¡¬í”„íŠ¸ ìƒì„±."""
    retrieved_examples = []
    try:
        for _, doc in retrieved_docs.iterrows():
            content_parts = doc['content'].split()
            example_input = ' '.join(content_parts[:-6])
            frequency = int(content_parts[-4])
            intensity = int(content_parts[-3])
            T_value = frequency * intensity
            example_output = f'{{"ë¹ˆë„": {frequency}, "ê°•ë„": {intensity}, "T": {T_value}}}'
            retrieved_examples.append((example_input, example_output))
    except Exception as e:
        st.error(f"í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None
    
    prompt = ""
    for i, (example_input, example_output) in enumerate(retrieved_examples, 1):
        prompt += f"ì˜ˆì‹œ {i}:\nì…ë ¥: {example_input}\nì¶œë ¥: {example_output}\n\n"
    
    prompt += (
        f"ì…ë ¥: {query_text}\n"
        "ìœ„ ì…ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ë¹ˆë„ì™€ ê°•ë„ë¥¼ ì˜ˆì¸¡í•˜ì„¸ìš”. "
        "ë¹ˆë„ëŠ” 1ì—ì„œ 5 ì‚¬ì´ì˜ ì •ìˆ˜ì…ë‹ˆë‹¤. "
        "ê°•ë„ëŠ” 1ì—ì„œ 5 ì‚¬ì´ì˜ ì •ìˆ˜ì…ë‹ˆë‹¤. "
        "TëŠ” ë¹ˆë„ì™€ ê°•ë„ë¥¼ ê³±í•œ ê°’ì…ë‹ˆë‹¤.\n"
        "ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:\n"
        '{"ë¹ˆë„": ìˆ«ì, "ê°•ë„": ìˆ«ì, "T": ìˆ«ì}\n'
        "ì¶œë ¥:\n"
    )
    return prompt

def parse_gpt_output(gpt_output):
    """
    GPT ì¶œë ¥ì—ì„œ {ë¹ˆë„, ê°•ë„, T}ë¥¼ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¶”ì¶œ.
    ë§¤ì¹­ ì„±ê³µ ì‹œ (ë¹ˆë„, ê°•ë„, T)ë¥¼ ë¦¬í„´, ì‹¤íŒ¨ ì‹œ None ë¦¬í„´.
    """
    if not gpt_output:
        return None
        
    json_pattern = r'\{"ë¹ˆë„":\s*([1-5]),\s*"ê°•ë„":\s*([1-5]),\s*"T":\s*([0-9]+)\}'
    match = re.search(json_pattern, gpt_output)
    if match:
        pred_frequency = int(match.group(1))
        pred_intensity = int(match.group(2))
        pred_T = int(match.group(3))
        return pred_frequency, pred_intensity, pred_T
    else:
        return None

def main():
    st.set_page_config(
        page_title="ìœ„í—˜ì„± í‰ê°€ ì‹œìŠ¤í…œ",
        page_icon="ğŸ—ï¸",
        layout="wide"
    )

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not check_files_exist():
        st.stop()

    # ----- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” -----
    # (index, user inputs ë“±ì„ ë³´ê´€)
    if "index" not in st.session_state:
        st.session_state.index = None
    if "retriever_pool_df" not in st.session_state:
        st.session_state.retriever_pool_df = None
    if "index_loaded" not in st.session_state:
        st.session_state.index_loaded = False

    # ë©”ì¸ íƒ€ì´í‹€
    st.title("ìƒì„±í˜• AI ê¸°ë°˜ ìœ„í—˜ì„± í‰ê°€ ì‹œìŠ¤í…œ")

    # ë°ì´í„°ì…‹ ì„ íƒ
    selected_dataset_name = st.selectbox(
        "ë°ì´í„°ì…‹ ì„ íƒ",
        options=list(dataset_options.keys()),
        key="dataset_selector"
    )
    st.write(f"ì„ íƒëœ ë°ì´í„°ì…‹: {selected_dataset_name}")

    # ë¡œê³  í‘œì‹œ
    col1, col2 = st.columns(2)
    with col1:
        try:
            st.image("cau.png", width=200)
        except Exception as e:
            st.error(f"ì¤‘ì•™ëŒ€í•™êµ ë¡œê³  ë¡œë”© ì‹¤íŒ¨: {str(e)}")
    with col2:
        try:
            st.image("doosan.png", width=200)
        except Exception as e:
            st.error(f"ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹° ë¡œê³  ë¡œë”© ì‹¤íŒ¨: {str(e)}")

    # API í‚¤ ì…ë ¥
    api_key = st.text_input(
        "OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
        type="password",
        help="API í‚¤ëŠ” ì•ˆì „í•˜ê²Œ ë³´ê´€ë©ë‹ˆë‹¤."
    )
    if not api_key:
        st.warning("ê³„ì†í•˜ë ¤ë©´ OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        return
    
    # API í‚¤ ê²€ì¦
    if not validate_api_key(api_key):
        st.warning("ìœ íš¨í•˜ì§€ ì•Šì€ API í‚¤ì…ë‹ˆë‹¤. ì˜¬ë°”ë¥¸ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        return
    
    openai.api_key = api_key

    # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    with st.spinner('ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...'):
        df = load_data(dataset_options[selected_dataset_name])
    if df is None:
        st.error("ë°ì´í„° ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return

    # train/test ë¶„í• 
    try:
        train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)
        test_df = test_df[['ì‘ì—…í™œë™ ë° ë‚´ìš©', 'ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥', 'ë¹ˆë„', 'ê°•ë„', 'T']]
    except Exception as e:
        st.error(f"ë°ì´í„° ë¶„í•  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return

    # Retriever Pool êµ¬ì„± (ì„¸ì…˜ ìƒíƒœì— ì €ì¥)
    if st.session_state.retriever_pool_df is None:
        try:
            retriever_pool_df = train_df.copy()
            retriever_pool_df['content'] = retriever_pool_df.apply(
                lambda row: ' '.join(row.values.astype(str)), axis=1
            )
            st.session_state.retriever_pool_df = retriever_pool_df
        except Exception as e:
            st.error(f"ê²€ìƒ‰ í’€ êµ¬ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return
    
    # ì¸ë±ìŠ¤ ìë™ ë¡œë“œ (ì²˜ìŒ ì‹¤í–‰ì‹œ)
    if not st.session_state.index_loaded:
        with st.spinner('ì¸ë±ìŠ¤ íŒŒì¼ì„ ìë™ìœ¼ë¡œ ë¡œë“œí•˜ëŠ” ì¤‘...'):
            try:
                faiss_index = load_index_file("phase1_general_api_updated.index")
                if faiss_index is not None:
                    st.session_state.index = faiss_index
                    st.session_state.index_loaded = True
                    st.success("ì¸ë±ìŠ¤ íŒŒì¼ì´ ìë™ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                else:
                    st.error("ì¸ë±ìŠ¤ ìë™ ë¡œë“œ ì‹¤íŒ¨")
                    return
            except Exception as e:
                st.error(f"ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                return
    
    # ----- íƒ­ êµ¬ë¶„ -----
    tabs = st.tabs(["ì‚¬ìš©ì ì…ë ¥ ì˜ˆì¸¡", "ìƒ˜í”Œ ì˜ˆì¸¡"])

    # ----- ê³µí†µ ì„¤ì • (ì‚¬ì´ë“œë°” ë“±) -----
    with st.sidebar:
        st.header("ğŸ“Š ë¶„ì„ ì„¤ì •")
        k_similar = st.slider("ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ìˆ˜", min_value=1, max_value=10, value=5)

    # íƒ­ 1) ì‚¬ìš©ì ì…ë ¥ ì˜ˆì¸¡
    with tabs[0]:
        st.subheader("ì‚¬ìš©ì ì…ë ¥ ì˜ˆì¸¡")

        if st.session_state.index is None:
            st.warning("ì¸ë±ìŠ¤ íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        else:
            with st.form("user_input_form"):
                user_work = st.text_input("ì‘ì—…í™œë™ (ì‚¬ìš©ì ì…ë ¥):", key="form_user_work")
                user_risk = st.text_input("ìœ í•´ìœ„í—˜ìš”ì¸ (ì‚¬ìš©ì ì…ë ¥):", key="form_user_risk")
                submitted = st.form_submit_button("ì‚¬ìš©ì ì…ë ¥ìœ¼ë¡œ ì˜ˆì¸¡í•˜ê¸°")

            if submitted:
                if not user_work or not user_risk:
                    st.warning("ì‘ì—…í™œë™ê³¼ ìœ í•´ìœ„í—˜ìš”ì¸ì„ ëª¨ë‘ ì…ë ¥í•˜ì„¸ìš”.")
                else:
                    query_text = f"{user_work} {user_risk}"
                    
                    try:
                        # ì¿¼ë¦¬ ì„ë² ë”©
                        with st.spinner('ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì¤‘...'):
                            query_embedding = embed_texts_with_openai([query_text])[0]
                            query_embedding_array = np.array([query_embedding], dtype='float32')
                        
                        # ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
                        with st.spinner('ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ì¤‘...'):
                            distances, indices = st.session_state.index.search(query_embedding_array, k_similar)
                            
                            # ì¸ë±ìŠ¤ ë²”ìœ„ ê²€ì¦
                            valid_indices = [idx for idx in indices[0] if idx < len(st.session_state.retriever_pool_df)]
                            if len(valid_indices) == 0:
                                st.error("ì¸ë±ìŠ¤ì™€ ë°ì´í„°í”„ë ˆì„ ê°„ ë¶ˆì¼ì¹˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                                return
                                
                            retrieved_docs = st.session_state.retriever_pool_df.iloc[valid_indices]

                        # GPT í”„ë¡¬í”„íŠ¸ ìƒì„± & í˜¸ì¶œ
                        with st.spinner('GPT ëª¨ë¸ í˜¸ì¶œ ì¤‘...'):
                            prompt = construct_prompt(retrieved_docs, query_text)
                            if not prompt:
                                st.error("í”„ë¡¬í”„íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                                return
                                
                            generated_output = generate_with_gpt4(prompt)

                        st.markdown(f"**ì‚¬ìš©ì ì…ë ¥ ì¿¼ë¦¬**: {query_text}")
                        parse_result = parse_gpt_output(generated_output)
                        if parse_result is not None:
                            f_val, i_val, t_val = parse_result
                            grade = determine_grade(t_val)
                            st.write(f"GPT ì˜ˆì¸¡ â†’ ë¹ˆë„: {f_val}, ê°•ë„: {i_val}, T: {t_val}, ë“±ê¸‰: {grade}")
                        else:
                            st.write(f"GPT ì˜ˆì¸¡(ì›ë¬¸): {generated_output}")
                    except Exception as e:
                        st.error(f"ì˜ˆì¸¡ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    # íƒ­ 2) ìƒ˜í”Œ ì˜ˆì¸¡
    # íƒ­ 2) ìƒ˜í”Œ ì˜ˆì¸¡
        with tabs[1]:
            st.subheader("ìƒ˜í”Œ ì˜ˆì¸¡ (ìƒìœ„ 3ê°œë§Œ í‘œì‹œ)")
        
            if st.session_state.index is None:
                st.warning("ì¸ë±ìŠ¤ íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            else:
                # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
                st.write(f"ì¸ë±ìŠ¤ ë²¡í„° ìˆ˜: {st.session_state.index.ntotal}")
                st.write(f"ë°ì´í„°í”„ë ˆì„ í–‰ ìˆ˜: {len(st.session_state.retriever_pool_df)}")
                st.write(f"test_df í–‰ ìˆ˜: {len(test_df)}")
                
                try:
                    sample_df = test_df.iloc[:3].copy().reset_index(drop=True)
                    st.write(f"ìƒ˜í”Œ ë°ì´í„°í”„ë ˆì„ í–‰ ìˆ˜: {len(sample_df)}")
        
                    for idx, row in sample_df.iterrows():
                        st.markdown(f"**ìƒ˜í”Œ {idx+1}**")
                        st.markdown(f"- ì‘ì—…í™œë™: {row['ì‘ì—…í™œë™ ë° ë‚´ìš©']}")
                        st.markdown(f"- ìœ í•´ìœ„í—˜ìš”ì¸: {row['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥']}")
                        st.markdown(f"- ì‹¤ì œ ë¹ˆë„: {row['ë¹ˆë„']}, ì‹¤ì œ ê°•ë„: {row['ê°•ë„']}, ì‹¤ì œ T: {row['T']}")
        
                        query_text = f"{row['ì‘ì—…í™œë™ ë° ë‚´ìš©']} {row['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥']}"
                        st.write(f"ì¿¼ë¦¬ í…ìŠ¤íŠ¸: {query_text}")
        
                        try:
                            # ì¿¼ë¦¬ ì„ë² ë”©
                            st.write("ì„ë² ë”© ìƒì„± ì¤‘...")
                            query_embedding = embed_texts_with_openai([query_text])[0]
                            query_embedding_array = np.array([query_embedding], dtype='float32')
                            st.write("ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        
                            # FAISS ê²€ìƒ‰
                            st.write("ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
                            distances, indices = st.session_state.index.search(query_embedding_array, k_similar)
                            st.write(f"ê²€ìƒ‰ëœ ì¸ë±ìŠ¤: {indices[0]}")
                            st.write(f"ê²€ìƒ‰ëœ ê±°ë¦¬: {distances[0]}")
                            
                            # ì¸ë±ìŠ¤ ë²”ìœ„ ê²€ì¦
                            st.write(f"ìµœëŒ€ ì¸ë±ìŠ¤ ê°’: {max(indices[0]) if len(indices[0]) > 0 else 'No indices'}")
                            st.write(f"ë°ì´í„°í”„ë ˆì„ í¬ê¸°: {len(st.session_state.retriever_pool_df)}")
                            
                            valid_indices = [i for i in indices[0] if i < len(st.session_state.retriever_pool_df)]
                            st.write(f"ìœ íš¨í•œ ì¸ë±ìŠ¤ ìˆ˜: {len(valid_indices)}")
                            
                            if len(valid_indices) == 0:
                                st.error(f"ìƒ˜í”Œ {idx+1}: ì¸ë±ìŠ¤ì™€ ë°ì´í„°í”„ë ˆì„ ê°„ ë¶ˆì¼ì¹˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                                st.error("ê²€ìƒ‰ëœ ì¸ë±ìŠ¤ê°€ ëª¨ë‘ ë°ì´í„°í”„ë ˆì„ ë²”ìœ„ë¥¼ ë²—ì–´ë‚©ë‹ˆë‹¤.")
                                continue
                                
                            retrieved_docs = st.session_state.retriever_pool_df.iloc[valid_indices]
                            st.write(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}")
                            
                            # ì²« ë²ˆì§¸ ê²€ìƒ‰ ë¬¸ì„œ ë‚´ìš© ìƒ˜í”Œ í‘œì‹œ
                            if len(retrieved_docs) > 0:
                                st.write("ì²« ë²ˆì§¸ ê²€ìƒ‰ ë¬¸ì„œ ë‚´ìš© ìƒ˜í”Œ:")
                                st.write(retrieved_docs.iloc[0]['content'][:200] + "...")
        
                            # GPT í˜¸ì¶œ
                            st.write("í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...")
                            prompt = construct_prompt(retrieved_docs, query_text)
                            if not prompt:
                                st.error(f"ìƒ˜í”Œ {idx+1}: í”„ë¡¬í”„íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                                continue
                            
                            st.write("GPT ëª¨ë¸ í˜¸ì¶œ ì¤‘...")
                            generated_output = generate_with_gpt4(prompt)
                            st.write(f"GPT ì›ë³¸ ì¶œë ¥: {generated_output}")
        
                            # GPT ì˜ˆì¸¡ íŒŒì‹±
                            parse_result = parse_gpt_output(generated_output)
                            if parse_result is not None:
                                f_val, i_val, t_val = parse_result
                                grade = determine_grade(t_val)
                                st.write(f"**GPT ì˜ˆì¸¡** â†’ ë¹ˆë„: {f_val}, ê°•ë„: {i_val}, T: {t_val}, ë“±ê¸‰: {grade}")
                            else:
                                st.write(f"**GPT ì˜ˆì¸¡(íŒŒì‹± ì‹¤íŒ¨)**: {generated_output}")
                        
                        except Exception as e:
                            st.error(f"ìƒ˜í”Œ {idx+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                            import traceback
                            st.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
                        
                        st.markdown("---")
        
                    st.markdown("### ì˜ˆì¸¡ ì™„ë£Œ")
                    st.info("ìƒê¸° í‘œì‹œëœ ìƒ˜í”Œ 3ê°œëŠ” ì‹¤ì œ ë°ì´í„°ì…‹ì—ì„œ ì¼ë¶€ë§Œ ë°œì·Œí•œ ì˜ˆì‹œì…ë‹ˆë‹¤.")
                
                except Exception as e:
                    st.error(f"ìƒ˜í”Œ ì˜ˆì¸¡ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    import traceback
                    st.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        st.error(f"í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
